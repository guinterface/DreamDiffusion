{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bm7w7ewEVuVk",
    "outputId": "f410b8d3-eb82-47ad-c37b-060f394026ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: GPUtil in /home/guisi/.local/lib/python3.12/site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kh8BJczMU1df",
    "outputId": "6e1480fa-be3f-4a0b-b2b1-a9be61e93802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 179.4 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n",
      "Total GPU memory: 81920.0GB\n",
      "Used GPU memory: 0.0GB\n",
      "Free GPU memory: 81050.0GB\n"
     ]
    }
   ],
   "source": [
    "#@title Give me a better RAM machine\n",
    "\n",
    "import os \n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "def get_gpu_memory():\n",
    "    GPUs = GPUtil.getGPUs()\n",
    "    if len(GPUs) == 0:\n",
    "        return 0\n",
    "    return GPUs[0].memoryTotal, GPUs[0].memoryUsed, GPUs[0].memoryFree\n",
    "\n",
    "gpu_total, gpu_used, gpu_free = get_gpu_memory()\n",
    "print(f\"Total GPU memory: {gpu_total:.1f}GB\")\n",
    "print(f\"Used GPU memory: {gpu_used:.1f}GB\")\n",
    "print(f\"Free GPU memory: {gpu_free:.1f}GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZh0Ue-4o3CN",
    "outputId": "1ae65e09-dfb0-4edb-ade5-83817c34c76d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "k-4fuFvpswL5"
   },
   "outputs": [],
   "source": [
    "#@title Loadd std\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load .pth file\n",
    "eeg_data = torch.load('dataset/eegdataset/eeg_data.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset', 'image', 'label'])\n",
      "{'eeg': array([[-6.80129412e-03, -8.13837010e-03, -7.24686161e-03, ...,\n",
      "         1.38477710e-05,  1.24644385e-05,  1.26223081e-05],\n",
      "       [-3.86516619e-03, -4.62602843e-03, -4.12041591e-03, ...,\n",
      "         7.35388677e-06,  6.62768373e-06,  7.17609039e-06],\n",
      "       [-2.82500738e-03, -3.38188469e-03, -3.01239802e-03, ...,\n",
      "         5.32329222e-06,  4.18285358e-06,  4.79385925e-06],\n",
      "       ...,\n",
      "       [-7.14428731e-03, -8.54811083e-03, -7.61207333e-03, ...,\n",
      "         1.20062692e-05,  9.07801884e-06,  8.92867290e-06],\n",
      "       [-5.59089883e-03, -6.69045521e-03, -5.95824649e-03, ...,\n",
      "         1.27233213e-05,  9.12206139e-06,  8.60137537e-06],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), 'image': 0, 'label': 0, 'subject': 1}\n"
     ]
    }
   ],
   "source": [
    "print(eeg_data.keys())\n",
    "\n",
    "model_state_dict = eeg_data['dataset'][0]\n",
    "print(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgsbonfim\u001b[0m (\u001b[33mneuro-cife\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCurrent Settings\u001b[0m\n",
      "{\n",
      "  \"_extra_http_headers\": null,\n",
      "  \"_proxies\": null,\n",
      "  \"api_key\": null,\n",
      "  \"base_url\": \"https://api.wandb.ai\",\n",
      "  \"entity\": null,\n",
      "  \"git_remote\": \"origin\",\n",
      "  \"ignore_globs\": [],\n",
      "  \"project\": null,\n",
      "  \"root_dir\": null,\n",
      "  \"section\": \"default\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "!wandb status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-9BA3Z9rk167"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07X_s3P1lZFV",
    "outputId": "e565e2ef-0dba-4f17-d769-bd60e263c640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (128, 515)\n",
      "Data Type: float32\n",
      "First few entries: [[ 0.03016403  0.1650207   0.299093   ...  0.45803174  0.23275055\n",
      "   0.035388  ]\n",
      " [ 0.11413796  0.38894385  0.6745958  ...  1.352438    0.7022273\n",
      "   0.1589681 ]\n",
      " [-0.04542113 -0.00612628  0.01903304 ... -0.39836854 -0.21198858\n",
      "  -0.06583108]\n",
      " ...\n",
      " [ 0.04823001  0.20895463  0.42897996 ... -0.77373064 -0.3612818\n",
      "   0.02656385]\n",
      " [-0.00283181  0.00311891  0.01241865 ... -0.04714558 -0.02521099\n",
      "  -0.0039387 ]\n",
      " [ 0.02853161  0.1162913   0.24745055 ... -0.50116014 -0.2344262\n",
      "   0.01591536]]\n"
     ]
    }
   ],
   "source": [
    "#@title Check the Load the .npy file\n",
    "data = np.load('dataset/eegdataset/eeg/10000.npy')\n",
    "\n",
    "# Print its attributes\n",
    "print(\"Shape:\", data.shape)\n",
    "print(\"Data Type:\", data.dtype)\n",
    "print(\"First few entries:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Now save all the files - don't run this unless eeg is not populated\n",
    "# # Assuming you've already loaded eeg_data from the .pth file\n",
    "# import numpy as np\n",
    "# dataset = eeg_data['dataset']\n",
    "\n",
    "# # Define a base path to save\n",
    "# base_save_path = \"dataset/eegdataset2/\"\n",
    "\n",
    "\n",
    "# # Loop through all items in the dataset\n",
    "# for idx, tensor_item in enumerate(dataset):\n",
    "    \n",
    "\n",
    "#     # Loop over all attributes in the tensor_item\n",
    "#     for key, value in tensor_item.items():\n",
    "\n",
    "#         # Construct the subfolder path based on the attribute\n",
    "#         subfolder_path = os.path.join(base_save_path, key)\n",
    "\n",
    "#         # Check if the subfolder exists, if not, create it\n",
    "#         if not os.path.exists(subfolder_path):\n",
    "#             os.makedirs(subfolder_path)\n",
    "\n",
    "#         # If the value is a torch.Tensor, convert it to a numpy array\n",
    "#         if isinstance(value, torch.Tensor):\n",
    "#             ndarray = value.numpy()\n",
    "#             try:\n",
    "#               np.save(f\"{subfolder_path}/{idx}.npy\", ndarray)\n",
    "#             except Exception as e:\n",
    "#               print(f\"Error saving file at index {idx}: {e}\")\n",
    "\n",
    "#         elif isinstance(value, np.ndarray):\n",
    "#             try:\n",
    "#               np.save(f\"{subfolder_path}/{idx}.npy\", value)\n",
    "#             except Exception as e:\n",
    "#               print(f\"Error saving file at index {idx}: {e}\")\n",
    "#         #else:\n",
    "#             # If the value is not a tensor, simply save it as it is\n",
    "#             #try:\n",
    "#             #  np.save(f\"{subfolder_path}/{idx}.npy\", np.array(value))\n",
    "#             #except Exception as e:\n",
    "#             #  print(f\"Error saving file at index {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "0SDOJLu-0B6k"
   },
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"my-awesome-project\",\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 10,\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECT8m9uN0MZ2",
    "outputId": "09244673-37a8-4028-cc68-fdf99e3ca16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1189 files in the directory.\n"
     ]
    }
   ],
   "source": [
    "#@title THere should be around 12k entries of .npy files.\n",
    "import os\n",
    "\n",
    "# Path to the directory\n",
    "dir_path = 'dataset/eegdataset2/eeg/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "# Print the number of files\n",
    "print(f\"There are {len(files)} files in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t2NckeKMWZ16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch==1.12.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision==0.13.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torch==1.12.1) (4.12.0)\n",
      "Requirement already satisfied: numpy in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torchvision==0.13.1) (1.24.4)\n",
      "Requirement already satisfied: requests in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torchvision==0.13.1) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torchvision==0.13.1) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision==0.13.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision==0.13.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision==0.13.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision==0.13.1) (2024.2.2)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://download.pytorch.org/whl/cu116 torch==1.12.1 torchvision==0.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tK62bopDI7B",
    "outputId": "414671f9-caf0-4d4c-a63a-3bb3bcc4a8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (0.3.0)\n",
      "Requirement already satisfied: transformers in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->transformers) (2024.2.2)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/guisi/.local/lib/python3.12/site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /home/guisi/.local/lib/python3.12/site-packages (from scipy) (1.26.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /home/guisi/.local/lib/python3.12/site-packages (0.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: natsort in /home/guisi/.local/lib/python3.12/site-packages (8.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/guisi/.local/lib/python3.12/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /home/guisi/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/guisi/.local/lib/python3.12/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/guisi/.local/lib/python3.12/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.12/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/guisi/.local/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/miniconda/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/guisi/.local/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/guisi/.local/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/guisi/.local/lib/python3.12/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/guisi/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "48GYiAlBBBN7"
   },
   "outputs": [],
   "source": [
    "#@title Handle dataset related tasks scripts:\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import interpolate\n",
    "from einops import rearrange\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.interpolate import interp1d\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "def pad_to_patch_size(x, patch_size):\n",
    "    assert x.ndim == 2\n",
    "    return np.pad(x, ((0,0),(0, patch_size-x.shape[1]%patch_size)), 'wrap')\n",
    "\n",
    "def pad_to_length(x, length):\n",
    "    assert x.ndim == 3\n",
    "    assert x.shape[-1] <= length\n",
    "    if x.shape[-1] == length:\n",
    "        return x\n",
    "\n",
    "    return np.pad(x, ((0,0),(0,0), (0, length - x.shape[-1])), 'wrap')\n",
    "\n",
    "def normalize(x, mean=None, std=None):\n",
    "    mean = np.mean(x) if mean is None else mean\n",
    "    std = np.std(x) if std is None else std\n",
    "    return (x - mean) / (std * 1.0)\n",
    "\n",
    "def process_voxel_ts(v, p, t=8):\n",
    "    '''\n",
    "    v: voxel timeseries of a subject. (1200, num_voxels)\n",
    "    p: patch size\n",
    "    t: time step of the averaging window for v. Kamitani used 8 ~ 12s\n",
    "    return: voxels_reduced. reduced for the alignment of the patch size (num_samples, num_voxels_reduced)\n",
    "\n",
    "    '''\n",
    "    # average the time axis first\n",
    "    num_frames_per_window = t // 0.75 # ~0.75s per frame in HCP\n",
    "    v_split = np.array_split(v, len(v) // num_frames_per_window, axis=0)\n",
    "    v_split = np.concatenate([np.mean(f,axis=0).reshape(1,-1) for f in v_split],axis=0)\n",
    "    # pad the num_voxels\n",
    "    # v_split = np.concatenate([v_split, np.zeros((v_split.shape[0], p - v_split.shape[1] % p))], axis=-1)\n",
    "    v_split = pad_to_patch_size(v_split, p)\n",
    "    v_split = normalize(v_split)\n",
    "    return v_split\n",
    "\n",
    "def augmentation(data, aug_times=2, interpolation_ratio=0.5):\n",
    "    '''\n",
    "    data: num_samples, num_voxels_padded\n",
    "    return: data_aug: num_samples*aug_times, num_voxels_padded\n",
    "    '''\n",
    "    num_to_generate = int((aug_times-1)*len(data))\n",
    "    if num_to_generate == 0:\n",
    "        return data\n",
    "    pairs_idx = np.random.choice(len(data), size=(num_to_generate, 2), replace=True)\n",
    "    data_aug = []\n",
    "    for i in pairs_idx:\n",
    "        z = interpolate_voxels(data[i[0]], data[i[1]], interpolation_ratio)\n",
    "        data_aug.append(np.expand_dims(z,axis=0))\n",
    "    data_aug = np.concatenate(data_aug, axis=0)\n",
    "\n",
    "    return np.concatenate([data, data_aug], axis=0)\n",
    "\n",
    "def interpolate_voxels(x, y, ratio=0.5):\n",
    "    ''''\n",
    "    x, y: one dimension voxels array\n",
    "    ratio: ratio for interpolation\n",
    "    return: z same shape as x and y\n",
    "\n",
    "    '''\n",
    "    values = np.stack((x,y))\n",
    "    points = (np.r_[0, 1], np.arange(len(x)))\n",
    "    xi = np.c_[np.full((len(x)), ratio), np.arange(len(x)).reshape(-1,1)]\n",
    "    z = interpolate.interpn(points, values, xi)\n",
    "    return z\n",
    "\n",
    "def img_norm(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = (img / 255.0) * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "\n",
    "def channel_first(img):\n",
    "        if img.shape[-1] == 3:\n",
    "            return rearrange(img, 'h w c -> c h w')\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def file_ext(name: Union[str, Path]) -> str:\n",
    "    return str(name).split('.')[-1]\n",
    "\n",
    "def is_npy_ext(fname: Union[str, Path]) -> bool:\n",
    "    ext = file_ext(fname).lower()\n",
    "    return f'{ext}' == 'npy'# type: ignore\n",
    "\n",
    "class eeg_pretrain_dataset(Dataset):\n",
    "    def __init__(self, path='dataset/eegdataset2/eeg/', roi='VC', patch_size=16, transform=identity, aug_times=2,\n",
    "                num_sub_limit=None, include_kam=False, include_hcp=True):\n",
    "        super(eeg_pretrain_dataset, self).__init__()\n",
    "        data = []\n",
    "        images = []\n",
    "        self.input_paths = [str(f) for f in sorted(Path(path).rglob('*')) if is_npy_ext(f) and os.path.isfile(f)]\n",
    "\n",
    "        assert len(self.input_paths) != 0, 'No data found'\n",
    "        self.data_len  = 512\n",
    "        self.data_chan = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.input_paths[index]\n",
    "\n",
    "        data = np.load(data_path)\n",
    "\n",
    "        if data.shape[-1] > self.data_len:\n",
    "            idx = np.random.randint(0, int(data.shape[-1] - self.data_len)+1)\n",
    "\n",
    "            data = data[:, idx: idx+self.data_len]\n",
    "        else:\n",
    "            x = np.linspace(0, 1, data.shape[-1])\n",
    "            x2 = np.linspace(0, 1, self.data_len)\n",
    "            f = interp1d(x, data)\n",
    "            data = f(x2)\n",
    "        ret = np.zeros((self.data_chan, self.data_len))\n",
    "        if (self.data_chan > data.shape[-2]):\n",
    "            for i in range((self.data_chan//data.shape[-2])):\n",
    "\n",
    "                ret[i * data.shape[-2]: (i+1) * data.shape[-2], :] = data\n",
    "            if self.data_chan % data.shape[-2] != 0:\n",
    "\n",
    "                ret[ -(self.data_chan%data.shape[-2]):, :] = data[: (self.data_chan%data.shape[-2]), :]\n",
    "        elif(self.data_chan < data.shape[-2]):\n",
    "            idx2 = np.random.randint(0, int(data.shape[-2] - self.data_chan)+1)\n",
    "            ret = data[idx2: idx2+self.data_chan, :]\n",
    "        # print(ret.shape)\n",
    "        elif(self.data_chan == data.shape[-2]):\n",
    "            ret = data\n",
    "        ret = ret/10 # reduce an order\n",
    "        # torch.tensor()\n",
    "        ret = torch.from_numpy(ret).float()\n",
    "        return {'eeg': ret } #,\n",
    "\n",
    "\n",
    "\n",
    "def get_img_label(class_index:dict, img_filename:list, naive_label_set=None):\n",
    "    img_label = []\n",
    "    wind = []\n",
    "    desc = []\n",
    "    for _, v in class_index.items():\n",
    "        n_list = []\n",
    "        for n in v[:-1]:\n",
    "            n_list.append(int(n[1:]))\n",
    "        wind.append(n_list)\n",
    "        desc.append(v[-1])\n",
    "\n",
    "    naive_label = {} if naive_label_set is None else naive_label_set\n",
    "    for _, file in enumerate(img_filename):\n",
    "        name = int(file[0].split('.')[0])\n",
    "        naive_label[name] = []\n",
    "        nl = list(naive_label.keys()).index(name)\n",
    "        for c, (w, d) in enumerate(zip(wind, desc)):\n",
    "            if name in w:\n",
    "                img_label.append((c, d, nl))\n",
    "                break\n",
    "    return img_label, naive_label\n",
    "\n",
    "class base_dataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=identity):\n",
    "        super(base_dataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        if self.y is None:\n",
    "            return self.transform(self.x[index])\n",
    "        else:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "\n",
    "def remove_repeats(fmri, img_lb):\n",
    "    assert len(fmri) == len(img_lb), 'len error'\n",
    "    fmri_dict = {}\n",
    "    for f, lb in zip(fmri, img_lb):\n",
    "        if lb in fmri_dict.keys():\n",
    "            fmri_dict[lb].append(f)\n",
    "        else:\n",
    "            fmri_dict[lb] = [f]\n",
    "    lbs = []\n",
    "    fmris = []\n",
    "    for k, v in fmri_dict.items():\n",
    "        lbs.append(k)\n",
    "        fmris.append(np.mean(np.stack(v), axis=0))\n",
    "    return np.stack(fmris), lbs\n",
    "\n",
    "\n",
    "def list_get_all_index(list, value):\n",
    "    return [i for i, v in enumerate(list) if v == value]\n",
    "\n",
    "EEG_EXTENSIONS = [\n",
    "    '.mat'\n",
    "]\n",
    "\n",
    "\n",
    "def is_mat_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in EEG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def make_dataset(dir):\n",
    "\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "    for root, _, fnames in sorted(os.walk(dir, topdown=False)):#\n",
    "        for fname in fnames:\n",
    "            if is_mat_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class EEGDataset_r(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, eeg_signals_path, image_transform=identity):\n",
    "\n",
    "        self.imagenet = 'dataset/eegdataset/ImageFine-Tuning'\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = 440\n",
    "        self.data_len = 512\n",
    "        # # Compute size\n",
    "        self.size = 100\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Process EEG\n",
    "        eeg = torch.randn(128,512)\n",
    "\n",
    "        # print(image.shape)\n",
    "        label = torch.tensor(0).long()\n",
    "        image = torch.randn(3,512,512)\n",
    "        image_raw = image\n",
    "\n",
    "        return {'eeg': eeg, 'label': label, 'image': self.image_transform(image), 'image_raw': image_raw}\n",
    "\n",
    "\n",
    "class EEGDataset_s(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, eeg_signals_path, image_transform=identity):\n",
    "        # Load EEG signals\n",
    "        loaded = torch.load(eeg_signals_path)\n",
    "        # if opt.subject!=0:\n",
    "        #     self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==opt.subject]\n",
    "        # else:\n",
    "        self.data = loaded['dataset']\n",
    "        self.labels = loaded[\"label\"]\n",
    "        self.images = loaded[\"image\"]\n",
    "        self.imagenet = 'dataset/eegdataset/ImageFine-Tuning'\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = 440\n",
    "        # Compute size\n",
    "        self.size = len(self.data)\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Process EEG\n",
    "        eeg = torch.from_numpy(self.data[i][\"eeg\"]).float().t()\n",
    "\n",
    "        eeg = eeg[20:460,:]\n",
    "\n",
    "        # Get label\n",
    "        image_name = self.images[self.data[i][\"image\"]]\n",
    "        # image_path = os.path.join(self.imagenet, image_name.split('_')[0], image_name+'.JPEG')\n",
    "        return image_name\n",
    "\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, eeg_signals_path, image_transform=identity, subject = 4):\n",
    "        # Load EEG signals\n",
    "        loaded = torch.load(eeg_signals_path)\n",
    "        # if opt.subject!=0:\n",
    "        #     self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==opt.subject]\n",
    "        # else:\n",
    "        # print(loaded)\n",
    "        if subject!=0:\n",
    "            self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==subject]\n",
    "        else:\n",
    "            self.data = loaded['dataset']\n",
    "        self.labels = loaded[\"label\"]\n",
    "        self.images = loaded[\"image\"]\n",
    "        self.imagenet = 'dataset/eegdataset/ImageFine-Tuning'\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = 440\n",
    "        self.data_len = 512\n",
    "        # Compute size\n",
    "        self.size = len(self.data)\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Process EEG\n",
    "        # print(self.data[i])\n",
    "        print(i)\n",
    "        eeg = torch.from_numpy(self.data[i][\"eeg\"]).float().t()\n",
    "\n",
    "        eeg = eeg[20:460,:]\n",
    "        ##### 2023 2 13 add preprocess and transpose\n",
    "        eeg = np.array(eeg.transpose(0,1))\n",
    "        x = np.linspace(0, 1, eeg.shape[-1])\n",
    "        x2 = np.linspace(0, 1, self.data_len)\n",
    "        f = interp1d(x, eeg)\n",
    "        eeg = f(x2)\n",
    "        eeg = torch.from_numpy(eeg).float()\n",
    "        ##### 2023 2 13 add preprocess\n",
    "        label = self.labels[i]\n",
    "        print(label)\n",
    "\n",
    "        # Get label\n",
    "        image_name = self.images[self.data[i][\"image\"]]\n",
    "        image_path = os.path.join(self.imagenet, label, image_name)\n",
    "        # print(image_path)\n",
    "        image_raw = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        image = np.array(image_raw) / 255.0\n",
    "        image_raw = self.processor(images=image_raw, return_tensors=\"pt\")\n",
    "        image_raw['pixel_values'] = image_raw['pixel_values'].squeeze(0)\n",
    "\n",
    "\n",
    "        return {'eeg': eeg, 'label': label, 'image': self.image_transform(image), 'image_raw': image_raw}\n",
    "        # Return\n",
    "        # return eeg, label\n",
    "\n",
    "class Splitter:\n",
    "\n",
    "    def __init__(self, dataset, split_path, split_num=0, split_name=\"train\", subject=4):\n",
    "        # Set EEG dataset\n",
    "        self.dataset = dataset\n",
    "        # Load split\n",
    "        loaded = torch.load(split_path)\n",
    "\n",
    "        self.split_idx = loaded[\"splits\"][split_num][split_name]\n",
    "        # # Filter data\n",
    "        self.split_idx = [i for i in self.split_idx if i <= len(self.dataset.data)]\n",
    "        # Compute size\n",
    "\n",
    "        self.size = len(self.split_idx)\n",
    "        self.num_voxels = 440\n",
    "        self.data_len = 512\n",
    "        self.num_samples = 3\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[self.split_idx[i]]\n",
    "\n",
    "\n",
    "def create_EEG_dataset(eeg_signals_path='dataset/eegdataset/eeg_data.pth',\n",
    "            splits_path = 'dataset/eeg_dataset/block_splits_by_image_single.pth',\n",
    "            # splits_path = '../dreamdiffusion/datasets/block_splits_by_image_all.pth',\n",
    "            image_transform=identity, subject = 0):\n",
    "    # if subject == 0:\n",
    "        # splits_path = '../dreamdiffusion/datasets/block_splits_by_image_all.pth'\n",
    "    if isinstance(image_transform, list):\n",
    "        dataset_train = EEGDataset(eeg_signals_path, image_transform[0], subject )\n",
    "        dataset_test = EEGDataset(eeg_signals_path, image_transform[1], subject)\n",
    "    else:\n",
    "        dataset_train = EEGDataset(eeg_signals_path, image_transform, subject)\n",
    "        dataset_test = EEGDataset(eeg_signals_path, image_transform, subject)\n",
    "    split_train = Splitter(dataset_train, split_path = splits_path, split_num = 0, split_name = 'train', subject= subject)\n",
    "    split_test = Splitter(dataset_test, split_path = splits_path, split_num = 0, split_name = 'test', subject = subject)\n",
    "    return (split_train, split_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_EEG_dataset_r(eeg_signals_path='dataset/eegdataset/eeg_data.pth',\n",
    "            # splits_path = '../dreamdiffusion/datasets/block_splits_by_image_single.pth',\n",
    "            splits_path = 'dataset/eeg_dataset/block_splits_by_image_single.pth',\n",
    "            image_transform=identity):\n",
    "    if isinstance(image_transform, list):\n",
    "        dataset_train = EEGDataset_r(eeg_signals_path, image_transform[0])\n",
    "        dataset_test = EEGDataset_r(eeg_signals_path, image_transform[1])\n",
    "    else:\n",
    "        dataset_train = EEGDataset_r(eeg_signals_path, image_transform)\n",
    "        dataset_test = EEGDataset_r(eeg_signals_path, image_transform)\n",
    "    # split_train = Splitter(dataset_train, split_path = splits_path, split_num = 0, split_name = 'train')\n",
    "    # split_test = Splitter(dataset_test, split_path = splits_path, split_num = 0, split_name = 'test')\n",
    "    return (dataset_train,dataset_test)\n",
    "\n",
    "class random_crop:\n",
    "    def __init__(self, size, p):\n",
    "        self.size = size\n",
    "        self.p = p\n",
    "    def __call__(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.RandomCrop(size=(self.size, self.size))(img)\n",
    "        return img\n",
    "def normalize2(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "def channel_last(img):\n",
    "        if img.shape[-1] == 3:\n",
    "            return img\n",
    "        return rearrange(img, 'c h w -> h w c')\n",
    "if __name__ == '__main__':\n",
    "    import scipy.io as scio\n",
    "    import copy\n",
    "    import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNjJZQcAH0Q8",
    "outputId": "22aa504b-691a-4c11-e387-529123d6d41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm==0.5.4 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (0.5.4)\n",
      "Requirement already satisfied: torch>=1.4 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from timm==0.5.4) (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from timm==0.5.4) (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torch>=1.4->timm==0.5.4) (4.12.0)\n",
      "Requirement already satisfied: numpy in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torchvision->timm==0.5.4) (1.24.4)\n",
      "Requirement already satisfied: requests in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torchvision->timm==0.5.4) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from torchvision->timm==0.5.4) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision->timm==0.5.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision->timm==0.5.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision->timm==0.5.4) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests->torchvision->timm==0.5.4) (2024.2.2)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm==0.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/guisi/.local/lib/python3.12/site-packages (0.17.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/guisi/.local/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/guisi/.local/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/guisi/.local/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/miniconda/lib/python3.12/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/guisi/.local/lib/python3.12/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/miniconda/lib/python3.12/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda/lib/python3.12/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/miniconda/lib/python3.12/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/guisi/.local/lib/python3.12/site-packages (from wandb) (2.3.1)\n",
      "Requirement already satisfied: setproctitle in /home/guisi/.local/lib/python3.12/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.12/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/miniconda/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/guisi/.local/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/guisi/.local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "UQJJ2aJFHBzr"
   },
   "outputs": [],
   "source": [
    "#@title MAE for EEG part\n",
    "\n",
    "# utils\n",
    "import math\n",
    "import os\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, length, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_l = np.arange(length, dtype=float)\n",
    "\n",
    "    grid_l = grid_l.reshape([1, length])\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid_l)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Interpolate position embeddings for high-resolution\n",
    "# References:\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches # cls token\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int(pos_embed_checkpoint.shape[-2] - num_extra_tokens)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %d to %d\" % (orig_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, embedding_size).permute(0, 2, 1)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size))\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 1)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed\n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, config):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < config.warmup_epochs:\n",
    "        lr = config.lr * epoch / config.warmup_epochs\n",
    "    else:\n",
    "        lr = config.min_lr + (config.lr - config.min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - config.warmup_epochs) / (config.num_epoch - config.warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def save_model(config, epoch, model, optimizer, loss_scaler, checkpoint_paths):\n",
    "    os.makedirs(checkpoint_paths, exist_ok=True)\n",
    "    to_save = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'scaler': loss_scaler.state_dict(),\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(to_save, os.path.join(checkpoint_paths, 'checkpoint.pth'))\n",
    "\n",
    "\n",
    "def load_model(config, model, checkpoint_path ):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    print(f'Model loaded with {checkpoint_path}')\n",
    "\n",
    "def patchify(imgs, patch_size):\n",
    "    \"\"\"\n",
    "    imgs: (N, 1, num_voxels)\n",
    "    x: (N, L, patch_size)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.ndim == 3 and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], h, p))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x, patch_size):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size)\n",
    "    imgs: (N, 1, num_voxels)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = x.shape[1]\n",
    "\n",
    "    imgs = x.reshape(shape=(x.shape[0], 1, h * p))\n",
    "    return imgs\n",
    "\n",
    "import sys\n",
    "#sys.path.append('../dreamdiffusion/code/')\n",
    "# print(sys.path)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from timm.models.vision_transformer import Block\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbed1D(nn.Module):\n",
    "    \"\"\" 1 Dimensional version of data (fmri voxels) to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, time_len=224, patch_size=1, in_chans=128, embed_dim=256):\n",
    "        super().__init__()\n",
    "        num_patches = time_len // patch_size\n",
    "        self.patch_shape = patch_size\n",
    "        self.time_len = time_len\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "\n",
    "        print(x.shape)\n",
    "        B, C, V = x.shape # batch, channel, voxels\n",
    "        x = x[:, :128, :]\n",
    "        print(x.shape)\n",
    "        B, C, V = x.shape # batch, channel, voxels\n",
    "\n",
    "        # assert V == self.num_voxels, \\\n",
    "        #     f\"Input fmri length ({V}) doesn't match model ({self.num_voxels}).\"\n",
    "        x = self.proj(x).transpose(1, 2).contiguous() # put embed_dim at the last dimension\n",
    "        return x\n",
    "\n",
    "class MAEforEEG(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, time_len=512, patch_size=4, embed_dim=1024, in_chans=128,\n",
    "                 depth=24, num_heads=16, decoder_embed_dim=512,\n",
    "                 decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, focus_range=None, focus_rate=None, img_recon_weight=1.0,\n",
    "                 use_nature_img_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed1D(time_len, patch_size, in_chans, embed_dim)\n",
    "\n",
    "        num_patches = int(time_len / patch_size)\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, in_chans * patch_size, bias=True) # encoder to decoder\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # nature image decoder specifics\n",
    "        if use_nature_img_loss:\n",
    "            self.nature_img_decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "            self.nature_img_mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "            self.nature_img_decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "            self.nature_img_decoder_blocks = nn.ModuleList([\n",
    "                Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "                for i in range(2)])\n",
    "\n",
    "            self.nature_img_decoder_norm = norm_layer(decoder_embed_dim)\n",
    "            self.nature_img_decoder_pred = nn.Sequential(\n",
    "                nn.Conv1d(num_patches, 512, kernel_size=1, stride=1, bias=True),\n",
    "                nn.Linear(decoder_embed_dim, 28*28, bias=True)\n",
    "            )\n",
    "            # --------------------------------------------------------------------------\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.focus_range = focus_range\n",
    "        self.focus_rate = focus_rate\n",
    "        self.img_recon_weight = img_recon_weight\n",
    "        self.use_nature_img_loss = use_nature_img_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        if self.use_nature_img_loss:\n",
    "            nature_img_decoder_pos_embed = get_1d_sincos_pos_embed(self.nature_img_decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "            self.nature_img_decoder_pos_embed.data.copy_(torch.from_numpy(nature_img_decoder_pos_embed).float().unsqueeze(0))\n",
    "            torch.nn.init.normal_(self.nature_img_mask_token, std=.02)\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            torch.nn.init.normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 1, num_voxels)\n",
    "        imgs: [N, chan, T]\n",
    "        x: (N, L, patch_size)\n",
    "        x: [N, chan * 4, T/4]\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        assert imgs.ndim == 3 and imgs.shape[1] % p == 0\n",
    "\n",
    "        # h = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], imgs.shape[1] // p, -1))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size)\n",
    "        imgs: (N, 1, num_voxels)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        h = x.shape[1]\n",
    "\n",
    "        imgs = x.reshape(shape=(x.shape[0], -1, x.shape[2] // p))\n",
    "        return imgs.transpose(1,2)\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        if self.focus_range is not None:\n",
    "            len_mask = L - len_keep\n",
    "            weights = [1-self.focus_rate] * L\n",
    "            weights[self.focus_range[0] // self.patch_size : self.focus_range[1] // self.patch_size\n",
    "                        ] = [self.focus_rate] * (self.focus_range[1] // self.patch_size - self.focus_range[0] // self.patch_size)\n",
    "            weights = torch.tensor(weights).repeat(N, 1).to(x.device)\n",
    "            ids_mask = torch.multinomial(weights, len_mask, replacement=False)\n",
    "\n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        if self.focus_range is not None:\n",
    "            for i in range(N):\n",
    "                noise[i, ids_mask[i,:]] = 1.1  # set mask portion to 1.1\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        # print('encoder embed')\n",
    "        # print(x.shape)\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore = None):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "        # print('decoder embed')\n",
    "        # print(x.shape)\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        # x_ = torch.cat([x, mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "        # x = x_\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "        # x = x + self.decoder_pos_embed[:, 1:, :]\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        # print(x.shape)\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_nature_img_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.nature_img_decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.nature_img_mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.nature_img_decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.nature_img_decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.nature_img_decoder_norm(x)\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "        # predictor projection\n",
    "        # x = x.mean(dim=1, keepdim=True)\n",
    "        x = self.nature_img_decoder_pred(x)\n",
    "        x = x.view(x.shape[0], 512, 28, 28)\n",
    "\n",
    "        return x # n, 512, 28, 28\n",
    "\n",
    "    def forward_nature_img_loss(self, inputs, reconstructions):\n",
    "        loss = ((torch.tanh(inputs) - torch.tanh(reconstructions))**2).mean()\n",
    "        if torch.isnan(reconstructions).sum():\n",
    "            print('nan in reconstructions')\n",
    "        if torch.isnan(inputs).sum():\n",
    "            print('nan in inputs')\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 1, num_voxels]\n",
    "        imgs: [N, chan, T]\n",
    "        pred: [N, L, p]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "        imgs = imgs.transpose(1,2)\n",
    "        target = self.patchify(imgs)\n",
    "        # target = imgs.transpose(1,2)\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "        # loss = loss.mean()\n",
    "        loss = (loss * mask).sum() / mask.sum()  if mask.sum() != 0 else (loss * mask).sum() # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, img_features=None, valid_idx=None, mask_ratio=0.75):\n",
    "        # latent = self.forward_encoder(imgs, mask_ratio)\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "            # print(x)\n",
    "        # print(latent.shape)\n",
    "        # # print(mask)\n",
    "        # print(mask.shape)\n",
    "        # # print(ids_restore)\n",
    "        # print(ids_restore.shape)\n",
    "\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p]\n",
    "        # pred = self.forward_decoder(latent)  # [N, L, p]\n",
    "        # pred = pred\n",
    "        # print(pred.shape)\n",
    "        # mask=None\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        # print(self.unpatchify(pred.transpose(1,2)).shape)\n",
    "\n",
    "        if self.use_nature_img_loss and img_features is not None:\n",
    "            # valid_idx = torch.nonzero(nature_image.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
    "            if len(valid_idx) != 0:\n",
    "                nature_image_recon = self.forward_nature_img_decoder(latent[valid_idx], ids_restore[valid_idx])\n",
    "                loss_nature_image_recon = self.forward_nature_img_loss(img_features, nature_image_recon)\n",
    "                if torch.isnan(loss_nature_image_recon).sum():\n",
    "                    print(loss_nature_image_recon)\n",
    "                    print(\"loss_nature_image_recon is nan\")\n",
    "\n",
    "                loss = loss + self.img_recon_weight*loss_nature_image_recon\n",
    "\n",
    "        return loss, pred, mask\n",
    "\n",
    "class eeg_encoder(nn.Module):\n",
    "    def __init__(self, time_len=512, patch_size=4, embed_dim=1024, in_chans=128,\n",
    "                 depth=24, num_heads=16, mlp_ratio=1., norm_layer=nn.LayerNorm, global_pool=False):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed1D(time_len, patch_size, in_chans, embed_dim)\n",
    "\n",
    "        num_patches = int(time_len / patch_size)\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.global_pool = global_pool\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            torch.nn.init.normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        # print(x.shape)\n",
    "        # print(self.pos_embed[:, 1:, :].shape)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        # print(x.shape)\n",
    "        if self.global_pool:\n",
    "            x = x.mean(dim=1, keepdim=True)\n",
    "        # print(x.shape)\n",
    "        x = self.norm(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        if imgs.ndim == 2:\n",
    "            imgs = torch.unsqueeze(imgs, dim=0)  # N, n_seq, embed_dim\n",
    "        latent = self.forward_encoder(imgs) # N, n_seq, embed_dim\n",
    "        return latent # N, n_seq, embed_dim\n",
    "\n",
    "    def load_checkpoint(self, state_dict):\n",
    "        if self.global_pool:\n",
    "            state_dict = {k: v for k, v in state_dict.items() if ('mask_token' not in k and 'norm' not in k)}\n",
    "        else:\n",
    "            state_dict = {k: v for k, v in state_dict.items() if ('mask_token' not in k)}\n",
    "        interpolate_pos_embed(self, state_dict)\n",
    "\n",
    "        m, u = self.load_state_dict(state_dict, strict=False)\n",
    "        print('missing keys:', u)\n",
    "        print('unexpected keys:', m)\n",
    "        return\n",
    "\n",
    "class classify_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.Conv1d(128, 1, 1, stride=1)#nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Linear(1024, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class mapping(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.Conv1d(128, 1, 1, stride=1)#nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Linear(1024, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    # mae = MAEforEEG(time_len=512)\n",
    "    # mae.forward_encoder(input,0.5)\n",
    "    # print(encoder)\n",
    "    #input = torch.randn(2,128,512)\n",
    "    # loss = mae(input)\n",
    "    # print(input[:,:,0:4])\n",
    "    # print(input.transpose(1,2)[:,0:4,:])\n",
    "    # print(mae.patchify(input.transpose(1,2))[:,0,:])\n",
    "    # print(loss)\n",
    "    #encoder = eeg_encoder()\n",
    "    #out = encoder(input)\n",
    "    #print(out.shape)\n",
    "    #clss = classify_network2()\n",
    "    #pre_cls = clss(out)\n",
    "    #print(pre_cls.shape)\n",
    "    # x, mask, ids_restore = mae.forward_encoder(input,0.75)\n",
    "    # # pred = mae.forward_decoder(latent, ids_restore)\n",
    "\n",
    "    # # print(x)\n",
    "    # print(x.shape)\n",
    "    # # print(mask)\n",
    "    # print(mask.shape)\n",
    "    # # print(ids_restore)\n",
    "    # print(ids_restore.shape)\n",
    "    # pred = mae.forward_decoder(x, ids_restore)\n",
    "\n",
    "    # # print(pred)\n",
    "    # print(pred.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # import sys\n",
    "    # sys.path.append('..')\n",
    "    # print(sys.path)\n",
    "    # encoder = eeg_encoder2(num_voxels=440)\n",
    "    # decoder = eeg_decoder2(num_voxels=440)\n",
    "    # cond = cond_stage_model(encoder)\n",
    "    # clss = classify_network2()\n",
    "\n",
    "    # print(encoder)\n",
    "    # lstm = Model()\n",
    "    #128*1024 \n",
    "    # input = torch.randn(1,128,128)\n",
    "    # # out = encoder(input)\n",
    "    # out, latent_crossattn = cond(input)\n",
    "    # print(out.shape)\n",
    "    # print(latent_crossattn.shape)\n",
    "    # pre_cls = clss(latent_crossattn)\n",
    "    # print(pre_cls.shape)\n",
    "    # recon = decoder(latent_crossattn)\n",
    "    # print(recon.shape)\n",
    "    # out = lstm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "Cf55A0hwUd_r"
   },
   "outputs": [],
   "source": [
    "#@title Configs\n",
    "class Config_MBM_finetune: # back compatibility\n",
    "    pass\n",
    "\n",
    "class Config_MAE_fMRI: # back compatibility\n",
    "    pass\n",
    "\n",
    "class Config_MBM_EEG(Config_MAE_fMRI):\n",
    "    # configs for fmri_pretrain.py\n",
    "    def __init__(self):\n",
    "    # --------------------------------------------\n",
    "    # MAE for fMRI\n",
    "        # Training Parameters\n",
    "        self.lr = 2.5e-4\n",
    "        self.min_lr = 0.\n",
    "        self.weight_decay = 0.05\n",
    "        self.num_epoch = 700\n",
    "        self.warmup_epochs = 40\n",
    "        self.batch_size = 100\n",
    "        self.clip_grad = 0.8\n",
    "\n",
    "        # Model Parameters\n",
    "        self.mask_ratio = 0.15\n",
    "        self.patch_size = 4 #  1\n",
    "        self.embed_dim = 1024 #256 # has to be a multiple of num_heads\n",
    "        self.decoder_embed_dim = 512 #128\n",
    "        self.depth = 24\n",
    "        self.num_heads = 16\n",
    "        self.decoder_num_heads = 16\n",
    "        self.mlp_ratio = 1.0\n",
    "\n",
    "        # Project setting\n",
    "        self.root_path = '/eegdataset2'\n",
    "        self.output_path = 'dataset/eegdataset2/exp'\n",
    "        self.seed = 2022\n",
    "        self.roi = 'VC'\n",
    "        self.aug_times = 1\n",
    "        self.num_sub_limit = None\n",
    "        self.include_hcp = True\n",
    "        self.include_kam = True\n",
    "        self.accum_iter = 1\n",
    "\n",
    "        self.use_nature_img_loss = False\n",
    "        self.img_recon_weight = 0.5\n",
    "        self.focus_range = None # [0, 1500] # None to disable it\n",
    "        self.focus_rate = 0.6\n",
    "\n",
    "        # distributed training\n",
    "        self.local_rank = 0\n",
    "\n",
    "\n",
    "class Config_EEG_finetune(Config_MBM_finetune):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Project setting\n",
    "        self.root_path = 'dataset/eegdataset2'\n",
    "        self.crop_ratio = 0.2\n",
    "        # self.root_path = '.'\n",
    "        self.output_path = 'dataset/eegdataset2/exp'\n",
    "\n",
    "        self.eeg_signals_path = 'dataset/eeg_dataset2/eeg_5_95_std.pth'\n",
    "        self.splits_path = 'dataset/eeg_dataset2/block_splits_by_image_all.pth'\n",
    "\n",
    "        self.dataset = 'EEG'\n",
    "        self.pretrain_mbm_path = 'dataset/eegdataset2/eeg_pretrain/checkpoint.pth'\n",
    "\n",
    "        self.include_nonavg_test = True\n",
    "\n",
    "        self.lr = 2.5e-4\n",
    "        self.min_lr = 0.\n",
    "        self.weight_decay = 0.05\n",
    "        self.num_epoch = 700\n",
    "        self.warmup_epochs = 40\n",
    "        self.batch_size = 100\n",
    "        self.clip_grad = 0.8\n",
    "\n",
    "        # Model Parameters\n",
    "        self.mask_ratio = 0.15\n",
    "        self.patch_size = 4 #  1\n",
    "        self.embed_dim = 1024 #256 # has to be a multiple of num_heads\n",
    "        self.decoder_embed_dim = 512 #128\n",
    "        self.depth = 24\n",
    "        self.num_heads = 16\n",
    "        self.decoder_num_heads = 16\n",
    "        self.mlp_ratio = 1.0\n",
    "\n",
    "        # Project setting\n",
    "        self.seed = 2022\n",
    "        self.roi = 'VC'\n",
    "        self.aug_times = 1\n",
    "        self.num_sub_limit = None\n",
    "        self.include_hcp = True\n",
    "        self.include_kam = True\n",
    "        self.accum_iter = 1\n",
    "\n",
    "        self.use_nature_img_loss = False\n",
    "        self.img_recon_weight = 0.5\n",
    "        self.focus_range = None # [0, 1500] # None to disable it\n",
    "        self.focus_rate = 0.6\n",
    "\n",
    "        # distributed training\n",
    "        self.local_rank = 0\n",
    "        # Training Parameters\n",
    "        self.lr = 5.3e-5\n",
    "        self.weight_decay = 0.05\n",
    "        self.num_epoch = 15\n",
    "        self.batch_size = 16 if self.dataset == 'GOD' else 4\n",
    "        self.mask_ratio = 0.5\n",
    "        self.accum_iter = 1\n",
    "        self.clip_grad = 0.8\n",
    "        self.warmup_epochs = 2\n",
    "        self.min_lr = 0.\n",
    "        self.use_nature_img_loss = False\n",
    "        self.img_recon_weight = 0.5\n",
    "        self.focus_range = None # [0, 1500] # None to disable it\n",
    "        self.focus_rate = 0.6\n",
    "\n",
    "\n",
    "        # distributed training\n",
    "        self.local_rank = 0\n",
    "        self.crop_ratio = 0.2\n",
    "        self.eval_avg = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "B7BC-DsNUbFC"
   },
   "outputs": [],
   "source": [
    "#@title Trainer util code\n",
    "import math, sys\n",
    "import torch\n",
    "from math import inf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device, epoch,\n",
    "                        loss_scaler, log_writer=None, config=None, start_time=None, model_without_ddp=None,\n",
    "                        img_feature_extractor=None, preprocess=None):\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = []\n",
    "    total_cor = []\n",
    "    accum_iter = config.accum_iter\n",
    "    for data_iter_step, (data_dcit) in enumerate(data_loader):\n",
    "\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        # print(data_iter_step)\n",
    "        # print(len(data_loader))\n",
    "\n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, config)\n",
    "        samples = data_dcit['eeg']\n",
    "\n",
    "        img_features = None\n",
    "        valid_idx = None\n",
    "        if img_feature_extractor is not None:\n",
    "            images = data_dcit['image']\n",
    "            valid_idx = torch.nonzero(images.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
    "            img_feature_extractor.eval()\n",
    "            with torch.no_grad():\n",
    "                img_features = img_feature_extractor(preprocess(images[valid_idx]).to(device))['layer2']\n",
    "        samples = samples.to(device)\n",
    "        # img_features = img_features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            loss, pred, _ = model(samples, img_features, valid_idx=valid_idx, mask_ratio=config.mask_ratio)\n",
    "        # loss.backward()\n",
    "        # norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\n",
    "        # optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training at step {data_iter_step} epoch {epoch}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # loss /= accum_iter\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=config.clip_grad)\n",
    "\n",
    "        # if (data_iter_step + 1) % accum_iter == 0:\n",
    "        # cal the cor\n",
    "        pred = pred.to('cpu').detach()\n",
    "        samples = samples.to('cpu').detach()\n",
    "        # pred = pred.transpose(1,2) #model_without_ddp.unpatchify(pred)\n",
    "        pred = model_without_ddp.unpatchify(pred)\n",
    "        # print(pred.shape)\n",
    "        # print(samples.shape)\n",
    "        # for p, s in zip(pred, samples):\n",
    "        #     print(p[0], s[0])\n",
    "        #     print(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))\n",
    "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0)))\n",
    "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))[0,1])\n",
    "\n",
    "        cor = torch.mean(torch.tensor([torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))[0,1] for p, s in zip(pred, samples)])).item()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss.append(loss_value)\n",
    "        total_cor.append(cor)\n",
    "        if device == torch.device('cuda:0'):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print('train_loss_step:', np.mean(total_loss), 'lr:', lr, 'cor', np.mean(total_cor))\n",
    "\n",
    "    if log_writer is not None:\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_writer.log('train_loss_step', np.mean(total_loss), step=epoch)\n",
    "        log_writer.log('lr', lr, step=epoch)\n",
    "        log_writer.log('cor', np.mean(total_cor), step=epoch)\n",
    "        if start_time is not None:\n",
    "            log_writer.log('time (min)', (time.time() - start_time)/60.0, step=epoch)\n",
    "    if config.local_rank == 0:\n",
    "        print(f'[Epoch {epoch}] loss: {np.mean(total_loss)}')\n",
    "\n",
    "    return np.mean(total_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQb7LGC-WGYi",
    "outputId": "3cd8a9d6-386e-4f16-cb49-7e3e8f5166c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (0.17.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (2.3.1)\n",
      "Requirement already satisfied: setproctitle in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: typing-extensions in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from wandb) (4.12.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/guisi/.local/lib/python3.12/site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/guisi/.local/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/guisi/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/guisi/.local/lib/python3.12/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/guisi/.local/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/guisi/.local/lib/python3.12/site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.12/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/guisi/.local/lib/python3.12/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/guisi/.local/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the local_rank environment variable\n",
    "os.environ[\"RANK\"] = \"0\"  # Set the appropriate value for each process\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"172.17.0.1\"\n",
    "os.environ['MASTER_PORT'] = '49152'  # Replace with your master port\n",
    "os.environ['NNODES'] = '1'  # Number of nodes\n",
    "os.environ['NODE_RANK'] = '0'  # Node rank (this node's rank)\n",
    "os.environ['WORLD_SIZE'] = '1'  # Node rank (this node's rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "8RTMC7_zC3cq",
    "outputId": "d8509e6b-84c0-4818-8ba7-a3e0e8bc7607"
   },
   "outputs": [],
   "source": [
    "#@title Pretrain EEG state\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import argparse\n",
    "import time\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import copy\n",
    "\n",
    "os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "os.environ['WANDB_DIR'] = \".\"\n",
    "\n",
    "class wandb_logger:\n",
    "    def __init__(self, config):\n",
    "        wandb.init(\n",
    "                    project=\"dreamdiffusion\",\n",
    "                    anonymous=\"allow\",\n",
    "                    group='stageA_sc-mbm',\n",
    "                    config=config,\n",
    "                    reinit=True)\n",
    "\n",
    "        self.config = config\n",
    "        self.step = None\n",
    "\n",
    "    def log(self, name, data, step=None):\n",
    "        if step is None:\n",
    "            wandb.log({name: data})\n",
    "        else:\n",
    "            wandb.log({name: data}, step=step)\n",
    "            self.step = step\n",
    "\n",
    "    def watch_model(self, *args, **kwargs):\n",
    "        wandb.watch(*args, **kwargs)\n",
    "\n",
    "    def log_image(self, name, fig):\n",
    "        if self.step is None:\n",
    "            wandb.log({name: wandb.Image(fig)})\n",
    "        else:\n",
    "            wandb.log({name: wandb.Image(fig)}, step=self.step)\n",
    "\n",
    "    def finish(self):\n",
    "        wandb.finish(quiet=True)\n",
    "\n",
    "    def finalize(self,  *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def save_dir(self,  *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def log_dir(self,  *args, **kwargs):\n",
    "        pass\n",
    "    def log_graph(self,  *args, **kwargs):\n",
    "        pass\n",
    "    def log_hyperparams(self,  *args, **kwargs):\n",
    "        pass\n",
    "    def save(self,  *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MBM pre-training for fMRI', add_help=False)\n",
    "\n",
    "    # Training Parameters\n",
    "    parser.add_argument('--lr', type=float)\n",
    "    parser.add_argument('--weight_decay', type=float)\n",
    "    parser.add_argument('--num_epoch', type=int)\n",
    "    parser.add_argument('--batch_size', type=int)\n",
    "\n",
    "    # Model Parameters\n",
    "    parser.add_argument('--mask_ratio', type=float)\n",
    "    parser.add_argument('--patch_size', type=int)\n",
    "    parser.add_argument('--embed_dim', type=int)\n",
    "    parser.add_argument('--decoder_embed_dim', type=int)\n",
    "    parser.add_argument('--depth', type=int)\n",
    "    parser.add_argument('--num_heads', type=int)\n",
    "    parser.add_argument('--decoder_num_heads', type=int)\n",
    "    parser.add_argument('--mlp_ratio', type=float)\n",
    "\n",
    "    # Project setting\n",
    "    parser.add_argument('--root_path', type=str)\n",
    "    parser.add_argument('--seed', type=str)\n",
    "    parser.add_argument('--roi', type=str)\n",
    "    parser.add_argument('--aug_times', type=int)\n",
    "    parser.add_argument('--num_sub_limit', type=int)\n",
    "\n",
    "    parser.add_argument('--include_hcp', type=bool)\n",
    "    parser.add_argument('--include_kam', type=bool)\n",
    "\n",
    "    parser.add_argument('--use_nature_img_loss', type=bool)\n",
    "    parser.add_argument('--img_recon_weight', type=float)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--local_rank', type=int)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def create_readme(config, path):\n",
    "    print(config.__dict__)\n",
    "    with open(os.path.join(path, 'README.md'), 'w+') as f:\n",
    "        print(config.__dict__, file=f)\n",
    "\n",
    "def fmri_transform(x, sparse_rate=0.2):\n",
    "    # x: 1, num_voxels\n",
    "    x_aug = copy.deepcopy(x)\n",
    "    idx = np.random.choice(x.shape[0], int(x.shape[0]*sparse_rate), replace=False)\n",
    "    x_aug[idx] = 0\n",
    "    return torch.FloatTensor(x_aug)\n",
    "\n",
    "def main(config):\n",
    "    # print('num of gpu:')\n",
    "    # print(torch.cuda.device_count())\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        torch.cuda.set_device(config.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    output_path = os.path.join(config.root_path, 'results', 'eeg_pretrain',  '%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")))\n",
    "    config.output_path = output_path\n",
    "    # logger = wandb_logger(config) if config.local_rank == 0 else None\n",
    "    logger = None\n",
    "\n",
    "    if config.local_rank == 0:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        create_readme(config, output_path)\n",
    "\n",
    "    device = torch.device(f'cuda:{config.local_rank}') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "\n",
    "    # create dataset and dataloader\n",
    "    dataset_pretrain = eeg_pretrain_dataset(path='dataset/eegdataset/eeg/', roi=config.roi, patch_size=config.patch_size,\n",
    "                transform=fmri_transform, aug_times=config.aug_times, num_sub_limit=config.num_sub_limit,\n",
    "                include_kam=config.include_kam, include_hcp=config.include_hcp)\n",
    "\n",
    "    print(f'Dataset size: {len(dataset_pretrain)}\\n Time len: {dataset_pretrain.data_len}')\n",
    "    sampler = torch.utils.data.DistributedSampler(dataset_pretrain, rank=config.local_rank) if torch.cuda.device_count() > 1 else None\n",
    "\n",
    "    dataloader_eeg = DataLoader(dataset_pretrain, batch_size=75, sampler=sampler,\n",
    "                shuffle=(sampler is None), pin_memory=True)\n",
    "\n",
    "    # create model\n",
    "    config.time_len=dataset_pretrain.data_len\n",
    "    model = MAEforEEG(time_len=dataset_pretrain.data_len, patch_size=config.patch_size, embed_dim=config.embed_dim,\n",
    "                    decoder_embed_dim=config.decoder_embed_dim, depth=config.depth,\n",
    "                    num_heads=config.num_heads, decoder_num_heads=config.decoder_num_heads, mlp_ratio=config.mlp_ratio,\n",
    "                    focus_range=config.focus_range, focus_rate=config.focus_rate,\n",
    "                    img_recon_weight=config.img_recon_weight, use_nature_img_loss=config.use_nature_img_loss)\n",
    "    model.to(device)\n",
    "    model_without_ddp = model\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        model = DistributedDataParallel(model, device_ids=[config.local_rank], output_device=config.local_rank, find_unused_parameters=config.use_nature_img_loss)\n",
    "\n",
    "    param_groups = optim_factory.add_weight_decay(model, config.weight_decay)\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=config.lr, betas=(0.9, 0.95))\n",
    "    print(optimizer)\n",
    "    loss_scaler = NativeScalerWithGradNormCount()\n",
    "\n",
    "    start_epoch = 0\n",
    "    checkpoint_path = '/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth'\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}, starting from epoch {start_epoch}\")\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.watch_model(model,log='all', log_freq=1000)\n",
    "\n",
    "    cor_list = []\n",
    "    start_time = time.time()\n",
    "    print('Start Training the EEG MAE ... ...')\n",
    "    img_feature_extractor = None\n",
    "    preprocess = None\n",
    "    if config.use_nature_img_loss:\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        from torchvision.models.feature_extraction import create_feature_extractor\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        preprocess = weights.transforms()\n",
    "        m = resnet50(weights=weights)\n",
    "        img_feature_extractor = create_feature_extractor(m, return_nodes={f'layer2': 'layer2'}).to(device).eval()\n",
    "        for param in img_feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    for ep in range(start_epoch, config.num_epoch):\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            sampler.set_epoch(ep) # to shuffle the data at every epoch\n",
    "        cor = train_one_epoch(model, dataloader_eeg, optimizer, device, ep, loss_scaler, logger, config, start_time, model_without_ddp,\n",
    "                            img_feature_extractor, preprocess)\n",
    "        cor_list.append(cor)\n",
    "        if (ep == 500):\n",
    "            print('Saving the model...');\n",
    "            save_model(config, ep, model_without_ddp, optimizer, loss_scaler, os.path.join(output_path,'checkpoints'))\n",
    "            # plot figures\n",
    "            plot_recon_figures(model, device, dataset_pretrain, output_path, 5, config, logger, model_without_ddp)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "    if logger is not None:\n",
    "        logger.log('max cor', np.max(cor_list), step=config.num_epoch-1)\n",
    "        logger.finish()\n",
    "    return\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_recon_figures(model, device, dataset, output_path, num_figures = 5, config=None, logger=None, model_without_ddp=None):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(num_figures, 3, figsize=(30,15))\n",
    "    fig.tight_layout()\n",
    "    axs[0,0].set_title('Ground-truth')\n",
    "    axs[0,1].set_title('Masked Ground-truth')\n",
    "    axs[0,2].set_title('Reconstruction')\n",
    "\n",
    "    for ax in axs:\n",
    "        sample = next(iter(dataloader))['eeg']\n",
    "        sample = sample.to(device)\n",
    "        _, pred, mask = model(sample, mask_ratio=config.mask_ratio)\n",
    "        # sample_with_mask = model_without_ddp.patchify(sample.transpose(1,2))[0].to('cpu').numpy().reshape(-1, model_without_ddp.patch_size)\n",
    "        sample_with_mask = sample.to('cpu').squeeze(0)[0].numpy().reshape(-1, model_without_ddp.patch_size)\n",
    "        # pred = model_without_ddp.unpatchify(pred.transpose(1,2)).to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
    "        # sample = sample.to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
    "        pred = model_without_ddp.unpatchify(pred).to('cpu').squeeze(0)[0].numpy()\n",
    "        # pred = model_without_ddp.unpatchify(model_without_ddp.patchify(sample.transpose(1,2))).to('cpu').squeeze(0)[0].numpy()\n",
    "        sample = sample.to('cpu').squeeze(0)[0].numpy()\n",
    "        mask = mask.to('cpu').numpy().reshape(-1)\n",
    "\n",
    "        cor = np.corrcoef([pred, sample])[0,1]\n",
    "\n",
    "        x_axis = np.arange(0, sample.shape[-1])\n",
    "        # groundtruth\n",
    "        ax[0].plot(x_axis, sample)\n",
    "        # groundtruth with mask\n",
    "        s = 0\n",
    "        for x, m in zip(sample_with_mask,mask):\n",
    "            if m == 0:\n",
    "                ax[1].plot(x_axis[s:s+len(x)], x, color='#1f77b4')\n",
    "            s += len(x)\n",
    "        # pred\n",
    "        ax[2].plot(x_axis, pred)\n",
    "        ax[2].set_ylabel('cor: %.4f'%cor, weight = 'bold')\n",
    "        ax[2].yaxis.set_label_position(\"right\")\n",
    "\n",
    "    fig_name = 'reconst-%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
    "    fig.savefig(os.path.join(output_path, f'{fig_name}.png'))\n",
    "    if logger is not None:\n",
    "        logger.log_image('reconst', fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_recon_figures2(model, device, dataset, output_path, num_figures = 5, config=None, logger=None, model_without_ddp=None):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(num_figures, 2, figsize=(20,15))\n",
    "    fig.tight_layout()\n",
    "    axs[0,0].set_title('Ground-truth')\n",
    "    # axs[0,1].set_title('Masked Ground-truth')\n",
    "    axs[0,1].set_title('Reconstruction')\n",
    "\n",
    "    for ax in axs:\n",
    "        sample = next(iter(dataloader))['eeg']\n",
    "        sample = sample.to(device)\n",
    "        _, pred, mask = model(sample, mask_ratio=config.mask_ratio)\n",
    "        # sample_with_mask = model_without_ddp.patchify(sample.transpose(1,2))[0].to('cpu').numpy().reshape(-1, model_without_ddp.patch_size)\n",
    "        sample_with_mask = sample.to('cpu').squeeze(0)[0].numpy().reshape(-1, model_without_ddp.patch_size)\n",
    "        # pred = model_without_ddp.unpatchify(pred.transpose(1,2)).to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
    "        # sample = sample.to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
    "        pred = model_without_ddp.unpatchify(pred).to('cpu').squeeze(0)[0].numpy()\n",
    "        # pred = model_without_ddp.unpatchify(model_without_ddp.patchify(sample.transpose(1,2))).to('cpu').squeeze(0)[0].numpy()\n",
    "        sample = sample.to('cpu').squeeze(0)[0].numpy()\n",
    "        cor = np.corrcoef([pred, sample])[0,1]\n",
    "\n",
    "        x_axis = np.arange(0, sample.shape[-1])\n",
    "        # groundtruth\n",
    "        ax[0].plot(x_axis, sample)\n",
    "\n",
    "        ax[1].plot(x_axis, pred)\n",
    "        ax[1].set_ylabel('cor: %.4f'%cor, weight = 'bold')\n",
    "        ax[1].yaxis.set_label_position(\"right\")\n",
    "\n",
    "    fig_name = 'reconst-%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
    "    fig.savefig(os.path.join(output_path, f'{fig_name}.png'))\n",
    "    if logger is not None:\n",
    "        logger.log_image('reconst', fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def update_config(args, config):\n",
    "    for attr in config.__dict__:\n",
    "        if hasattr(args, attr):\n",
    "            if getattr(args, attr) != None:\n",
    "                setattr(config, attr, getattr(args, attr))\n",
    "    return config\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     config = Config_EEG_finetune()\n",
    "#     # Set local_rank based on your distributed training setup\n",
    "#     config.local_rank = int(os.environ.get(\"RANK\", 0))\n",
    "#     config = update_config(\"\", config)\n",
    "#     main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# STOP\n",
    "!torchrun secondmain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/miniconda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/opt/miniconda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/miniconda/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n    self._run_once()\n  File \"/opt/miniconda/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n    handle._run()\n  File \"/opt/miniconda/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n    res = shell.run_cell(\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_3289/1877741278.py\", line 5, in <module>\n    import torchvision.transforms as transforms\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torchvision/__init__.py\", line 5, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:306\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:174\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 174\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:430\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:448\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m config\u001b[38;5;241m.\u001b[39mlocal_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m update_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, config)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 102\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(config):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# print('num of gpu:')\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# print(torch.cuda.device_count())\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m         torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39minit_process_group(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    104\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mroot_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg_pretrain\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:399\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    397\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_setDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:312\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(orig_traceback)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             )\n\u001b[0;32m--> 312\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/miniconda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/opt/miniconda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/miniconda/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n    self._run_once()\n  File \"/opt/miniconda/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n    handle._run()\n  File \"/opt/miniconda/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n    res = shell.run_cell(\n  File \"/opt/miniconda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_3289/1877741278.py\", line 5, in <module>\n    import torchvision.transforms as transforms\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torchvision/__init__.py\", line 5, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/guisi/.local/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#STOP\n",
    "if __name__ == '__main__':\n",
    "    config = Config_EEG_finetune()\n",
    "    # Set local_rank based on your distributed training setup\n",
    "    config.local_rank = int(os.environ.get(\"RANK\", 0))\n",
    "    config = update_config(\"\", config)\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "KUSUi8N23Y57"
   },
   "outputs": [],
   "source": [
    "#@title utils for generative part\n",
    "import importlib\n",
    "import torch.nn\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    print(string)\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        if config == '__is_first_stage__':\n",
    "            return None\n",
    "        elif config == \"__is_unconditional__\":\n",
    "            return None\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n",
    "    # select alphas for computing the variance schedule\n",
    "    alphas = alphacums[ddim_timesteps]\n",
    "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
    "\n",
    "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
    "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
    "    if verbose:\n",
    "        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n",
    "        print(f'For the chosen value of eta, which is {eta}, '\n",
    "              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n",
    "    return sigmas, alphas, alphas_prev\n",
    "\n",
    "\n",
    "def make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n",
    "    if ddim_discr_method == 'uniform':\n",
    "        c = num_ddpm_timesteps // num_ddim_timesteps\n",
    "        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
    "    elif ddim_discr_method == 'quad':\n",
    "        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n",
    "    else:\n",
    "        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
    "\n",
    "    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n",
    "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "    steps_out = ddim_timesteps + 1\n",
    "    if verbose:\n",
    "        print(f'Selected timesteps for ddim sampler: {steps_out}')\n",
    "    return steps_out\n",
    "\n",
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "pVCHtMPV3f5w",
    "outputId": "bb5116d6-d710-444b-c5bc-57e4ef0116fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: omegaconf==2.1.1 in /home/guisi/.local/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: tqdm==4.64.0 in /home/guisi/.local/lib/python3.12/site-packages (4.64.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/guisi/.local/lib/python3.12/site-packages (from omegaconf==2.1.1) (4.8)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/miniconda/lib/python3.12/site-packages (from omegaconf==2.1.1) (6.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install omegaconf==2.1.1 tqdm==4.64.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytorch-lightning in /home/guisi/.local/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (1.26.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (2.3.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (4.64.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/miniconda/lib/python3.12/site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /home/guisi/.local/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.12/site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/miniconda/lib/python3.12/site-packages (from pytorch-lightning) (4.10.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (0.11.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/guisi/.local/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.12/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: filelock in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.86)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/guisi/.local/lib/python3.12/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/miniconda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "2o8tbNLw38vG"
   },
   "outputs": [],
   "source": [
    "#@title Plm sampler\n",
    "\"\"\"SAMPLING ONLY.\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class PLMSSampler(object):\n",
    "    def __init__(self, model, schedule=\"linear\", **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.ddpm_num_timesteps = model.num_timesteps\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def register_buffer(self, name, attr):\n",
    "        if type(attr) == torch.Tensor:\n",
    "            if attr.device != torch.device(\"cuda\"):\n",
    "                attr = attr.to(torch.device(\"cuda\"))\n",
    "        setattr(self, name, attr)\n",
    "\n",
    "    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n",
    "        if ddim_eta != 0:\n",
    "            raise ValueError('ddim_eta must be 0 for PLMS')\n",
    "        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n",
    "                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n",
    "        alphas_cumprod = self.model.alphas_cumprod\n",
    "        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n",
    "        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(self.model.betas))\n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
    "        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n",
    "\n",
    "        # ddim sampling parameters\n",
    "        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n",
    "                                                                                   ddim_timesteps=self.ddim_timesteps,\n",
    "                                                                                   eta=ddim_eta,verbose=verbose)\n",
    "        self.register_buffer('ddim_sigmas', ddim_sigmas)\n",
    "        self.register_buffer('ddim_alphas', ddim_alphas)\n",
    "        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n",
    "        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n",
    "        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n",
    "            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n",
    "                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n",
    "        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self,\n",
    "               S,\n",
    "               batch_size,\n",
    "               shape,\n",
    "               conditioning=None,\n",
    "               callback=None,\n",
    "               normals_sequence=None,\n",
    "               img_callback=None,\n",
    "               quantize_x0=False,\n",
    "               eta=0.,\n",
    "               mask=None,\n",
    "               x0=None,\n",
    "               temperature=1.,\n",
    "               noise_dropout=0.,\n",
    "               score_corrector=None,\n",
    "               corrector_kwargs=None,\n",
    "               verbose=True,\n",
    "               x_T=None,\n",
    "               log_every_t=100,\n",
    "               unconditional_guidance_scale=1.,\n",
    "               unconditional_conditioning=None,\n",
    "               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n",
    "               **kwargs\n",
    "               ):\n",
    "        if conditioning is not None:\n",
    "            if isinstance(conditioning, dict):\n",
    "                cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n",
    "                if cbs != batch_size:\n",
    "                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n",
    "            else:\n",
    "                if conditioning.shape[0] != batch_size:\n",
    "                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n",
    "\n",
    "        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n",
    "        # sampling\n",
    "        C, H, W = shape\n",
    "        size = (batch_size, C, H, W)\n",
    "        print(f'Data shape for PLMS sampling is {size}')\n",
    "\n",
    "        samples, intermediates = self.plms_sampling(conditioning, size,\n",
    "                                                    callback=callback,\n",
    "                                                    img_callback=img_callback,\n",
    "                                                    quantize_denoised=quantize_x0,\n",
    "                                                    mask=mask, x0=x0,\n",
    "                                                    ddim_use_original_steps=False,\n",
    "                                                    noise_dropout=noise_dropout,\n",
    "                                                    temperature=temperature,\n",
    "                                                    score_corrector=score_corrector,\n",
    "                                                    corrector_kwargs=corrector_kwargs,\n",
    "                                                    x_T=x_T,\n",
    "                                                    log_every_t=log_every_t,\n",
    "                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                    unconditional_conditioning=unconditional_conditioning,\n",
    "                                                    **kwargs\n",
    "                                                    )\n",
    "        return samples, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plms_sampling(self, cond, shape,\n",
    "                      x_T=None, ddim_use_original_steps=False,\n",
    "                      callback=None, timesteps=None, quantize_denoised=False,\n",
    "                      mask=None, x0=None, img_callback=None, log_every_t=100,\n",
    "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
    "                      unconditional_guidance_scale=1., unconditional_conditioning=None, generator=None):\n",
    "        device = self.model.betas.device\n",
    "        b = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=device, generator=generator)\n",
    "        else:\n",
    "            img = x_T\n",
    "\n",
    "        if timesteps is None:\n",
    "            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n",
    "        elif timesteps is not None and not ddim_use_original_steps:\n",
    "            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n",
    "            timesteps = self.ddim_timesteps[:subset_end]\n",
    "\n",
    "        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n",
    "        time_range = list(reversed(range(0,timesteps))) if ddim_use_original_steps else np.flip(timesteps)\n",
    "        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n",
    "        print(f\"Running PLMS Sampling with {total_steps} timesteps\")\n",
    "\n",
    "        iterator = tqdm(time_range, desc='PLMS Sampler', total=total_steps)\n",
    "        old_eps = []\n",
    "\n",
    "        for i, step in enumerate(iterator):\n",
    "            index = total_steps - i - 1\n",
    "            ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
    "            ts_next = torch.full((b,), time_range[min(i + 1, len(time_range) - 1)], device=device, dtype=torch.long)\n",
    "\n",
    "            if mask is not None:\n",
    "                assert x0 is not None\n",
    "                img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            outs = self.p_sample_plms(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n",
    "                                      quantize_denoised=quantize_denoised, temperature=temperature,\n",
    "                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n",
    "                                      corrector_kwargs=corrector_kwargs,\n",
    "                                      unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                      unconditional_conditioning=unconditional_conditioning,\n",
    "                                      old_eps=old_eps, t_next=ts_next)\n",
    "            img, pred_x0, e_t = outs\n",
    "            old_eps.append(e_t)\n",
    "            if len(old_eps) >= 4:\n",
    "                old_eps.pop(0)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(pred_x0, i)\n",
    "\n",
    "            if index % log_every_t == 0 or index == total_steps - 1:\n",
    "                intermediates['x_inter'].append(img)\n",
    "                intermediates['pred_x0'].append(pred_x0)\n",
    "\n",
    "        return img, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_plms(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n",
    "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
    "                      unconditional_guidance_scale=1., unconditional_conditioning=None, old_eps=None, t_next=None):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "\n",
    "        def get_model_output(x, t):\n",
    "            if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n",
    "                e_t = self.model.apply_model(x, t, c)\n",
    "            else:\n",
    "                x_in = torch.cat([x] * 2)\n",
    "                t_in = torch.cat([t] * 2)\n",
    "                c_in = torch.cat([unconditional_conditioning, c])\n",
    "                e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(2)\n",
    "                e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
    "\n",
    "            if score_corrector is not None:\n",
    "                assert self.model.parameterization == \"eps\"\n",
    "                e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n",
    "\n",
    "            return e_t\n",
    "\n",
    "        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n",
    "        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n",
    "        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n",
    "        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n",
    "\n",
    "        def get_x_prev_and_pred_x0(e_t, index):\n",
    "            # select parameters corresponding to the currently considered timestep\n",
    "            a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
    "            a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
    "            sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
    "            sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
    "\n",
    "            # current prediction for x_0\n",
    "            pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "            if quantize_denoised:\n",
    "                pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n",
    "            # direction pointing to x_t\n",
    "            dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
    "            noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n",
    "            if noise_dropout > 0.:\n",
    "                noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
    "            x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "            return x_prev, pred_x0\n",
    "\n",
    "        e_t = get_model_output(x, t)\n",
    "        if len(old_eps) == 0:\n",
    "            # Pseudo Improved Euler (2nd order)\n",
    "            x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t, index)\n",
    "            e_t_next = get_model_output(x_prev, t_next)\n",
    "            e_t_prime = (e_t + e_t_next) / 2\n",
    "        elif len(old_eps) == 1:\n",
    "            # 2nd order Pseudo Linear Multistep (Adams-Bashforth)\n",
    "            e_t_prime = (3 * e_t - old_eps[-1]) / 2\n",
    "        elif len(old_eps) == 2:\n",
    "            # 3nd order Pseudo Linear Multistep (Adams-Bashforth)\n",
    "            e_t_prime = (23 * e_t - 16 * old_eps[-1] + 5 * old_eps[-2]) / 12\n",
    "        elif len(old_eps) >= 3:\n",
    "            # 4nd order Pseudo Linear Multistep (Adams-Bashforth)\n",
    "            e_t_prime = (55 * e_t - 59 * old_eps[-1] + 37 * old_eps[-2] - 9 * old_eps[-3]) / 24\n",
    "\n",
    "        x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t_prime, index)\n",
    "\n",
    "        return x_prev, pred_x0, e_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (1.7.7)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.9.* in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (1.12.1+cu116)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (4.64.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec!=2021.06.0,>=2021.05.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2024.5.0)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (2.14.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (0.11.4)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (0.3.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pytorch-lightning) (4.12.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.64.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.6)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.20.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (69.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.43.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (7.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->pytorch-lightning) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->pytorch-lightning) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->pytorch-lightning) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->pytorch-lightning) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (3.19.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning) (3.2.2)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "SLWkZAYJ3CLJ"
   },
   "outputs": [],
   "source": [
    "#@title Ldm part\n",
    "import wandb\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append(\"/home/guisi/DreamDiffusion/code/dc_ldm\")\n",
    "sys.path.append(\"code\")\n",
    "sys.path.append(\"/home/guisi/DreamDiffusion/code\")\n",
    "\n",
    "from dc_ldm.models.diffusion.ddpm import LatentDiffusion\n",
    "\n",
    "from PIL import Image\n",
    "def create_model_from_config(config, num_voxels, global_pool):\n",
    "    model = eeg_encoder(time_len=num_voxels, patch_size=config.patch_size, embed_dim=config.embed_dim,\n",
    "                depth=config.depth, num_heads=config.num_heads, mlp_ratio=config.mlp_ratio, global_pool=global_pool)\n",
    "    return model\n",
    "\n",
    "def contrastive_loss(logits, dim):\n",
    "    neg_ce = torch.diag(F.log_softmax(logits, dim=dim))\n",
    "    return -neg_ce.mean()\n",
    "\n",
    "def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity, dim=0)\n",
    "    image_loss = contrastive_loss(similarity, dim=1)\n",
    "    return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "class cond_stage_model(nn.Module):\n",
    "    def __init__(self, metafile, num_voxels=440, cond_dim=1280, global_pool=True, clip_tune = True, cls_tune = False):\n",
    "        super().__init__()\n",
    "        # prepare pretrained fmri mae\n",
    "        if metafile is not None:\n",
    "            model = create_model_from_config(metafile['config'], num_voxels, global_pool)\n",
    "\n",
    "            model.load_checkpoint(metafile['model'])\n",
    "        else:\n",
    "            model = eeg_encoder(time_len=num_voxels, global_pool=global_pool)\n",
    "        self.mae = model\n",
    "        if clip_tune:\n",
    "            self.mapping = mapping()\n",
    "        if cls_tune:\n",
    "            self.cls_net = classify_network()\n",
    "\n",
    "        self.fmri_seq_len = model.num_patches\n",
    "        self.fmri_latent_dim = model.embed_dim\n",
    "        if global_pool == False:\n",
    "            self.channel_mapper = nn.Sequential(\n",
    "                nn.Conv1d(self.fmri_seq_len, self.fmri_seq_len // 2, 1, bias=True),\n",
    "                nn.Conv1d(self.fmri_seq_len // 2, 77, 1, bias=True)\n",
    "            )\n",
    "        self.dim_mapper = nn.Linear(self.fmri_latent_dim, cond_dim, bias=True)\n",
    "        self.global_pool = global_pool\n",
    "\n",
    "        # self.image_embedder = FrozenImageEmbedder()\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # n, c, w = x.shape\n",
    "    #     latent_crossattn = self.mae(x)\n",
    "    #     if self.global_pool == False:\n",
    "    #         latent_crossattn = self.channel_mapper(latent_crossattn)\n",
    "    #     latent_crossattn = self.dim_mapper(latent_crossattn)\n",
    "    #     out = latent_crossattn\n",
    "    #     return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # n, c, w = x.shape\n",
    "        latent_crossattn = self.mae(x)\n",
    "        latent_return = latent_crossattn\n",
    "        if self.global_pool == False:\n",
    "            latent_crossattn = self.channel_mapper(latent_crossattn)\n",
    "        latent_crossattn = self.dim_mapper(latent_crossattn)\n",
    "        out = latent_crossattn\n",
    "        return out, latent_return\n",
    "\n",
    "    # def recon(self, x):\n",
    "    #     recon = self.decoder(x)\n",
    "    #     return recon\n",
    "\n",
    "    def get_cls(self, x):\n",
    "        return self.cls_net(x)\n",
    "\n",
    "    def get_clip_loss(self, x, image_embeds):\n",
    "        # image_embeds = self.image_embedder(image_inputs)\n",
    "        target_emb = self.mapping(x)\n",
    "        # similarity_matrix = nn.functional.cosine_similarity(target_emb.unsqueeze(1), image_embeds.unsqueeze(0), dim=2)\n",
    "        # loss = clip_loss(similarity_matrix)\n",
    "        loss = 1 - torch.cosine_similarity(target_emb, image_embeds, dim=-1).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class eLDM:\n",
    "\n",
    "    def __init__(self, metafile, num_voxels, device=torch.device('cpu'),\n",
    "                 pretrain_root='../pretrains/',\n",
    "                 logger=None, ddim_steps=15, global_pool=True, use_time_cond=False, clip_tune = True, cls_tune = False):\n",
    "        # self.ckp_path = os.path.join(pretrain_root, 'model.ckpt')\n",
    "        self.ckp_path = '/home/guisi/DreamDiffusion/dataset/eeg_dataset/v1-5-pruned.ckpt'\n",
    "        self.config_path = os.path.join(pretrain_root, 'models/config15.yaml')\n",
    "        self.config_path = '/home/guisi/DreamDiffusion/pretrains/models/config15.yaml'\n",
    "        config = OmegaConf.load(self.config_path)\n",
    "        config.model.params.unet_config.params.use_time_cond = use_time_cond\n",
    "        config.model.params.unet_config.params.global_pool = global_pool\n",
    "\n",
    "        self.cond_dim = config.model.params.unet_config.params.context_dim\n",
    "\n",
    "        model = instantiate_from_config(config.model)\n",
    "        pl_sd = torch.load(self.ckp_path, map_location=\"cpu\")['state_dict']\n",
    "\n",
    "        m, u = model.load_state_dict(pl_sd, strict=False)\n",
    "        model.cond_stage_trainable = True\n",
    "        model.cond_stage_model = cond_stage_model(metafile, num_voxels, self.cond_dim, global_pool=global_pool, clip_tune = clip_tune,cls_tune = cls_tune)\n",
    "\n",
    "        model.ddim_steps = ddim_steps\n",
    "        model.re_init_ema()\n",
    "        if logger is not None:\n",
    "            logger.watch_model(model, log=\"all\", log_graph=False)\n",
    "\n",
    "        model.p_channels = config.model.params.channels\n",
    "        model.p_image_size = config.model.params.image_size\n",
    "        model.ch_mult = config.model.params.first_stage_config.params.ddconfig.ch_mult\n",
    "\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "\n",
    "        self.model.clip_tune = clip_tune\n",
    "        self.model.cls_tune = cls_tune\n",
    "\n",
    "        self.ldm_config = config\n",
    "        self.pretrain_root = pretrain_root\n",
    "        self.fmri_latent_dim = model.cond_stage_model.fmri_latent_dim\n",
    "        self.metafile = metafile\n",
    "\n",
    "    def finetune(self, trainers, dataset, test_dataset, bs1, lr1,\n",
    "                output_path, config=None, logger=None):\n",
    "        config.trainer = None\n",
    "        config.logger = logger\n",
    "        self.model.main_config = config\n",
    "        self.model.output_path = output_path\n",
    "        # self.model.train_dataset = dataset\n",
    "        self.model.run_full_validation_threshold = 0.15\n",
    "        # stage one: train the cond encoder with the pretrained one\n",
    "\n",
    "        # # stage one: only optimize conditional encoders\n",
    "        print('\\n##### Stage One: only optimize conditional encoders #####')\n",
    "        dataset.num_samples = 3\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=bs1, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=bs1, shuffle=False)\n",
    "        self.model.unfreeze_whole_model()\n",
    "        self.model.freeze_first_stage()\n",
    "        # self.model.freeze_whole_model()\n",
    "        # self.model.unfreeze_cond_stage()\n",
    "\n",
    "        self.model.learning_rate = lr1\n",
    "        self.model.train_cond_stage_only = True\n",
    "        self.model.eval_avg = config.eval_avg\n",
    "        logger.watch_model(self.model, log='all', log_freq=100)        \n",
    "        trainers.fit(self.model, dataloader, val_dataloaders=test_loader)\n",
    "\n",
    "        self.model.unfreeze_whole_model()\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'config': config,\n",
    "                'state': torch.random.get_rng_state()\n",
    "\n",
    "            },\n",
    "            os.path.join(output_path, 'checkpoint.pth')\n",
    "        )\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, fmri_embedding, num_samples, ddim_steps, HW=None, limit=None, state=None, output_path = None):\n",
    "        # fmri_embedding: n, seq_len, embed_dim\n",
    "        all_samples = []\n",
    "        if HW is None:\n",
    "            shape = (self.ldm_config.model.params.channels,\n",
    "                self.ldm_config.model.params.image_size, self.ldm_config.model.params.image_size)\n",
    "        else:\n",
    "            num_resolutions = len(self.ldm_config.model.params.first_stage_config.params.ddconfig.ch_mult)\n",
    "            shape = (self.ldm_config.model.params.channels,\n",
    "                HW[0] // 2**(num_resolutions-1), HW[1] // 2**(num_resolutions-1))\n",
    "\n",
    "        model = self.model.to(self.device)\n",
    "        sampler = PLMSSampler(model)\n",
    "        # sampler = DDIMSampler(model)\n",
    "        if state is not None:\n",
    "            torch.cuda.set_rng_state(state)\n",
    "\n",
    "        with model.ema_scope():\n",
    "            model.eval()\n",
    "            for count, item in enumerate(fmri_embedding):\n",
    "                print(\"COUNT FMRI\")\n",
    "                print(item)\n",
    "                if limit is not None:\n",
    "                    if count >= limit:\n",
    "                        break\n",
    "                print(item)\n",
    "                latent = item['eeg']\n",
    "                gt_image = rearrange(item['image'], 'h w c -> 1 c h w') # h w c\n",
    "                print(f\"rendering {num_samples} examples in {ddim_steps} steps.\")\n",
    "                # assert latent.shape[-1] == self.fmri_latent_dim, 'dim error'\n",
    "\n",
    "                c, re_latent = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                # c = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                conditioning=c,\n",
    "                                                batch_size=num_samples,\n",
    "                                                shape=shape,\n",
    "                                                verbose=False)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
    "                gt_image = torch.clamp((gt_image+1.0)/2.0, min=0.0, max=1.0)\n",
    "\n",
    "                all_samples.append(torch.cat([gt_image, x_samples_ddim.detach().cpu()], dim=0)) # put groundtruth at first\n",
    "                if output_path is not None:\n",
    "                    samples_t = (255. * torch.cat([gt_image, x_samples_ddim.detach().cpu()], dim=0).numpy()).astype(np.uint8)\n",
    "                    for copy_idx, img_t in enumerate(samples_t):\n",
    "                        img_t = rearrange(img_t, 'c h w -> h w c')\n",
    "                        Image.fromarray(img_t).save(os.path.join(output_path,\n",
    "                            f'./test{count}-{copy_idx}.png'))\n",
    "\n",
    "        # display as grid\n",
    "        grid = torch.stack(all_samples, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=num_samples+1)\n",
    "\n",
    "        # to image\n",
    "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        model = model.to('cpu')\n",
    "\n",
    "        return grid, (255. * torch.stack(all_samples, 0).cpu().numpy()).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "YvghHNmtGTDI"
   },
   "outputs": [],
   "source": [
    "#@title other utils\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import abc\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "from inspect import isfunction\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def log_txt_as_img(wh, xc, size=10):\n",
    "    # wh a tuple of (width, height)\n",
    "    # xc a list of captions to plot\n",
    "    b = len(xc)\n",
    "    txts = list()\n",
    "    for bi in range(b):\n",
    "        txt = Image.new(\"RGB\", wh, color=\"white\")\n",
    "        draw = ImageDraw.Draw(txt)\n",
    "        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n",
    "        nc = int(40 * (wh[0] / 256))\n",
    "        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n",
    "\n",
    "        try:\n",
    "            draw.text((0, 0), lines, fill=\"black\", font=font)\n",
    "        except UnicodeEncodeError:\n",
    "            print(\"Cant encode string for logging. Skipping.\")\n",
    "\n",
    "        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n",
    "        txts.append(txt)\n",
    "    txts = np.stack(txts)\n",
    "    txts = torch.tensor(txts)\n",
    "    return txts\n",
    "\n",
    "\n",
    "def ismap(x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        return False\n",
    "    return (len(x.shape) == 4) and (x.shape[1] > 3)\n",
    "\n",
    "\n",
    "def isimage(x):\n",
    "    if not isinstance(x,torch.Tensor):\n",
    "        return False\n",
    "    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n",
    "\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "\n",
    "def count_params(model, verbose=False):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    if verbose:\n",
    "        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        if config in ['__is_first_stage__', \"__is_unconditional__\"]:\n",
    "            return None\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    \n",
    "    target_class_str = config[\"target\"]\n",
    "    \n",
    "    try:\n",
    "        target_class = get_obj_from_str(target_class_str)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Class '{target_class_str}' not found in the global scope.\")\n",
    "    \n",
    "    return target_class(**config.get(\"params\", dict()))\n",
    "\n",
    "def get_obj_from_str(string):\n",
    "    try:\n",
    "        # Directly get the global object from the global scope\n",
    "        obj = globals()[string]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Object '{string}' not found in the global scope.\")\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _do_parallel_data_prefetch(func, Q, data, idx, idx_to_fn=False):\n",
    "    # create dummy dataset instance\n",
    "\n",
    "    # run prefetching\n",
    "    if idx_to_fn:\n",
    "        res = func(data, worker_id=idx)\n",
    "    else:\n",
    "        res = func(data)\n",
    "    Q.put([idx, res])\n",
    "    Q.put(\"Done\")\n",
    "\n",
    "\n",
    "def parallel_data_prefetch(\n",
    "        func: callable, data, n_proc, target_data_type=\"ndarray\", cpu_intensive=True, use_worker_id=False\n",
    "):\n",
    "    # if target_data_type not in [\"ndarray\", \"list\"]:\n",
    "    #     raise ValueError(\n",
    "    #         \"Data, which is passed to parallel_data_prefetch has to be either of type list or ndarray.\"\n",
    "    #     )\n",
    "    if isinstance(data, np.ndarray) and target_data_type == \"list\":\n",
    "        raise ValueError(\"list expected but function got ndarray.\")\n",
    "    elif isinstance(data, abc.Iterable):\n",
    "        if isinstance(data, dict):\n",
    "            print(\n",
    "                f'WARNING:\"data\" argument passed to parallel_data_prefetch is a dict: Using only its values and disregarding keys.'\n",
    "            )\n",
    "            data = list(data.values())\n",
    "        if target_data_type == \"ndarray\":\n",
    "            data = np.asarray(data)\n",
    "        else:\n",
    "            data = list(data)\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f\"The data, that shall be processed parallel has to be either an np.ndarray or an Iterable, but is actually {type(data)}.\"\n",
    "        )\n",
    "\n",
    "    if cpu_intensive:\n",
    "        Q = mp.Queue(1000)\n",
    "        proc = mp.Process\n",
    "    else:\n",
    "        Q = Queue(1000)\n",
    "        proc = Thread\n",
    "    # spawn processes\n",
    "    if target_data_type == \"ndarray\":\n",
    "        arguments = [\n",
    "            [func, Q, part, i, use_worker_id]\n",
    "            for i, part in enumerate(np.array_split(data, n_proc))\n",
    "        ]\n",
    "    else:\n",
    "        step = (\n",
    "            int(len(data) / n_proc + 1)\n",
    "            if len(data) % n_proc != 0\n",
    "            else int(len(data) / n_proc)\n",
    "        )\n",
    "        arguments = [\n",
    "            [func, Q, part, i, use_worker_id]\n",
    "            for i, part in enumerate(\n",
    "                [data[i: i + step] for i in range(0, len(data), step)]\n",
    "            )\n",
    "        ]\n",
    "    processes = []\n",
    "    for i in range(n_proc):\n",
    "        p = proc(target=_do_parallel_data_prefetch, args=arguments[i])\n",
    "        processes += [p]\n",
    "\n",
    "    # start processes\n",
    "    print(f\"Start prefetching...\")\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    gather_res = [[] for _ in range(n_proc)]\n",
    "    try:\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "\n",
    "        k = 0\n",
    "        while k < n_proc:\n",
    "            # get result\n",
    "            res = Q.get()\n",
    "            if res == \"Done\":\n",
    "                k += 1\n",
    "            else:\n",
    "                gather_res[res[0]] = res[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "        for p in processes:\n",
    "            p.terminate()\n",
    "\n",
    "        raise e\n",
    "    finally:\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        print(f\"Prefetching complete. [{time.time() - start} sec.]\")\n",
    "\n",
    "    if target_data_type == 'ndarray':\n",
    "        if not isinstance(gather_res[0], np.ndarray):\n",
    "            return np.concatenate([np.asarray(r) for r in gather_res], axis=0)\n",
    "\n",
    "        # order outputs\n",
    "        return np.concatenate(gather_res, axis=0)\n",
    "    elif target_data_type == 'list':\n",
    "        out = []\n",
    "        for r in gather_res:\n",
    "            out.extend(r)\n",
    "        return out\n",
    "    else:\n",
    "        return gather_res\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AbstractDistribution:\n",
    "    def sample(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def mode(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class DiracDistribution(AbstractDistribution):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def sample(self):\n",
    "        return self.value\n",
    "\n",
    "    def mode(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class DiagonalGaussianDistribution(object):\n",
    "    def __init__(self, parameters, deterministic=False):\n",
    "        self.parameters = parameters\n",
    "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
    "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
    "        self.deterministic = deterministic\n",
    "        self.std = torch.exp(0.5 * self.logvar)\n",
    "        self.var = torch.exp(self.logvar)\n",
    "        if self.deterministic:\n",
    "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
    "        return x\n",
    "\n",
    "    def kl(self, other=None):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        else:\n",
    "            if other is None:\n",
    "                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n",
    "                                       + self.var - 1.0 - self.logvar,\n",
    "                                       dim=[1, 2, 3])\n",
    "            else:\n",
    "                return 0.5 * torch.sum(\n",
    "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
    "                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n",
    "                    dim=[1, 2, 3])\n",
    "\n",
    "    def nll(self, sample, dims=[1,2,3]):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        logtwopi = np.log(2.0 * np.pi)\n",
    "        return 0.5 * torch.sum(\n",
    "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
    "            dim=dims)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n",
    "    Compute the KL divergence between two gaussians.\n",
    "    Shapes are automatically broadcasted, so batches can be compared to\n",
    "    scalars, among other use cases.\n",
    "    \"\"\"\n",
    "    tensor = None\n",
    "    for obj in (mean1, logvar1, mean2, logvar2):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            tensor = obj\n",
    "            break\n",
    "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
    "\n",
    "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
    "    # Tensors, but it does not work for torch.exp().\n",
    "    logvar1, logvar2 = [\n",
    "        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n",
    "        for x in (logvar1, logvar2)\n",
    "    ]\n",
    "\n",
    "    return 0.5 * (\n",
    "        -1.0\n",
    "        + logvar2\n",
    "        - logvar1\n",
    "        + torch.exp(logvar1 - logvar2)\n",
    "        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MO-4JPhfY6a",
    "outputId": "25c3f9ce-0d02-4526-d65f-24d936434f71"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning==1.7.7 torchmetrics==0.11.4 torchtext==0.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "GS67Yq6yfloT"
   },
   "outputs": [],
   "source": [
    "#@title Distributions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AbstractDistribution:\n",
    "    def sample(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def mode(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class DiracDistribution(AbstractDistribution):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def sample(self):\n",
    "        return self.value\n",
    "\n",
    "    def mode(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class DiagonalGaussianDistribution(object):\n",
    "    def __init__(self, parameters, deterministic=False):\n",
    "        self.parameters = parameters\n",
    "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
    "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
    "        self.deterministic = deterministic\n",
    "        self.std = torch.exp(0.5 * self.logvar)\n",
    "        self.var = torch.exp(self.logvar)\n",
    "        if self.deterministic:\n",
    "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
    "        return x\n",
    "\n",
    "    def kl(self, other=None):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        else:\n",
    "            if other is None:\n",
    "                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n",
    "                                       + self.var - 1.0 - self.logvar,\n",
    "                                       dim=[1, 2, 3])\n",
    "            else:\n",
    "                return 0.5 * torch.sum(\n",
    "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
    "                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n",
    "                    dim=[1, 2, 3])\n",
    "\n",
    "    def nll(self, sample, dims=[1,2,3]):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        logtwopi = np.log(2.0 * np.pi)\n",
    "        return 0.5 * torch.sum(\n",
    "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
    "            dim=dims)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n",
    "    Compute the KL divergence between two gaussians.\n",
    "    Shapes are automatically broadcasted, so batches can be compared to\n",
    "    scalars, among other use cases.\n",
    "    \"\"\"\n",
    "    tensor = None\n",
    "    for obj in (mean1, logvar1, mean2, logvar2):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            tensor = obj\n",
    "            break\n",
    "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
    "\n",
    "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
    "    # Tensors, but it does not work for torch.exp().\n",
    "    logvar1, logvar2 = [\n",
    "        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n",
    "        for x in (logvar1, logvar2)\n",
    "    ]\n",
    "\n",
    "    return 0.5 * (\n",
    "        -1.0\n",
    "        + logvar2\n",
    "        - logvar1\n",
    "        + torch.exp(logvar1 - logvar2)\n",
    "        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923.37s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytorch-lightning in /home/guisi/.local/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (1.26.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (2.3.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (4.64.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/miniconda/lib/python3.12/site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /home/guisi/.local/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.12/site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/miniconda/lib/python3.12/site-packages (from pytorch-lightning) (4.10.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/guisi/.local/lib/python3.12/site-packages (from pytorch-lightning) (0.11.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/guisi/.local/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.12/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: filelock in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.13.0->pytorch-lightning) (11.8.86)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guisi/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/guisi/.local/lib/python3.12/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/miniconda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "TSSjtweEHzub"
   },
   "outputs": [],
   "source": [
    "#@title Auto Encoders\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "# from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
    "import torch.nn as nn\n",
    "from packaging import version\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved version over VectorQuantizer in taming, can be used as a drop-in replacement. Mostly\n",
    "    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n",
    "    \"\"\"\n",
    "    # NOTE: due to a bug the beta term was applied to the wrong term. for\n",
    "    # backwards compatibility we use the buggy version by default, but you can\n",
    "    # specify legacy=False to fix it.\n",
    "    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\",\n",
    "                 sane_index_shape=False, legacy=True):\n",
    "        super().__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "        self.legacy = legacy\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "        self.remap = remap\n",
    "        if self.remap is not None:\n",
    "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
    "            self.re_embed = self.used.shape[0]\n",
    "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
    "            if self.unknown_index == \"extra\":\n",
    "                self.unknown_index = self.re_embed\n",
    "                self.re_embed = self.re_embed+1\n",
    "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
    "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
    "        else:\n",
    "            self.re_embed = n_e\n",
    "\n",
    "        self.sane_index_shape = sane_index_shape\n",
    "\n",
    "    def remap_to_used(self, inds):\n",
    "        ishape = inds.shape\n",
    "        assert len(ishape)>1\n",
    "        inds = inds.reshape(ishape[0],-1)\n",
    "        used = self.used.to(inds)\n",
    "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
    "        new = match.argmax(-1)\n",
    "        unknown = match.sum(2)<1\n",
    "        if self.unknown_index == \"random\":\n",
    "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
    "        else:\n",
    "            new[unknown] = self.unknown_index\n",
    "        return new.reshape(ishape)\n",
    "\n",
    "    def unmap_to_all(self, inds):\n",
    "        ishape = inds.shape\n",
    "        assert len(ishape)>1\n",
    "        inds = inds.reshape(ishape[0],-1)\n",
    "        used = self.used.to(inds)\n",
    "        if self.re_embed > self.used.shape[0]: # extra token\n",
    "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
    "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
    "        return back.reshape(ishape)\n",
    "\n",
    "    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
    "        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n",
    "        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n",
    "        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n",
    "        # reshape z -> (batch, height, width, channel) and flatten\n",
    "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
    "        z_flattened = z.view(-1, self.e_dim)\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
    "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
    "\n",
    "        min_encoding_indices = torch.argmin(d, dim=1)\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
    "        perplexity = None\n",
    "        min_encodings = None\n",
    "\n",
    "        # compute loss for embedding\n",
    "        if not self.legacy:\n",
    "            loss = self.beta * torch.mean((z_q.detach()-z)**2) + \\\n",
    "                   torch.mean((z_q - z.detach()) ** 2)\n",
    "        else:\n",
    "            loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
    "                   torch.mean((z_q - z.detach()) ** 2)\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # reshape back to match original input shape\n",
    "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
    "\n",
    "        if self.remap is not None:\n",
    "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n",
    "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
    "            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n",
    "\n",
    "        if self.sane_index_shape:\n",
    "            min_encoding_indices = min_encoding_indices.reshape(\n",
    "                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
    "\n",
    "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
    "\n",
    "    def get_codebook_entry(self, indices, shape):\n",
    "        # shape specifying (batch, height, width, channel)\n",
    "        if self.remap is not None:\n",
    "            indices = indices.reshape(shape[0],-1) # add batch axis\n",
    "            indices = self.unmap_to_all(indices)\n",
    "            indices = indices.reshape(-1) # flatten again\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = self.embedding(indices)\n",
    "\n",
    "        if shape is not None:\n",
    "            z_q = z_q.view(shape)\n",
    "            # reshape back to match original input shape\n",
    "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return z_q\n",
    "\n",
    "class VQModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 ddconfig,\n",
    "                 lossconfig,\n",
    "                 n_embed,\n",
    "                 embed_dim,\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 image_key=\"image\",\n",
    "                 colorize_nlabels=None,\n",
    "                 monitor=None,\n",
    "                 batch_resize_range=None,\n",
    "                 scheduler_config=None,\n",
    "                 lr_g_factor=1.0,\n",
    "                 remap=None,\n",
    "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
    "                 use_ema=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_embed = n_embed\n",
    "        self.image_key = image_key\n",
    "        self.encoder = DiffusionEncoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig)\n",
    "        self.loss = instantiate_from_config(lossconfig)\n",
    "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
    "                                        remap=remap,\n",
    "                                        sane_index_shape=sane_index_shape)\n",
    "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "        if colorize_nlabels is not None:\n",
    "            assert type(colorize_nlabels)==int\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "        self.batch_resize_range = batch_resize_range\n",
    "        if self.batch_resize_range is not None:\n",
    "            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n",
    "\n",
    "        self.use_ema = use_ema\n",
    "        if self.use_ema:\n",
    "            self.model_ema = LitEma(self)\n",
    "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
    "        self.scheduler_config = scheduler_config\n",
    "        self.lr_g_factor = lr_g_factor\n",
    "\n",
    "    @contextmanager\n",
    "    def ema_scope(self, context=None):\n",
    "        if self.use_ema:\n",
    "            self.model_ema.store(self.parameters())\n",
    "            self.model_ema.copy_to(self)\n",
    "            if context is not None:\n",
    "                print(f\"{context}: Switched to EMA weights\")\n",
    "        try:\n",
    "            yield None\n",
    "        finally:\n",
    "            if self.use_ema:\n",
    "                self.model_ema.restore(self.parameters())\n",
    "                if context is not None:\n",
    "                    print(f\"{context}: Restored training weights\")\n",
    "\n",
    "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
    "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        missing, unexpected = self.load_state_dict(sd, strict=False)\n",
    "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "        if len(missing) > 0:\n",
    "            print(f\"Missing Keys: {missing}\")\n",
    "            print(f\"Unexpected Keys: {unexpected}\")\n",
    "\n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        if self.use_ema:\n",
    "            self.model_ema(self)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        quant, emb_loss, info = self.quantize(h)\n",
    "        return quant, emb_loss, info\n",
    "\n",
    "    def encode_to_prequant(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        return h\n",
    "\n",
    "    def decode(self, quant):\n",
    "        quant = self.post_quant_conv(quant)\n",
    "        dec = self.decoder(quant)\n",
    "        return dec\n",
    "\n",
    "    def decode_code(self, code_b):\n",
    "        quant_b = self.quantize.embed_code(code_b)\n",
    "        dec = self.decode(quant_b)\n",
    "        return dec\n",
    "\n",
    "    def forward(self, input, return_pred_indices=False):\n",
    "        quant, diff, (_,_,ind) = self.encode(input)\n",
    "        dec = self.decode(quant)\n",
    "        if return_pred_indices:\n",
    "            return dec, diff, ind\n",
    "        return dec, diff\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
    "        if self.batch_resize_range is not None:\n",
    "            lower_size = self.batch_resize_range[0]\n",
    "            upper_size = self.batch_resize_range[1]\n",
    "            if self.global_step <= 4:\n",
    "                # do the first few batches with max size to avoid later oom\n",
    "                new_resize = upper_size\n",
    "            else:\n",
    "                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n",
    "            if new_resize != x.shape[2]:\n",
    "                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n",
    "            x = x.detach()\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        # https://github.com/pytorch/pytorch/issues/37142\n",
    "        # try not to fool the heuristics\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            # autoencode\n",
    "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
    "                                            predicted_indices=ind)\n",
    "\n",
    "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
    "            return aeloss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            # discriminator\n",
    "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
    "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
    "            return discloss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        log_dict = self._validation_step(batch, batch_idx)\n",
    "        with self.ema_scope():\n",
    "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
    "        return log_dict\n",
    "\n",
    "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
    "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
    "                                        self.global_step,\n",
    "                                        last_layer=self.get_last_layer(),\n",
    "                                        split=\"val\"+suffix,\n",
    "                                        predicted_indices=ind\n",
    "                                        )\n",
    "\n",
    "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
    "                                            self.global_step,\n",
    "                                            last_layer=self.get_last_layer(),\n",
    "                                            split=\"val\"+suffix,\n",
    "                                            predicted_indices=ind\n",
    "                                            )\n",
    "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
    "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
    "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
    "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
    "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        return self.log_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr_d = self.learning_rate\n",
    "        lr_g = self.lr_g_factor*self.learning_rate\n",
    "        print(\"lr_d\", lr_d)\n",
    "        print(\"lr_g\", lr_g)\n",
    "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
    "                                  list(self.decoder.parameters())+\n",
    "                                  list(self.quantize.parameters())+\n",
    "                                  list(self.quant_conv.parameters())+\n",
    "                                  list(self.post_quant_conv.parameters()),\n",
    "                                  lr=lr_g, betas=(0.5, 0.9))\n",
    "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
    "                                    lr=lr_d, betas=(0.5, 0.9))\n",
    "\n",
    "        if self.scheduler_config is not None:\n",
    "            scheduler = instantiate_from_config(self.scheduler_config)\n",
    "\n",
    "            print(\"Setting up LambdaLR scheduler...\")\n",
    "            scheduler = [\n",
    "                {\n",
    "                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1\n",
    "                },\n",
    "                {\n",
    "                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1\n",
    "                },\n",
    "            ]\n",
    "            return [opt_ae, opt_disc], scheduler\n",
    "        return [opt_ae, opt_disc], []\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.conv_out.weight\n",
    "\n",
    "    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        x = x.to(self.device)\n",
    "        if only_inputs:\n",
    "            log[\"inputs\"] = x\n",
    "            return log\n",
    "        xrec, _ = self(x)\n",
    "        if x.shape[1] > 3:\n",
    "            # colorize with random projection\n",
    "            assert xrec.shape[1] > 3\n",
    "            x = self.to_rgb(x)\n",
    "            xrec = self.to_rgb(xrec)\n",
    "        log[\"inputs\"] = x\n",
    "        log[\"reconstructions\"] = xrec\n",
    "        if plot_ema:\n",
    "            with self.ema_scope():\n",
    "                xrec_ema, _ = self(x)\n",
    "                if x.shape[1] > 3: xrec_ema = self.to_rgb(xrec_ema)\n",
    "                log[\"reconstructions_ema\"] = xrec_ema\n",
    "        return log\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        assert self.image_key == \"segmentation\"\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
    "        x = F.conv2d(x, weight=self.colorize)\n",
    "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "class VQModelInterface(VQModel):\n",
    "    def __init__(self, embed_dim, *args, **kwargs):\n",
    "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        return h\n",
    "\n",
    "    def decode(self, h, force_not_quantize=False):\n",
    "        # also go through quantization layer\n",
    "        if not force_not_quantize:\n",
    "            quant, emb_loss, info = self.quantize(h)\n",
    "        else:\n",
    "            quant = h\n",
    "        quant = self.post_quant_conv(quant)\n",
    "        dec = self.decoder(quant)\n",
    "        return dec\n",
    "\n",
    "\n",
    "class AutoencoderKL(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 ddconfig,\n",
    "                 lossconfig,\n",
    "                 embed_dim,\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 image_key=\"image\",\n",
    "                 colorize_nlabels=None,\n",
    "                 monitor=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.image_key = image_key\n",
    "        self.encoder = DiffusionEncoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig)\n",
    "        self.loss = instantiate_from_config(lossconfig)\n",
    "        assert ddconfig[\"double_z\"]\n",
    "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "        self.embed_dim = embed_dim\n",
    "        if colorize_nlabels is not None:\n",
    "            assert type(colorize_nlabels)==int\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
    "        self.trainable = False\n",
    "\n",
    "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
    "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        self.load_state_dict(sd, strict=False)\n",
    "        print(f\"Restored from {path}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        moments = self.quant_conv(h)\n",
    "        posterior = DiagonalGaussianDistribution(moments)\n",
    "        return posterior\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.post_quant_conv(z)\n",
    "        dec = self.decoder(z)\n",
    "        return dec\n",
    "\n",
    "    def forward(self, input, sample_posterior=True):\n",
    "        posterior = self.encode(input)\n",
    "        if sample_posterior:\n",
    "            z = posterior.sample()\n",
    "        else:\n",
    "            z = posterior.mode()\n",
    "        dec = self.decode(z)\n",
    "        return dec, posterior\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        inputs = self.get_input(batch, self.image_key)\n",
    "        reconstructions, posterior = self(inputs)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            # train encoder+decoder+logvar\n",
    "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
    "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=False, on_epoch=False)\n",
    "            return aeloss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            # train the discriminator\n",
    "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
    "                                                last_layer=self.get_last_layer(), split=\"train\")\n",
    "\n",
    "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=False, on_epoch=False)\n",
    "            return discloss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs = self.get_input(batch, self.image_key)\n",
    "        reconstructions, posterior = self(inputs)\n",
    "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n",
    "                                        last_layer=self.get_last_layer(), split=\"val\")\n",
    "\n",
    "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"val\")\n",
    "\n",
    "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        return self.log_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
    "                                  list(self.decoder.parameters())+\n",
    "                                  list(self.quant_conv.parameters())+\n",
    "                                  list(self.post_quant_conv.parameters()),\n",
    "                                  lr=lr, betas=(0.5, 0.9))\n",
    "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
    "                                    lr=lr, betas=(0.5, 0.9))\n",
    "        return [opt_ae, opt_disc], []\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.conv_out.weight\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, only_inputs=False, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        x = x.to(self.device)\n",
    "        if not only_inputs:\n",
    "            xrec, posterior = self(x)\n",
    "            if x.shape[1] > 3:\n",
    "                # colorize with random projection\n",
    "                assert xrec.shape[1] > 3\n",
    "                x = self.to_rgb(x)\n",
    "                xrec = self.to_rgb(xrec)\n",
    "            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n",
    "            log[\"reconstructions\"] = xrec\n",
    "        log[\"inputs\"] = x\n",
    "        return log\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        assert self.image_key == \"segmentation\"\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
    "        x = F.conv2d(x, weight=self.colorize)\n",
    "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "class IdentityFirstStage(torch.nn.Module):\n",
    "    def __init__(self, *args, vq_interface=False, **kwargs):\n",
    "        self.vq_interface = vq_interface  # TODO: Should be true by default but check to not break older stuff\n",
    "        super().__init__()\n",
    "\n",
    "    def encode(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "    def quantize(self, x, *args, **kwargs):\n",
    "        if self.vq_interface:\n",
    "            return x, None, [None, None, None]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-image in /home/guisi/.local/lib/python3.12/site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/guisi/.local/lib/python3.12/site-packages (from scikit-image) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/guisi/.local/lib/python3.12/site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/guisi/.local/lib/python3.12/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /home/guisi/.local/lib/python3.12/site-packages (from scikit-image) (10.2.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/guisi/.local/lib/python3.12/site-packages (from scikit-image) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/guisi/.local/lib/python3.12/site-packages (from scikit-image) (2024.5.22)\n",
      "Requirement already satisfied: packaging>=21 in /opt/miniconda/lib/python3.12/site-packages (from scikit-image) (23.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/guisi/.local/lib/python3.12/site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "CwBYEQhegHap"
   },
   "outputs": [],
   "source": [
    "#@title Eval Metrics\n",
    "from os import get_inheritable\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.models import ViT_H_14_Weights, vit_h_14\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torchmetrics.functional import accuracy\n",
    "from PIL import Image\n",
    "\n",
    "def larger_the_better(gt, comp):\n",
    "    return gt > comp\n",
    "\n",
    "def smaller_the_better(gt, comp):\n",
    "    return gt < comp\n",
    "\n",
    "def mse_metric(img1, img2):\n",
    "    return (np.square(img1 - img2)).mean()\n",
    "\n",
    "def pcc_metric(img1, img2):\n",
    "    return np.corrcoef(img1.reshape(-1), img2.reshape(-1))[0, 1]\n",
    "\n",
    "def ssim_metric(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255, channel_axis=-1)\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class psm_wrapper:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex').to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, img1, img2):\n",
    "        if img1.shape[-1] == 3:\n",
    "            img1 = rearrange(img1, 'w h c -> c w h')\n",
    "            img2 = rearrange(img2, 'w h c -> c w h')\n",
    "        img1 = img1 / 127.5 - 1.0\n",
    "        img2 = img2 / 127.5 - 1.0\n",
    "        img1 = np.expand_dims(img1, axis=0)\n",
    "        img2 = np.expand_dims(img2, axis=0)\n",
    "        return self.lpips(torch.FloatTensor(img1).to(self.device), torch.FloatTensor(img2).to(self.device)).item()\n",
    "\n",
    "class fid_wrapper:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.fid = FrechetInceptionDistance(feature=64)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, pred_imgs, gt_imgs):\n",
    "        self.fid.reset()\n",
    "        self.fid.update(torch.tensor(rearrange(gt_imgs, 'n w h c -> n c w h')), real=True)\n",
    "        self.fid.update(torch.tensor(rearrange(pred_imgs, 'n w h c -> n c w h')), real=False)\n",
    "        return self.fid.compute().item()\n",
    "\n",
    "def pair_wise_score(pred_imgs, gt_imgs, metric, is_sucess):\n",
    "    # pred_imgs: n, w, h, 3\n",
    "    # gt_imgs: n, w, h, 3\n",
    "    # all in pixel values: 0 ~ 255\n",
    "    # return: list of scores 0 ~ 1.\n",
    "    assert len(pred_imgs) == len(gt_imgs)\n",
    "    assert np.min(pred_imgs) >= 0 and np.min(gt_imgs) >= 0\n",
    "    assert isinstance(metric, fid_wrapper) == False, 'FID not supported'\n",
    "    corrects = []\n",
    "    for idx, pred in enumerate(pred_imgs):\n",
    "        gt = gt_imgs[idx]\n",
    "        gt_score = metric(pred, gt)\n",
    "        rest = [img for i, img in enumerate(gt_imgs) if i != idx]\n",
    "        count = 0\n",
    "        for comp in rest:\n",
    "            comp_score = metric(pred, comp)\n",
    "            if is_sucess(gt_score, comp_score):\n",
    "                count += 1\n",
    "        corrects.append(count / len(rest))\n",
    "    return corrects\n",
    "\n",
    "def n_way_scores(pred_imgs, gt_imgs, metric, is_sucess, n=2, n_trials=100):\n",
    "    # pred_imgs: n, w, h, 3\n",
    "    # gt_imgs: n, w, h, 3\n",
    "    # all in pixel values: 0 ~ 255\n",
    "    # return: list of scores 0 ~ 1.\n",
    "    assert len(pred_imgs) == len(gt_imgs)\n",
    "    assert n <= len(pred_imgs) and n >= 2\n",
    "    assert np.min(pred_imgs) >= 0 and np.min(gt_imgs) >= 0\n",
    "    assert isinstance(metric, fid_wrapper) == False, 'FID not supported'\n",
    "    corrects = []\n",
    "    for idx, pred in enumerate(pred_imgs):\n",
    "        gt = gt_imgs[idx]\n",
    "        gt_score = metric(pred, gt)\n",
    "        rest = np.stack([img for i, img in enumerate(gt_imgs) if i != idx])\n",
    "        correct_count = 0\n",
    "        for _ in range(n_trials):\n",
    "            n_imgs_idx = np.random.choice(len(rest), n-1, replace=False)\n",
    "            n_imgs = rest[n_imgs_idx]\n",
    "            count = 0\n",
    "            for comp in n_imgs:\n",
    "                comp_score = metric(pred, comp)\n",
    "                if is_sucess(gt_score, comp_score):\n",
    "                    count += 1\n",
    "            if count == len(n_imgs):\n",
    "                correct_count += 1\n",
    "        corrects.append(correct_count / n_trials)\n",
    "    return corrects\n",
    "\n",
    "def metrics_only(pred_imgs, gt_imgs, metric, *args, **kwargs):\n",
    "    assert np.min(pred_imgs) >= 0 and np.min(gt_imgs) >= 0\n",
    "\n",
    "    return metric(pred_imgs, gt_imgs)\n",
    "\n",
    "def accuracia(output, target, top_k=(1, )):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            maxk = top_k\n",
    "            batch_size = target.size(0)\n",
    "\n",
    "            _, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            res = []\n",
    "            k=top_k\n",
    "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "            \n",
    "            return res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def n_way_top_k_acc(pred, class_id, n_way, num_trials=40, top_k=1):\n",
    "    pick_range =[i for i in np.arange(len(pred)) if i != class_id]\n",
    "    acc_list = []\n",
    "    for t in range(num_trials):\n",
    "        idxs_picked = np.random.choice(pick_range, n_way-1, replace=False)\n",
    "        pred_picked = torch.cat([pred[class_id].unsqueeze(0), pred[idxs_picked]])\n",
    "        acc = accuracia(output=pred_picked.unsqueeze(0), target = torch.tensor([0], device=pred.device), top_k=top_k)\n",
    "        acc_cpu = acc[0].cpu().item()      \n",
    "        acc_list.append(acc_cpu)\n",
    "\n",
    "    return np.mean(acc_list), np.std(acc_list)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_n_way_top_k_acc(pred_imgs, ground_truth, n_way, num_trials, top_k, device, return_std=False):\n",
    "    weights = ViT_H_14_Weights.DEFAULT\n",
    "    model = vit_h_14(weights=weights)\n",
    "    preprocess = weights.transforms()\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    acc_list = []\n",
    "    std_list = []\n",
    "    for pred, gt in zip(pred_imgs, ground_truth):\n",
    "        pred = preprocess(Image.fromarray(pred.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        gt = preprocess(Image.fromarray(gt.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        gt_class_id = model(gt).squeeze(0).softmax(0).argmax().item()\n",
    "        pred_out = model(pred).squeeze(0).softmax(0).detach()\n",
    "\n",
    "        acc, std = n_way_top_k_acc(pred_out, gt_class_id, n_way, num_trials, top_k)\n",
    "        acc_list.append(acc)\n",
    "        std_list.append(std)\n",
    "\n",
    "    if return_std:\n",
    "        return acc_list, std_list\n",
    "    return acc_list\n",
    "\n",
    "def get_similarity_metric(img1, img2, method='pair-wise', metric_name='mse', **kwargs):\n",
    "    # img1: n, w, h, 3\n",
    "    # img2: n, w, h, 3\n",
    "    # all in pixel values: 0 ~ 255\n",
    "    # return: list of scores 0 ~ 1.\n",
    "    if img1.shape[-1] != 3:\n",
    "        img1 = rearrange(img1, 'n c w h -> n w h c')\n",
    "    if img2.shape[-1] != 3:\n",
    "        img2 = rearrange(img2, 'n c w h -> n w h c')\n",
    "\n",
    "    if method == 'pair-wise':\n",
    "        eval_procedure_func = pair_wise_score\n",
    "    elif method == 'n-way':\n",
    "        eval_procedure_func = n_way_scores\n",
    "    elif method == 'metrics-only':\n",
    "        eval_procedure_func = metrics_only\n",
    "    elif method == 'class':\n",
    "        return get_n_way_top_k_acc(img1, img2, **kwargs)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if metric_name == 'mse':\n",
    "        metric_func = mse_metric\n",
    "        decision_func = smaller_the_better\n",
    "    elif metric_name == 'pcc':\n",
    "        metric_func = pcc_metric\n",
    "        decision_func = larger_the_better\n",
    "    elif metric_name == 'ssim':\n",
    "        metric_func = ssim_metric\n",
    "        decision_func = larger_the_better\n",
    "    elif metric_name == 'psm':\n",
    "        metric_func = psm_wrapper()\n",
    "        decision_func = smaller_the_better\n",
    "    elif metric_name == 'fid':\n",
    "        metric_func = fid_wrapper()\n",
    "        decision_func = smaller_the_better\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return eval_procedure_func(img1, img2, metric_func, decision_func, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "G3-Xy-GvFi1f"
   },
   "outputs": [],
   "source": [
    "#@title DDPM models\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LitEma(nn.Module):\n",
    "    def __init__(self, model, decay=0.9999, use_num_upates=True):\n",
    "        super().__init__()\n",
    "        if decay < 0.0 or decay > 1.0:\n",
    "            raise ValueError('Decay must be between 0 and 1')\n",
    "\n",
    "        self.m_name2s_name = {}\n",
    "        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))\n",
    "        self.register_buffer('num_updates', torch.tensor(0,dtype=torch.int) if use_num_upates\n",
    "                             else torch.tensor(-1,dtype=torch.int))\n",
    "\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                #remove as '.'-character is not allowed in buffers\n",
    "                s_name = name.replace('.','')\n",
    "                self.m_name2s_name.update({name:s_name})\n",
    "                self.register_buffer(s_name,p.clone().detach().data)\n",
    "\n",
    "        self.collected_params = []\n",
    "\n",
    "    def forward(self,model):\n",
    "        decay = self.decay\n",
    "\n",
    "        if self.num_updates >= 0:\n",
    "            self.num_updates += 1\n",
    "            decay = min(self.decay,(1 + self.num_updates) / (10 + self.num_updates))\n",
    "\n",
    "        one_minus_decay = 1.0 - decay\n",
    "\n",
    "        with torch.no_grad():\n",
    "            m_param = dict(model.named_parameters())\n",
    "            shadow_params = dict(self.named_buffers())\n",
    "\n",
    "            for key in m_param:\n",
    "                if m_param[key].requires_grad:\n",
    "                    sname = self.m_name2s_name[key]\n",
    "                    shadow_params[sname] = shadow_params[sname].type_as(m_param[key])\n",
    "                    shadow_params[sname].sub_(one_minus_decay * (shadow_params[sname] - m_param[key]))\n",
    "                else:\n",
    "                    assert not key in self.m_name2s_name\n",
    "\n",
    "    def copy_to(self, model):\n",
    "        m_param = dict(model.named_parameters())\n",
    "        shadow_params = dict(self.named_buffers())\n",
    "        for key in m_param:\n",
    "            if m_param[key].requires_grad:\n",
    "                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)\n",
    "            else:\n",
    "                assert not key in self.m_name2s_name\n",
    "\n",
    "    def store(self, parameters):\n",
    "        \"\"\"\n",
    "        Save the current parameters for restoring later.\n",
    "        Args:\n",
    "          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
    "            temporarily stored.\n",
    "        \"\"\"\n",
    "        self.collected_params = [param.clone() for param in parameters]\n",
    "\n",
    "    def restore(self, parameters):\n",
    "        \"\"\"\n",
    "        Restore the parameters stored with the `store` method.\n",
    "        Useful to validate the model with EMA parameters without affecting the\n",
    "        original optimization process. Store the parameters before the\n",
    "        `copy_to` method. After validation (or model saving), use this to\n",
    "        restore the former parameters.\n",
    "        Args:\n",
    "          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
    "            updated with the stored parameters.\n",
    "        \"\"\"\n",
    "        for c_param, param in zip(self.collected_params, parameters):\n",
    "            param.data.copy_(c_param.data)\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from einops import rearrange, repeat\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "__conditioning_keys__ = {'concat': 'c_concat',\n",
    "                         'crossattn': 'c_crossattn',\n",
    "                         'adm': 'y'}\n",
    "\n",
    "def disabled_train(self, mode=True):\n",
    "    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n",
    "    does not change anymore.\"\"\"\n",
    "    return self\n",
    "\n",
    "\n",
    "def uniform_on_device(r1, r2, shape, device):\n",
    "    return (r1 - r2) * torch.rand(*shape, device=device) + r2\n",
    "\n",
    "\n",
    "class DDPM(pl.LightningModule):\n",
    "    # classic DDPM with Gaussian diffusion, in image space\n",
    "    def __init__(self,\n",
    "                 unet_config,\n",
    "                 timesteps=1000,\n",
    "                 beta_schedule=\"linear\",\n",
    "                 loss_type=\"l2\",\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 load_only_unet=False,\n",
    "                 monitor=\"val/loss\",\n",
    "                 use_ema=True,\n",
    "                 first_stage_key=\"image\",\n",
    "                 image_size=256,\n",
    "                 channels=3,\n",
    "                 log_every_t=100,\n",
    "                 clip_denoised=True,\n",
    "                 linear_start=1e-4,\n",
    "                 linear_end=2e-2,\n",
    "                 cosine_s=8e-3,\n",
    "                 given_betas=None,\n",
    "                 original_elbo_weight=0.,\n",
    "                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n",
    "                 l_simple_weight=1.,\n",
    "                 conditioning_key=None,\n",
    "                 parameterization=\"eps\",  # all assuming fixed variance schedules\n",
    "                 scheduler_config=None,\n",
    "                 use_positional_encodings=False,\n",
    "                 learn_logvar=False,\n",
    "                 logvar_init=0.,\n",
    "                 ddim_steps=300\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert parameterization in [\"eps\", \"x0\"], 'currently only supporting \"eps\" and \"x0\"'\n",
    "        self.parameterization = parameterization\n",
    "        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n",
    "        self.cond_stage_model = None\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.log_every_t = log_every_t\n",
    "        self.first_stage_key = first_stage_key\n",
    "        self.image_size = image_size  # try conv?\n",
    "        self.channels = channels\n",
    "        self.use_positional_encodings = use_positional_encodings\n",
    "        self.model = DiffusionWrapper(unet_config, conditioning_key)\n",
    "        count_params(self.model, verbose=True)\n",
    "        self.use_ema = use_ema\n",
    "        if self.use_ema:\n",
    "            self.model_ema = LitEma(self.model)\n",
    "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
    "\n",
    "        self.use_scheduler = scheduler_config is not None\n",
    "        if self.use_scheduler:\n",
    "            self.scheduler_config = scheduler_config\n",
    "\n",
    "        self.v_posterior = v_posterior\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.l_simple_weight = l_simple_weight\n",
    "\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)\n",
    "\n",
    "        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n",
    "                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        self.learn_logvar = learn_logvar\n",
    "        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n",
    "        if self.learn_logvar:\n",
    "            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n",
    "\n",
    "        self.validation_count = 0\n",
    "        self.ddim_steps = ddim_steps\n",
    "        self.return_cond = False\n",
    "        self.output_path = None\n",
    "        self.main_config = None\n",
    "        self.best_val = 0.0\n",
    "        self.run_full_validation_threshold = 0.0\n",
    "        self.eval_avg = True\n",
    "\n",
    "    def re_init_ema(self):\n",
    "        if self.use_ema:\n",
    "            self.model_ema = LitEma(self.model)\n",
    "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
    "\n",
    "    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
    "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "        if exists(given_betas):\n",
    "            betas = given_betas\n",
    "        else:\n",
    "            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n",
    "                                       cosine_s=cosine_s)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.linear_start = linear_start\n",
    "        self.linear_end = linear_end\n",
    "        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n",
    "\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(betas))\n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
    "        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n",
    "                    1. - alphas_cumprod) + self.v_posterior * betas\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n",
    "        self.register_buffer('posterior_mean_coef1', to_torch(\n",
    "            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n",
    "        self.register_buffer('posterior_mean_coef2', to_torch(\n",
    "            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            lvlb_weights = self.betas ** 2 / (\n",
    "                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n",
    "        elif self.parameterization == \"x0\":\n",
    "            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n",
    "        else:\n",
    "            raise NotImplementedError(\"mu not supported\")\n",
    "        # TODO how to choose this term\n",
    "        lvlb_weights[0] = lvlb_weights[1]\n",
    "        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n",
    "        assert not torch.isnan(self.lvlb_weights).all()\n",
    "\n",
    "    @contextmanager\n",
    "    def ema_scope(self, context=None):\n",
    "        if self.use_ema:\n",
    "            self.model_ema.store(self.model.parameters())\n",
    "            self.model_ema.copy_to(self.model)\n",
    "            if context is not None:\n",
    "                print(f\"{context}: Switched to EMA weights\")\n",
    "        try:\n",
    "            yield None\n",
    "        finally:\n",
    "            if self.use_ema:\n",
    "                self.model_ema.restore(self.model.parameters())\n",
    "                if context is not None:\n",
    "                    print(f\"{context}: Restored training weights\")\n",
    "\n",
    "    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n",
    "        sd = torch.load(path, map_location=\"cpu\")\n",
    "        if \"state_dict\" in list(sd.keys()):\n",
    "            sd = sd[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n",
    "            sd, strict=False)\n",
    "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "        if len(missing) > 0:\n",
    "            print(f\"Missing Keys: {missing}\")\n",
    "        if len(unexpected) > 0:\n",
    "            print(f\"Unexpected Keys: {unexpected}\")\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n",
    "        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, x, t, clip_denoised: bool):\n",
    "        model_out = self.model(x, t)\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n",
    "        noise = noise_like(x.shape, device, repeat_noise)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_intermediates=False):\n",
    "        device = self.betas.device\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        intermediates = [img]\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n",
    "                                clip_denoised=self.clip_denoised)\n",
    "            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, return_intermediates=False):\n",
    "        image_size = self.image_size\n",
    "        channels = self.channels\n",
    "        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n",
    "                                  return_intermediates=return_intermediates)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n",
    "\n",
    "    def get_loss(self, pred, target, mean=True):\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = (target - pred).abs()\n",
    "            if mean:\n",
    "                loss = loss.mean()\n",
    "        elif self.loss_type == 'l2':\n",
    "            if mean:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred)\n",
    "            else:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        model_out = self.model(x_noisy, t)\n",
    "\n",
    "        loss_dict = {}\n",
    "        if self.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        elif self.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Paramterization {self.parameterization} not yet supported\")\n",
    "\n",
    "        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n",
    "\n",
    "        log_prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n",
    "        loss_simple = loss.mean() * self.l_simple_weight\n",
    "\n",
    "        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n",
    "        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n",
    "\n",
    "        loss = loss_simple + self.original_elbo_weight * loss_vlb\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss': loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        # b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n",
    "        # assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "        return self.p_losses(x, t, *args, **kwargs)\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = rearrange(x, 'b h w c -> b c h w')\n",
    "        x = x.to(memory_format=torch.contiguous_format).float()\n",
    "        return x\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        x = self.get_input(batch, self.first_stage_key)\n",
    "        loss, loss_dict = self(x)\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        self.cond_stage_model.train()  ###\n",
    "\n",
    "        loss, loss_dict = self.shared_step(batch)\n",
    "\n",
    "        self.log_dict(loss_dict, prog_bar=True,\n",
    "                    logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            lr = self.optimizers().param_groups[0]['lr']\n",
    "            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, data, num_samples, ddim_steps=300, HW=None, limit=None, state=None):\n",
    "        # fmri_embedding: n, seq_len, embed_dim\n",
    "        all_samples = []\n",
    "        if HW is None:\n",
    "            shape = (self.p_channels,\n",
    "                self.p_image_size, self.p_image_size)\n",
    "        else:\n",
    "            num_resolutions = len(self.ch_mult)\n",
    "            shape = (self.p_channels,\n",
    "                HW[0] // 2**(num_resolutions-1), HW[1] // 2**(num_resolutions-1))\n",
    "\n",
    "        model = self\n",
    "        sampler = PLMSSampler(model)\n",
    "        # sampler = DDIMSampler(model)\n",
    "        model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            state = torch.cuda.get_rng_state() if state is None else state\n",
    "            torch.cuda.set_rng_state(state)\n",
    "        else:\n",
    "            state = torch.get_rng_state() if state is None else state\n",
    "            torch.set_rng_state(state)\n",
    "\n",
    "        # rng = torch.Generator(device=self.device).manual_seed(2022).set_state(state)\n",
    "\n",
    "        # state = torch.cuda.get_rng_state()\n",
    "        with model.ema_scope():\n",
    "            for count, item in enumerate(zip(data['eeg'], data['image'])):\n",
    "                if limit is not None:\n",
    "                    if count >= limit:\n",
    "                        break\n",
    "                latent = item[0] # fmri embedding\n",
    "                gt_image = rearrange(item[1], 'h w c -> 1 c h w') # h w c\n",
    "                print(f\"rendering {num_samples} examples in {ddim_steps} steps.\")\n",
    "                # c = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                c, re_latent = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                conditioning=c,\n",
    "                                                batch_size=num_samples,\n",
    "                                                shape=shape,\n",
    "                                                verbose=False,\n",
    "                                                generator=None)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,min=0.0, max=1.0)\n",
    "                gt_image = torch.clamp((gt_image+1.0)/2.0,min=0.0, max=1.0)\n",
    "\n",
    "                all_samples.append(torch.cat([gt_image.detach().cpu(), x_samples_ddim.detach().cpu()], dim=0)) # put groundtruth at first\n",
    "\n",
    "        # display as grid\n",
    "        grid = torch.stack(all_samples, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=num_samples+1)\n",
    "\n",
    "        # to image\n",
    "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        return grid, (255. * torch.stack(all_samples, 0).cpu().numpy()).astype(np.uint8), state\n",
    "\n",
    "    def save_images(self, all_samples, suffix=0):\n",
    "        # print('output_path')\n",
    "        # print(self.output_path)\n",
    "        if self.output_path is not None:\n",
    "            os.makedirs(os.path.join(self.output_path, 'val', f'{self.validation_count}_{suffix}'), exist_ok=True)\n",
    "            for sp_idx, imgs in enumerate(all_samples):\n",
    "                # for copy_idx, img in enumerate(imgs[1:]):\n",
    "                for copy_idx, img in enumerate(imgs):\n",
    "                    img = rearrange(img, 'c h w -> h w c')\n",
    "                    Image.fromarray(img).save(os.path.join(self.output_path, 'val',\n",
    "                                    f'{self.validation_count}_{suffix}', f'test{sp_idx}-{copy_idx}.png'))\n",
    "\n",
    "    def full_validation(self, batch, state=None):\n",
    "        print('###### run full validation! ######\\n')\n",
    "        grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=5, limit=None, state=state)\n",
    "        metric, metric_list = self.get_eval_metric(all_samples)\n",
    "        self.save_images(all_samples, suffix='%.4f'%metric[-1])\n",
    "        metric_dict = {f'val/{k}_full':v for k, v in zip(metric_list, metric)}\n",
    "        # self.logger.log_metrics(metric_dict)\n",
    "        grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "        # self.logger.log_image(key=f'samples_test_full', images=[grid_imgs])\n",
    "        if metric[-1] > self.best_val:\n",
    "            self.best_val = metric[-1]\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict': self.state_dict(),\n",
    "                    'config': self.main_config,\n",
    "                    'state': state\n",
    "\n",
    "                },\n",
    "                os.path.join(self.output_path, 'checkpoint_best.pth')\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if batch_idx != 0:\n",
    "            return\n",
    "\n",
    "        if self.validation_count % 5 == 0 and self.trainer.current_epoch != 0:\n",
    "            self.full_validation(batch)\n",
    "        else:\n",
    "            # pass\n",
    "            grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=3, limit=5)\n",
    "            metric, metric_list = self.get_eval_metric(all_samples, avg=self.eval_avg)\n",
    "            grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "            # self.logger.log_image(key=f'samples_test', images=[grid_imgs])\n",
    "            metric_dict = {f'val/{k}':v for k, v in zip(metric_list, metric)}\n",
    "            # self.logger.log_metrics(metric_dict)\n",
    "            if metric[-1] > self.run_full_validation_threshold:\n",
    "                self.full_validation(batch, state=state)\n",
    "        self.validation_count += 1\n",
    "\n",
    "    def get_eval_metric(self, samples, avg=True):\n",
    "        metric_list = ['mse', 'pcc', 'ssim', 'psm']\n",
    "        res_list = []\n",
    "\n",
    "        gt_images = [img[0] for img in samples]\n",
    "        gt_images = rearrange(np.stack(gt_images), 'n c h w -> n h w c')\n",
    "        samples_to_run = np.arange(1, len(samples[0])) if avg else [1]\n",
    "        for m in metric_list:\n",
    "            res_part = []\n",
    "            for s in samples_to_run:\n",
    "                pred_images = [img[s] for img in samples]\n",
    "                pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "                res = get_similarity_metric(pred_images, gt_images, method='pair-wise', metric_name=m)\n",
    "                res_part.append(np.mean(res))\n",
    "            res_list.append(np.mean(res_part))\n",
    "        res_part = []\n",
    "        for s in samples_to_run:\n",
    "            pred_images = [img[s] for img in samples]\n",
    "            pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "            res = get_similarity_metric(pred_images, gt_images, 'class', None,\n",
    "                            n_way=50, num_trials=50, top_k=1, device='cuda')\n",
    "            res_part.append(np.mean(res))\n",
    "        res_list.append(np.mean(res_part))\n",
    "        res_list.append(np.max(res_part))\n",
    "        metric_list.append('top-1-class')\n",
    "        metric_list.append('top-1-class (max)')\n",
    "\n",
    "        return res_list, metric_list\n",
    "\n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        if self.use_ema:\n",
    "            self.model_ema(self.model)\n",
    "\n",
    "    def _get_rows_from_list(self, samples):\n",
    "        n_imgs_per_row = len(samples)\n",
    "        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n",
    "        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n",
    "        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n",
    "        return denoise_grid\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.first_stage_key)\n",
    "        N = min(x.shape[0], N)\n",
    "        n_row = min(x.shape[0], n_row)\n",
    "        x = x.to(self.device)[:N]\n",
    "        log[\"inputs\"] = x\n",
    "\n",
    "        # get diffusion row\n",
    "        diffusion_row = list()\n",
    "        x_start = x[:n_row]\n",
    "\n",
    "        for t in range(self.num_timesteps):\n",
    "            if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n",
    "                t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n",
    "                t = t.to(self.device).long()\n",
    "                noise = torch.randn_like(x_start)\n",
    "                x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "                diffusion_row.append(x_noisy)\n",
    "\n",
    "        log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n",
    "\n",
    "        if sample:\n",
    "            # get denoise row\n",
    "            with self.ema_scope(\"Plotting\"):\n",
    "                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n",
    "\n",
    "            log[\"samples\"] = samples\n",
    "            log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n",
    "\n",
    "        if return_keys:\n",
    "            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n",
    "                return log\n",
    "            else:\n",
    "                return {key: log[key] for key in return_keys}\n",
    "        return log\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        params = list(self.model.parameters())\n",
    "        if self.learn_logvar:\n",
    "            params = params + [self.logvar]\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "        return opt\n",
    "\n",
    "\n",
    "class LatentDiffusion(DDPM):\n",
    "    \"\"\"main class\"\"\"\n",
    "    def __init__(self,\n",
    "                first_stage_config,\n",
    "                cond_stage_config,\n",
    "                num_timesteps_cond=None,\n",
    "                cond_stage_key=\"image\",\n",
    "                cond_stage_trainable=True,\n",
    "                concat_mode=True,\n",
    "                cond_stage_forward=None,\n",
    "                conditioning_key=None,\n",
    "                scale_factor=1.0,\n",
    "                scale_by_std=False,\n",
    "                *args, **kwargs):\n",
    "        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n",
    "        self.scale_by_std = scale_by_std\n",
    "        assert self.num_timesteps_cond <= kwargs['timesteps']\n",
    "        # for backwards compatibility after implementation of DiffusionWrapper\n",
    "        if conditioning_key is None:\n",
    "            conditioning_key = 'concat' if concat_mode else 'crossattn'\n",
    "        if cond_stage_config == '__is_unconditional__':\n",
    "            conditioning_key = None\n",
    "        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n",
    "        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n",
    "        super().__init__(conditioning_key=conditioning_key, *args, **kwargs)\n",
    "        self.concat_mode = concat_mode\n",
    "        self.cond_stage_trainable = cond_stage_trainable\n",
    "        self.cond_stage_key = cond_stage_key\n",
    "        try:\n",
    "            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n",
    "        except:\n",
    "            self.num_downs = 0\n",
    "        if not scale_by_std:\n",
    "            self.scale_factor = scale_factor\n",
    "        else:\n",
    "            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n",
    "        self.instantiate_first_stage(first_stage_config)\n",
    "        self.instantiate_cond_stage(cond_stage_config)\n",
    "\n",
    "        self.cond_stage_forward = cond_stage_forward\n",
    "        self.clip_denoised = False\n",
    "        self.bbox_tokenizer = None\n",
    "\n",
    "        self.restarted_from_ckpt = False\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys)\n",
    "            self.restarted_from_ckpt = True\n",
    "        self.train_cond_stage_only = False\n",
    "        self.clip_tune = True\n",
    "        if self.clip_tune:\n",
    "            self.image_embedder = FrozenImageEmbedder()\n",
    "        self.cls_tune = False\n",
    "\n",
    "    def make_cond_schedule(self, ):\n",
    "        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n",
    "        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n",
    "        self.cond_ids[:self.num_timesteps_cond] = ids\n",
    "\n",
    "    @rank_zero_only\n",
    "    @torch.no_grad()\n",
    "    def on_train_batch_start(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # only for very first batch\n",
    "        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n",
    "            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n",
    "            # set rescale weight to 1./std of encodings\n",
    "            print(\"### USING STD-RESCALING ###\")\n",
    "            x = super().get_input(batch, self.first_stage_key)\n",
    "            x = x.to(self.device)\n",
    "            encoder_posterior = self.encode_first_stage(x)\n",
    "            z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "            del self.scale_factor\n",
    "            self.register_buffer('scale_factor', 1. / z.flatten().std())\n",
    "            print(f\"setting self.scale_factor to {self.scale_factor}\")\n",
    "            print(\"### USING STD-RESCALING ###\")\n",
    "\n",
    "    def register_schedule(self,\n",
    "                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
    "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n",
    "\n",
    "        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n",
    "        if self.shorten_cond_schedule:\n",
    "            self.make_cond_schedule()\n",
    "\n",
    "    def instantiate_first_stage(self, config):\n",
    "        model = instantiate_from_config(config)\n",
    "        self.first_stage_model = model.eval()\n",
    "\n",
    "    def freeze_diffusion_model(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_diffusion_model(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_cond_stage(self):\n",
    "        for param in self.cond_stage_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_cond_stage(self):\n",
    "        for param in self.cond_stage_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    def freeze_first_stage(self):\n",
    "        self.first_stage_model.trainable = False\n",
    "        for param in self.first_stage_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_first_stage(self):\n",
    "        self.first_stage_model.trainable = True\n",
    "        for param in self.first_stage_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_whole_model(self):\n",
    "        self.first_stage_model.trainable = False\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_whole_model(self):\n",
    "        self.first_stage_model.trainable = True\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def instantiate_cond_stage(self, config):\n",
    "        if not self.cond_stage_trainable:\n",
    "            if config == \"__is_first_stage__\":\n",
    "                print(\"Using first stage also as cond stage.\")\n",
    "                self.cond_stage_model = self.first_stage_model\n",
    "            elif config == \"__is_unconditional__\":\n",
    "                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n",
    "                self.cond_stage_model = None\n",
    "                # self.be_unconditional = True\n",
    "            else:\n",
    "                model = instantiate_from_config(config)\n",
    "                self.cond_stage_model = model.eval()\n",
    "                # self.cond_stage_model.train = disabled_train\n",
    "                for param in self.cond_stage_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            assert config != '__is_first_stage__'\n",
    "            assert config != '__is_unconditional__'\n",
    "            model = instantiate_from_config(config)\n",
    "            self.cond_stage_model = model\n",
    "\n",
    "    def _get_denoise_row_from_list(self, samples, desc='', force_no_decoder_quantization=False):\n",
    "        denoise_row = []\n",
    "        for zd in tqdm(samples, desc=desc):\n",
    "            denoise_row.append(self.decode_first_stage(zd.to(self.device),\n",
    "                                                            force_not_quantize=force_no_decoder_quantization))\n",
    "        n_imgs_per_row = len(denoise_row)\n",
    "        denoise_row = torch.stack(denoise_row)  # n_log_step, n_row, C, H, W\n",
    "        denoise_grid = rearrange(denoise_row, 'n b c h w -> b n c h w')\n",
    "        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n",
    "        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n",
    "        return denoise_grid\n",
    "\n",
    "    def get_first_stage_encoding(self, encoder_posterior):\n",
    "        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n",
    "            z = encoder_posterior.sample()\n",
    "        elif isinstance(encoder_posterior, torch.Tensor):\n",
    "            z = encoder_posterior\n",
    "        else:\n",
    "            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n",
    "        return self.scale_factor * z\n",
    "\n",
    "    def get_learned_conditioning(self, c):\n",
    "        # self.cond_stage_model.eval()\n",
    "        if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n",
    "            c, re_latent = self.cond_stage_model.encode(c)\n",
    "            # c = self.cond_stage_model.encode(c)\n",
    "        else:\n",
    "            c, re_latent = self.cond_stage_model(c)\n",
    "            # c = self.cond_stage_model(c)\n",
    "        # return c\n",
    "        return c, re_latent\n",
    "\n",
    "    def meshgrid(self, h, w):\n",
    "        y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n",
    "        x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n",
    "\n",
    "        arr = torch.cat([y, x], dim=-1)\n",
    "        return arr\n",
    "\n",
    "    def delta_border(self, h, w):\n",
    "        \"\"\"\n",
    "        :param h: height\n",
    "        :param w: width\n",
    "        :return: normalized distance to image border,\n",
    "         wtith min distance = 0 at border and max dist = 0.5 at image center\n",
    "        \"\"\"\n",
    "        lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n",
    "        arr = self.meshgrid(h, w) / lower_right_corner\n",
    "        dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n",
    "        dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n",
    "        edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n",
    "        return edge_dist\n",
    "\n",
    "    def get_weighting(self, h, w, Ly, Lx, device):\n",
    "        weighting = self.delta_border(h, w)\n",
    "        weighting = torch.clip(weighting, self.split_input_params[\"clip_min_weight\"],\n",
    "                               self.split_input_params[\"clip_max_weight\"], )\n",
    "        weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n",
    "\n",
    "        if self.split_input_params[\"tie_braker\"]:\n",
    "            L_weighting = self.delta_border(Ly, Lx)\n",
    "            L_weighting = torch.clip(L_weighting,\n",
    "                                     self.split_input_params[\"clip_min_tie_weight\"],\n",
    "                                     self.split_input_params[\"clip_max_tie_weight\"])\n",
    "\n",
    "            L_weighting = L_weighting.view(1, 1, Ly * Lx).to(device)\n",
    "            weighting = weighting * L_weighting\n",
    "        return weighting\n",
    "\n",
    "    def get_fold_unfold(self, x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n",
    "        \"\"\"\n",
    "        :param x: img of size (bs, c, h, w)\n",
    "        :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n",
    "        \"\"\"\n",
    "        bs, nc, h, w = x.shape\n",
    "\n",
    "        # number of crops in image\n",
    "        Ly = (h - kernel_size[0]) // stride[0] + 1\n",
    "        Lx = (w - kernel_size[1]) // stride[1] + 1\n",
    "\n",
    "        if uf == 1 and df == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n",
    "\n",
    "        elif uf > 1 and df == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n",
    "                                dilation=1, padding=0,\n",
    "                                stride=(stride[0] * uf, stride[1] * uf))\n",
    "            fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n",
    "\n",
    "        elif df > 1 and uf == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n",
    "                                dilation=1, padding=0,\n",
    "                                stride=(stride[0] // df, stride[1] // df))\n",
    "            fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return fold, unfold, normalization, weighting\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_input(self, batch, k, return_first_stage_outputs=False, force_c_encode=False,\n",
    "                  cond_key=None, return_original_cond=False, bs=None):\n",
    "        x = super().get_input(batch, k)\n",
    "        if bs is not None:\n",
    "            x = x[:bs]\n",
    "        x = x.to(self.device)\n",
    "        encoder_posterior = self.encode_first_stage(x)\n",
    "        # print('encoder_posterior.shape')\n",
    "        # print(encoder_posterior.shape)\n",
    "        z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "        # print('z.shape')\n",
    "        # print(z.shape)\n",
    "        # print(cond_key)\n",
    "        # print(self.cond_stage_key)\n",
    "        # print(cond_key)\n",
    "        if self.model.conditioning_key is not None:\n",
    "            if cond_key is None:\n",
    "                cond_key = self.cond_stage_key\n",
    "            if cond_key != self.first_stage_key:\n",
    "                if cond_key in ['caption', 'coordinates_bbox','fmri', 'eeg']:\n",
    "                    xc = batch[cond_key]\n",
    "                elif cond_key == 'class_label':\n",
    "                    xc = batch\n",
    "                else:\n",
    "                    xc = super().get_input(batch, cond_key).to(self.device)\n",
    "            else:\n",
    "                xc = x\n",
    "            # print('get input')\n",
    "            # print(not self.cond_stage_trainable)\n",
    "            # print(force_c_encode)\n",
    "            if not self.cond_stage_trainable or force_c_encode :\n",
    "                # print('get learned condition')\n",
    "                if isinstance(xc, dict) or isinstance(xc, list):\n",
    "                    # import pudb; pudb.set_trace()\n",
    "                    c, re_latent = self.get_learned_conditioning(xc)\n",
    "                    # c = self.get_learned_conditioning(xc)\n",
    "                else:\n",
    "                    c, re_latent = self.get_learned_conditioning(xc.to(self.device))\n",
    "                    # c = self.get_learned_conditioning(xc.to(self.device))\n",
    "            else:\n",
    "                c = xc\n",
    "            if bs is not None:\n",
    "                c = c[:bs]\n",
    "\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                ckey = __conditioning_keys__[self.model.conditioning_key]\n",
    "                c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n",
    "\n",
    "        else:\n",
    "            c = None\n",
    "            xc = None\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                c = {'pos_x': pos_x, 'pos_y': pos_y}\n",
    "        out = [z, c , batch['label'], batch['image_raw']]\n",
    "        if return_first_stage_outputs:\n",
    "            xrec = self.decode_first_stage(z)\n",
    "            out.extend([x, xrec])\n",
    "        if return_original_cond:\n",
    "            out.append(xc)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n",
    "        if predict_cids:\n",
    "            if z.dim() == 4:\n",
    "                z = torch.argmax(z.exp(), dim=1).long()\n",
    "            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n",
    "            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n",
    "\n",
    "        z = 1. / self.scale_factor * z\n",
    "\n",
    "        if hasattr(self, \"split_input_params\"):\n",
    "            if self.split_input_params[\"patch_distributed_vq\"]:\n",
    "                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n",
    "                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n",
    "                uf = self.split_input_params[\"vqf\"]\n",
    "                bs, nc, h, w = z.shape\n",
    "                if ks[0] > h or ks[1] > w:\n",
    "                    ks = (min(ks[0], h), min(ks[1], w))\n",
    "                    print(\"reducing Kernel\")\n",
    "\n",
    "                if stride[0] > h or stride[1] > w:\n",
    "                    stride = (min(stride[0], h), min(stride[1], w))\n",
    "                    print(\"reducing stride\")\n",
    "\n",
    "                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n",
    "\n",
    "                z = unfold(z)  # (bn, nc * prod(**ks), L)\n",
    "                # 1. Reshape to img shape\n",
    "                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "\n",
    "                # 2. apply model loop over last dim\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n",
    "                                                                 force_not_quantize=predict_cids or force_not_quantize)\n",
    "                                   for i in range(z.shape[-1])]\n",
    "                else:\n",
    "\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n",
    "                                   for i in range(z.shape[-1])]\n",
    "\n",
    "                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
    "                o = o * weighting\n",
    "                # Reverse 1. reshape to img shape\n",
    "                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
    "                # stitch crops together\n",
    "                decoded = fold(o)\n",
    "                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
    "                return decoded\n",
    "            else:\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "                else:\n",
    "                    return self.first_stage_model.decode(z)\n",
    "\n",
    "        else:\n",
    "            if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "            else:\n",
    "                return self.first_stage_model.decode(z)\n",
    "\n",
    "    # same as above but without decorator\n",
    "    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n",
    "        if predict_cids:\n",
    "            if z.dim() == 4:\n",
    "                z = torch.argmax(z.exp(), dim=1).long()\n",
    "            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n",
    "            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n",
    "\n",
    "        z = 1. / self.scale_factor * z\n",
    "\n",
    "        if hasattr(self, \"split_input_params\"):\n",
    "            if self.split_input_params[\"patch_distributed_vq\"]:\n",
    "                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n",
    "                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n",
    "                uf = self.split_input_params[\"vqf\"]\n",
    "                bs, nc, h, w = z.shape\n",
    "                if ks[0] > h or ks[1] > w:\n",
    "                    ks = (min(ks[0], h), min(ks[1], w))\n",
    "                    print(\"reducing Kernel\")\n",
    "\n",
    "                if stride[0] > h or stride[1] > w:\n",
    "                    stride = (min(stride[0], h), min(stride[1], w))\n",
    "                    print(\"reducing stride\")\n",
    "\n",
    "                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n",
    "\n",
    "                z = unfold(z)  # (bn, nc * prod(**ks), L)\n",
    "                # 1. Reshape to img shape\n",
    "                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "\n",
    "                # 2. apply model loop over last dim\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n",
    "                                                                 force_not_quantize=predict_cids or force_not_quantize)\n",
    "                                   for i in range(z.shape[-1])]\n",
    "                else:\n",
    "\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n",
    "                                   for i in range(z.shape[-1])]\n",
    "\n",
    "                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
    "                o = o * weighting\n",
    "                # Reverse 1. reshape to img shape\n",
    "                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
    "                # stitch crops together\n",
    "                decoded = fold(o)\n",
    "                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
    "                return decoded\n",
    "            else:\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "                else:\n",
    "                    return self.first_stage_model.decode(z)\n",
    "\n",
    "        else:\n",
    "            if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "            else:\n",
    "                return self.first_stage_model.decode(z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_first_stage(self, x):\n",
    "        return self.first_stage_model.encode(x)\n",
    "\n",
    "    def shared_step(self, batch, **kwargs):\n",
    "        self.freeze_first_stage()\n",
    "        # print('share step\\'s get input')\n",
    "        x, c, label, image_raw = self.get_input(batch, self.first_stage_key)\n",
    "        # print('get input shape')\n",
    "        # print('x.shape')\n",
    "        # print(x.shape)\n",
    "        # print('c.shape')\n",
    "        # print(c.shape)\n",
    "        if self.return_cond:\n",
    "            loss, cc = self(x, c, label, image_raw)\n",
    "            return loss, cc\n",
    "        else:\n",
    "            loss = self(x, c, label, image_raw)\n",
    "            return loss\n",
    "\n",
    "    def forward(self, x, c, label, image_raw, *args, **kwargs):\n",
    "        # print(self.num_timesteps)\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "        # print('t.shape')\n",
    "        # print(t.shape)\n",
    "        if self.model.conditioning_key is not None:\n",
    "            assert c is not None\n",
    "            imgs = c\n",
    "            if self.cond_stage_trainable:\n",
    "                # c = self.get_learned_conditioning(c)\n",
    "                c, re_latent = self.get_learned_conditioning(c)\n",
    "                # print('c.shape')\n",
    "                # print(c.shape)\n",
    "\n",
    "        prefix = 'train' if self.training else 'val'\n",
    "        loss, loss_dict = self.p_losses(x, c, t, *args, **kwargs)\n",
    "        # pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "        # rencon = self.cond_stage_model.recon(re_latent)\n",
    "        if self.clip_tune:\n",
    "            image_embeds = self.image_embedder(image_raw)\n",
    "            loss_clip = self.cond_stage_model.get_clip_loss(re_latent, image_embeds)\n",
    "        # loss_recon = self.recon_loss(imgs, rencon)\n",
    "        # loss_cls = self.cls_loss(label, pre_cls)\n",
    "            loss += loss_clip\n",
    "        # loss += loss_cls # loss_recon +  #(self.original_elbo_weight * loss_vlb)\n",
    "        # loss_dict.update({f'{prefix}/loss_recon': loss_recon})\n",
    "        # loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "            loss_dict.update({f'{prefix}/loss_clip': loss_clip})\n",
    "        if self.cls_tune:\n",
    "            pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "            loss_cls = self.cls_loss(label, pre_cls)\n",
    "            # image_embeds = self.image_embedder(image_raw)\n",
    "            # loss_clip = self.cond_stage_model.get_clip_loss(re_latent, image_embeds)\n",
    "        # loss_recon = self.recon_loss(imgs, rencon)\n",
    "        # loss_cls = self.cls_loss(label, pre_cls)\n",
    "            loss += loss_cls\n",
    "        # loss += loss_cls # loss_recon +  #(self.original_elbo_weight * loss_vlb)\n",
    "        # loss_dict.update({f'{prefix}/loss_recon': loss_recon})\n",
    "        # loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "            loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "                # if self.return_cond:\n",
    "                    # return self.p_losses(x, c, t, *args, **kwargs), c\n",
    "        # return self.p_losses(x, c, t, *args, **kwargs)\n",
    "        if self.return_cond:\n",
    "            return loss, loss_dict, c\n",
    "        return loss, loss_dict\n",
    "    # def recon_loss(self, )\n",
    "    def recon_loss(self, imgs, pred):\n",
    "        \"\"\"\n",
    "        imgs: [N, 1, num_voxels]\n",
    "        pred: [N, L, p]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "        # target = self.patchify(imgs)\n",
    "\n",
    "        loss = (pred - imgs) ** 2\n",
    "        loss = loss.mean()\n",
    "        # loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        # loss = (loss * mask).sum() / mask.sum()  if mask.sum() != 0 else (loss * mask).sum() # mean loss on removed patches\n",
    "        return loss\n",
    "    def cls_loss(self, label, pred):\n",
    "        return torch.nn.CrossEntropyLoss()(pred, label)\n",
    "\n",
    "    def _rescale_annotations(self, bboxes, crop_coordinates):  # TODO: move to dataset\n",
    "        def rescale_bbox(bbox):\n",
    "            x0 = torch.clamp((bbox[0] - crop_coordinates[0]) / crop_coordinates[2])\n",
    "            y0 = torch.clamp((bbox[1] - crop_coordinates[1]) / crop_coordinates[3])\n",
    "            w = min(bbox[2] / crop_coordinates[2], 1 - x0)\n",
    "            h = min(bbox[3] / crop_coordinates[3], 1 - y0)\n",
    "            return x0, y0, w, h\n",
    "\n",
    "        return [rescale_bbox(b) for b in bboxes]\n",
    "\n",
    "    def apply_model(self, x_noisy, t, cond, return_ids=False):\n",
    "\n",
    "        if isinstance(cond, dict):\n",
    "            # hybrid case, cond is exptected to be a dict\n",
    "            pass\n",
    "        else:\n",
    "            if not isinstance(cond, list):\n",
    "                cond = [cond]\n",
    "            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n",
    "            cond = {key: cond}\n",
    "\n",
    "        x_recon = self.model(x_noisy, t, **cond)\n",
    "        # print('x_recon')\n",
    "        # if isinstance(x_recon, tuple):\n",
    "        #     print('is tuple')\n",
    "        #     # print(len(x_recon))\n",
    "        #     # print(x_recon[0].shape)\n",
    "        # else:\n",
    "        #     print(x_recon.shape)\n",
    "\n",
    "        if isinstance(x_recon, tuple) and not return_ids:\n",
    "            return x_recon[0]\n",
    "        else:\n",
    "            return x_recon\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n",
    "               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _prior_bpd(self, x_start):\n",
    "        \"\"\"\n",
    "        Get the prior KL term for the variational lower-bound, measured in\n",
    "        bits-per-dim.\n",
    "        This term can't be optimized, as it only depends on the encoder.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :return: a batch of [N] KL values (in bits), one per batch element.\n",
    "        \"\"\"\n",
    "        batch_size = x_start.shape[0]\n",
    "        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n",
    "        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n",
    "        return mean_flat(kl_prior) / np.log(2.0)\n",
    "\n",
    "    def p_losses(self, x_start, cond, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        # print('p_losses')\n",
    "        # print('noise.shape')\n",
    "        # print(noise.shape)\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        # print('x_noisy[0].shape')\n",
    "        # print(x_noisy[0].shape)\n",
    "        model_output = self.apply_model(x_noisy, t, cond)\n",
    "\n",
    "        loss_dict = {}\n",
    "        prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        if self.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        elif self.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n",
    "        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n",
    "\n",
    "        logvar_t = self.logvar[t].to(self.device)\n",
    "        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n",
    "        # loss = loss_simple / torch.exp(self.logvar) + self.logvar\n",
    "        if self.learn_logvar:\n",
    "            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n",
    "            loss_dict.update({'logvar': self.logvar.data.mean()})\n",
    "\n",
    "        loss = self.l_simple_weight * loss.mean()\n",
    "\n",
    "        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n",
    "        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n",
    "        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n",
    "        loss += (self.original_elbo_weight * loss_vlb)\n",
    "        loss_dict.update({f'{prefix}/loss': loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_codebook_ids=False, quantize_denoised=False,\n",
    "                        return_x0=False, score_corrector=None, corrector_kwargs=None):\n",
    "        t_in = t\n",
    "        model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n",
    "\n",
    "        if score_corrector is not None:\n",
    "            assert self.parameterization == \"eps\"\n",
    "            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n",
    "\n",
    "        if return_codebook_ids:\n",
    "            model_out, logits = model_out\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1., 1.)\n",
    "        if quantize_denoised:\n",
    "            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "        if return_codebook_ids:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, logits\n",
    "        elif return_x0:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, x_recon\n",
    "        else:\n",
    "            return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False,\n",
    "                 return_codebook_ids=False, quantize_denoised=False, return_x0=False,\n",
    "                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised,\n",
    "                                       return_codebook_ids=return_codebook_ids,\n",
    "                                       quantize_denoised=quantize_denoised,\n",
    "                                       return_x0=return_x0,\n",
    "                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n",
    "        if return_x0:\n",
    "            model_mean, _, model_log_variance, x0 = outputs\n",
    "        else:\n",
    "            model_mean, _, model_log_variance = outputs\n",
    "\n",
    "        noise = noise_like(x.shape, device, repeat_noise) * temperature\n",
    "        if noise_dropout > 0.:\n",
    "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "\n",
    "        if return_x0:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0\n",
    "        else:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def progressive_denoising(self, cond, shape, verbose=True, callback=None, quantize_denoised=False,\n",
    "                              img_callback=None, mask=None, x0=None, temperature=1., noise_dropout=0.,\n",
    "                              score_corrector=None, corrector_kwargs=None, batch_size=None, x_T=None, start_T=None,\n",
    "                              log_every_t=None):\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        timesteps = self.num_timesteps\n",
    "        if batch_size is not None:\n",
    "            b = batch_size if batch_size is not None else shape[0]\n",
    "            shape = [batch_size] + list(shape)\n",
    "        else:\n",
    "            b = batch_size = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=self.device)\n",
    "        else:\n",
    "            img = x_T\n",
    "        intermediates = []\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n",
    "                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n",
    "            else:\n",
    "                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = tqdm(reversed(range(0, timesteps)), desc='Progressive Generation',\n",
    "                        total=timesteps) if verbose else reversed(\n",
    "            range(0, timesteps))\n",
    "        if type(temperature) == float:\n",
    "            temperature = [temperature] * timesteps\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != 'hybrid'\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img, x0_partial = self.p_sample(img, cond, ts,\n",
    "                                            clip_denoised=self.clip_denoised,\n",
    "                                            quantize_denoised=quantize_denoised, return_x0=True,\n",
    "                                            temperature=temperature[i], noise_dropout=noise_dropout,\n",
    "                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n",
    "            if mask is not None:\n",
    "                assert x0 is not None\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(x0_partial)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(img, i)\n",
    "        return img, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, cond, shape, return_intermediates=False,\n",
    "                      x_T=None, verbose=True, callback=None, timesteps=None, quantize_denoised=False,\n",
    "                      mask=None, x0=None, img_callback=None, start_T=None,\n",
    "                      log_every_t=None):\n",
    "\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        device = self.betas.device\n",
    "        b = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=device)\n",
    "        else:\n",
    "            img = x_T\n",
    "\n",
    "        intermediates = [img]\n",
    "        if timesteps is None:\n",
    "            timesteps = self.num_timesteps\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(\n",
    "            range(0, timesteps))\n",
    "\n",
    "        if mask is not None:\n",
    "            assert x0 is not None\n",
    "            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != 'hybrid'\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img = self.p_sample(img, cond, ts,\n",
    "                                clip_denoised=self.clip_denoised,\n",
    "                                quantize_denoised=quantize_denoised)\n",
    "            if mask is not None:\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(img, i)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, batch_size=16, return_intermediates=False, x_T=None,\n",
    "               verbose=True, timesteps=None, quantize_denoised=False,\n",
    "               mask=None, x0=None, shape=None,**kwargs):\n",
    "        if shape is None:\n",
    "            shape = (batch_size, self.channels, self.image_size, self.image_size)\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n",
    "                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n",
    "            else:\n",
    "                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n",
    "        return self.p_sample_loop(cond,\n",
    "                                  shape,\n",
    "                                  return_intermediates=return_intermediates, x_T=x_T,\n",
    "                                  verbose=verbose, timesteps=timesteps, quantize_denoised=quantize_denoised,\n",
    "                                  mask=mask, x0=x0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_log(self,cond,batch_size,ddim, ddim_steps,**kwargs):\n",
    "\n",
    "        if ddim:\n",
    "            ddim_sampler = DDIMSampler(self)\n",
    "            shape = (self.channels, self.image_size, self.image_size)\n",
    "            samples, intermediates =ddim_sampler.sample(ddim_steps,batch_size,\n",
    "                                                        shape,cond,verbose=False,**kwargs)\n",
    "\n",
    "        else:\n",
    "            samples, intermediates = self.sample(cond=cond, batch_size=batch_size,\n",
    "                                                 return_intermediates=True,**kwargs)\n",
    "\n",
    "        return samples, intermediates\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., return_keys=None,\n",
    "                   quantize_denoised=True, inpaint=True, plot_denoise_rows=False, plot_progressive_rows=True,\n",
    "                   plot_diffusion_rows=True, **kwargs):\n",
    "\n",
    "        use_ddim = ddim_steps is not None\n",
    "\n",
    "        log = dict()\n",
    "        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key,\n",
    "                                           return_first_stage_outputs=True,\n",
    "                                           force_c_encode=True,\n",
    "                                           return_original_cond=True,\n",
    "                                           bs=N)\n",
    "        N = min(x.shape[0], N)\n",
    "        n_row = min(x.shape[0], n_row)\n",
    "        log[\"inputs\"] = x\n",
    "        log[\"reconstruction\"] = xrec\n",
    "        if self.model.conditioning_key is not None:\n",
    "            if hasattr(self.cond_stage_model, \"decode\"):\n",
    "                xc = self.cond_stage_model.decode(c)\n",
    "                log[\"conditioning\"] = xc\n",
    "            elif self.cond_stage_key in [\"caption\"]:\n",
    "                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"caption\"])\n",
    "                log[\"conditioning\"] = xc\n",
    "            elif self.cond_stage_key == 'class_label':\n",
    "                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"])\n",
    "                log['conditioning'] = xc\n",
    "            elif isimage(xc):\n",
    "                log[\"conditioning\"] = xc\n",
    "            if ismap(xc):\n",
    "                log[\"original_conditioning\"] = self.to_rgb(xc)\n",
    "\n",
    "        if plot_diffusion_rows:\n",
    "            # get diffusion row\n",
    "            diffusion_row = list()\n",
    "            z_start = z[:n_row]\n",
    "            for t in range(self.num_timesteps):\n",
    "                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n",
    "                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n",
    "                    t = t.to(self.device).long()\n",
    "                    noise = torch.randn_like(z_start)\n",
    "                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n",
    "                    diffusion_row.append(self.decode_first_stage(z_noisy))\n",
    "\n",
    "            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n",
    "            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n",
    "            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n",
    "            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n",
    "            log[\"diffusion_row\"] = diffusion_grid\n",
    "\n",
    "        if sample:\n",
    "            # get denoise row\n",
    "            with self.ema_scope(\"Plotting\"):\n",
    "                samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n",
    "                                                         ddim_steps=ddim_steps,eta=ddim_eta)\n",
    "                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n",
    "            x_samples = self.decode_first_stage(samples)\n",
    "            log[\"samples\"] = x_samples\n",
    "            if plot_denoise_rows:\n",
    "                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n",
    "                log[\"denoise_row\"] = denoise_grid\n",
    "\n",
    "            if quantize_denoised and not isinstance(self.first_stage_model, AutoencoderKL) and not isinstance(\n",
    "                    self.first_stage_model, IdentityFirstStage):\n",
    "                # also display when quantizing x0 while sampling\n",
    "                with self.ema_scope(\"Plotting Quantized Denoised\"):\n",
    "                    samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n",
    "                                                             ddim_steps=ddim_steps,eta=ddim_eta,\n",
    "                                                             quantize_denoised=True)\n",
    "                    # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True,\n",
    "                    #                                      quantize_denoised=True)\n",
    "                x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "                log[\"samples_x0_quantized\"] = x_samples\n",
    "\n",
    "            if inpaint:\n",
    "                # make a simple center square\n",
    "                b, h, w = z.shape[0], z.shape[2], z.shape[3]\n",
    "                mask = torch.ones(N, h, w).to(self.device)\n",
    "                # zeros will be filled in\n",
    "                mask[:, h // 4:3 * h // 4, w // 4:3 * w // 4] = 0.\n",
    "                mask = mask[:, None, ...]\n",
    "                with self.ema_scope(\"Plotting Inpaint\"):\n",
    "\n",
    "                    samples, _ = self.sample_log(cond=c,batch_size=N,ddim=use_ddim, eta=ddim_eta,\n",
    "                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n",
    "                x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "                log[\"samples_inpainting\"] = x_samples\n",
    "                log[\"mask\"] = mask\n",
    "\n",
    "                # outpaint\n",
    "                with self.ema_scope(\"Plotting Outpaint\"):\n",
    "                    samples, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,eta=ddim_eta,\n",
    "                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n",
    "                x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "                log[\"samples_outpainting\"] = x_samples\n",
    "\n",
    "        if plot_progressive_rows:\n",
    "            with self.ema_scope(\"Plotting Progressives\"):\n",
    "                img, progressives = self.progressive_denoising(c,\n",
    "                                                               shape=(self.channels, self.image_size, self.image_size),\n",
    "                                                               batch_size=N)\n",
    "            prog_row = self._get_denoise_row_from_list(progressives, desc=\"Progressive Generation\")\n",
    "            log[\"progressive_row\"] = prog_row\n",
    "\n",
    "        if return_keys:\n",
    "            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n",
    "                return log\n",
    "            else:\n",
    "                return {key: log[key] for key in return_keys}\n",
    "        return log\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        if self.train_cond_stage_only:\n",
    "            print(f\"{self.__class__.__name__}: Only optimizing conditioner params!\")\n",
    "            cond_parms = [p for n, p in self.named_parameters()\n",
    "                    if 'attn2' in n or 'time_embed_condtion' in n or 'norm2' in n]\n",
    "            # cond_parms = [p for n, p in self.named_parameters()\n",
    "                    # if 'time_embed_condtion' in n]\n",
    "            # cond_parms = []\n",
    "\n",
    "            params = list(self.cond_stage_model.parameters()) + cond_parms\n",
    "\n",
    "            for p in params:\n",
    "                p.requires_grad = True\n",
    "\n",
    "        else:\n",
    "            params = list(self.model.parameters())\n",
    "            if self.cond_stage_trainable:\n",
    "                print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n",
    "                params = params + list(self.cond_stage_model.parameters())\n",
    "            if self.learn_logvar:\n",
    "                print('Diffusion model optimizing logvar')\n",
    "                params.append(self.logvar)\n",
    "\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            assert 'target' in self.scheduler_config\n",
    "            scheduler = instantiate_from_config(self.scheduler_config)\n",
    "\n",
    "            print(\"Setting up LambdaLR scheduler...\")\n",
    "            scheduler = [\n",
    "                {\n",
    "                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1\n",
    "                }]\n",
    "            return [opt], scheduler\n",
    "\n",
    "        return opt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def to_rgb(self, x):\n",
    "        x = x.float()\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n",
    "        x = nn.functional.conv2d(x, weight=self.colorize)\n",
    "        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffusionWrapper(pl.LightningModule):\n",
    "    def __init__(self, diff_model_config, conditioning_key):\n",
    "        super().__init__()\n",
    "        self.diffusion_model = instantiate_from_config(diff_model_config)\n",
    "        self.conditioning_key = conditioning_key\n",
    "        assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm']\n",
    "\n",
    "    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None):\n",
    "        if self.conditioning_key is None:\n",
    "            out = self.diffusion_model(x, t)\n",
    "        elif self.conditioning_key == 'concat':\n",
    "            xc = torch.cat([x] + c_concat, dim=1)\n",
    "            out = self.diffusion_model(xc, t)\n",
    "        elif self.conditioning_key == 'crossattn':\n",
    "            cc = torch.cat(c_crossattn, 1)\n",
    "            out = self.diffusion_model(x, t, context=cc)\n",
    "        elif self.conditioning_key == 'hybrid':\n",
    "            xc = torch.cat([x] + [c_concat], dim=1)\n",
    "            cc = torch.cat([c_crossattn], dim=1)\n",
    "            out = self.diffusion_model(xc, t, context=cc)\n",
    "        elif self.conditioning_key == 'adm':\n",
    "            cc = c_crossattn[0]\n",
    "            out = self.diffusion_model(x, t, y=cc)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EEGClassifier(pl.LightningModule):\n",
    "    \"\"\"main class\"\"\"\n",
    "    def __init__(self,\n",
    "                first_stage_config,\n",
    "                cond_stage_config,\n",
    "                num_timesteps_cond=None,\n",
    "                cond_stage_key=\"image\",\n",
    "                cond_stage_trainable=True,\n",
    "                concat_mode=True,\n",
    "                cond_stage_forward=None,\n",
    "                conditioning_key=None,\n",
    "                scale_factor=1.0,\n",
    "                scale_by_std=False,\n",
    "                *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # self.use_scheduler = scheduler_config is not None\n",
    "        # if self.use_scheduler:\n",
    "        #     self.scheduler_config = scheduler_config\n",
    "        self.cond_stage_trainable = True\n",
    "        self.main_config = None\n",
    "        self.best_val = 0.0\n",
    "        self.cond_stage_model = None\n",
    "        self.validation_count = 0\n",
    "\n",
    "    def forward(self, x, c, label, image_raw, *args, **kwargs):\n",
    "        # print(self.num_timesteps)\n",
    "        # t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "        # print('t.shape')\n",
    "        # print(t.shape)\n",
    "        # if self.model.conditioning_key is not None:\n",
    "        #     assert c is not None\n",
    "        #     imgs = c\n",
    "        #     if self.cond_stage_trainable:\n",
    "                # c = self.get_learned_conditioning(c)\n",
    "        c, re_latent = self.get_learned_conditioning(c)\n",
    "                # print('c.shape')\n",
    "                # print(c.shape)\n",
    "\n",
    "        prefix = 'train' if self.training else 'val'\n",
    "        # loss, loss_dict = self.p_losses(x, c, t, *args, **kwargs)\n",
    "        pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "\n",
    "        loss = self.cls_loss(label, pre_cls)\n",
    "\n",
    "        loss_dict = {}\n",
    "        loss_dict.update({f'{prefix}/loss_cls': loss})\n",
    "        # rencon = self.cond_stage_model.recon(re_latent)\n",
    "        if self.clip_tune:\n",
    "            image_embeds = self.image_embedder(image_raw)\n",
    "            loss_clip = self.cond_stage_model.get_clip_loss(re_latent, image_embeds)\n",
    "        # loss_recon = self.recon_loss(imgs, rencon)\n",
    "\n",
    "            loss += loss_clip\n",
    "        # loss += loss_cls # loss_recon +  #(self.original_elbo_weight * loss_vlb)\n",
    "        # loss_dict.update({f'{prefix}/loss_recon': loss_recon})\n",
    "        # loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "            loss_dict.update({f'{prefix}/loss_clip': loss_clip})\n",
    "                # if self.return_cond:\n",
    "                    # return self.p_losses(x, c, t, *args, **kwargs), c\n",
    "        # return self.p_losses(x, c, t, *args, **kwargs)\n",
    "        # if self.return_cond:\n",
    "        #     return loss, loss_dict, c\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        x,c, label, image_raw  = self.get_input(batch)\n",
    "        loss, loss_dict = self(x,c, label, image_raw)\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def cls_loss(self, label, pred):\n",
    "        return torch.nn.CrossEntropyLoss()(pred, label)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        self.cond_stage_model.train()  ###\n",
    "\n",
    "        loss, loss_dict = self.shared_step(batch)\n",
    "\n",
    "        self.log_dict(loss_dict, prog_bar=True,\n",
    "                    logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        # if self.use_scheduler:\n",
    "        #     lr = self.optimizers().param_groups[0]['lr']\n",
    "        #     self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        # if self.train_cond_stage_only:\n",
    "        #     print(f\"{self.__class__.__name__}: Only optimizing conditioner params!\")\n",
    "        #     cond_parms = [p for n, p in self.named_parameters()\n",
    "        #             if 'attn2' in n or 'time_embed_condtion' in n or 'norm2' in n]\n",
    "        #     # cond_parms = [p for n, p in self.named_parameters()\n",
    "        #             # if 'time_embed_condtion' in n]\n",
    "        #     # cond_parms = []\n",
    "\n",
    "        params = list(self.cond_stage_model.parameters()) # + cond_parms\n",
    "\n",
    "        for p in params:\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # else:\n",
    "        #     params = list(self.model.parameters())\n",
    "        #     if self.cond_stage_trainable:\n",
    "        #         print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n",
    "        #         params = params + list(self.cond_stage_model.parameters())\n",
    "        #     if self.learn_logvar:\n",
    "        #         print('Diffusion model optimizing logvar')\n",
    "        #         params.append(self.logvar)\n",
    "\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "\n",
    "        # if self.use_scheduler:\n",
    "        #     assert 'target' in self.scheduler_config\n",
    "        #     scheduler = instantiate_from_config(self.scheduler_config)\n",
    "\n",
    "        #     print(\"Setting up LambdaLR scheduler...\")\n",
    "        #     scheduler = [\n",
    "        #         {\n",
    "        #             'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n",
    "        #             'interval': 'step',\n",
    "        #             'frequency': 1\n",
    "        #         }]\n",
    "        #     return [opt], scheduler\n",
    "\n",
    "        return opt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_input(self, batch, k='image', return_first_stage_outputs=False, force_c_encode=False,\n",
    "                  cond_key=None, return_original_cond=False, bs=None):\n",
    "        # x = super().get_input(batch, k)\n",
    "        x = batch['image']\n",
    "        if bs is not None:\n",
    "            x = x[:bs]\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # print('z.shape')\n",
    "        # print(z.shape)\n",
    "        # print(cond_key)\n",
    "        # print(self.cond_stage_key)\n",
    "        # print(cond_key)\n",
    "        xc = batch['eeg']\n",
    "        c = xc\n",
    "        # if self.model.conditioning_key is not None:\n",
    "        #     if cond_key is None:\n",
    "        #         cond_key = self.cond_stage_key\n",
    "        #     if cond_key != self.first_stage_key:\n",
    "        #         if cond_key in ['caption', 'coordinates_bbox','fmri', 'eeg']:\n",
    "        #             xc = batch[cond_key]\n",
    "        #         elif cond_key == 'class_label':\n",
    "        #             xc = batch\n",
    "        #         else:\n",
    "        #             xc = super().get_input(batch, cond_key).to(self.device)\n",
    "        #     else:\n",
    "        #         xc = x\n",
    "        #     # print('get input')\n",
    "        #     # print(not self.cond_stage_trainable)\n",
    "        #     # print(force_c_encode)\n",
    "        #     if not self.cond_stage_trainable or force_c_encode :\n",
    "        #         # print('get learned condition')\n",
    "        #         if isinstance(xc, dict) or isinstance(xc, list):\n",
    "        #             # import pudb; pudb.set_trace()\n",
    "        #             c, re_latent = self.get_learned_conditioning(xc)\n",
    "        #             # c = self.get_learned_conditioning(xc)\n",
    "        #         else:\n",
    "        #             c, re_latent = self.get_learned_conditioning(xc.to(self.device))\n",
    "        #             # c = self.get_learned_conditioning(xc.to(self.device))\n",
    "        #     else:\n",
    "        #         c = xc\n",
    "        #     if bs is not None:\n",
    "        #         c = c[:bs]\n",
    "\n",
    "        #     if self.use_positional_encodings:\n",
    "        #         pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "        #         ckey = __conditioning_keys__[self.model.conditioning_key]\n",
    "        #         c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n",
    "\n",
    "        # else:\n",
    "        #     c = None\n",
    "        #     xc = None\n",
    "        #     if self.use_positional_encodings:\n",
    "        #         pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "        #         c = {'pos_x': pos_x, 'pos_y': pos_y}\n",
    "        out = [x, c , batch['label'], batch['image_raw']]\n",
    "        # if return_first_stage_outputs:\n",
    "        #     xrec = self.decode_first_stage(z)\n",
    "        #     out.extend([x, xrec])\n",
    "        # if return_original_cond:\n",
    "        #     out.append(xc)\n",
    "        return out\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "\n",
    "    def accuracy(self, output, target, topk=(1, )):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            maxk = max(topk)\n",
    "            batch_size = target.size(0)\n",
    "\n",
    "            _, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            res = []\n",
    "            for k in topk:\n",
    "                correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            return res\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        print('val step')\n",
    "        print('batch_idx:', batch_idx)\n",
    "        # if batch_idx != 0:\n",
    "        #     return\n",
    "\n",
    "        if self.validation_count % 1 == 0 and self.trainer.current_epoch != 0:\n",
    "            self.full_validation(batch)\n",
    "        # else:\n",
    "        #     # pass\n",
    "        #     grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=3, limit=5)\n",
    "        #     metric, metric_list = self.get_eval_metric(all_samples, avg=self.eval_avg)\n",
    "        #     grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "        #     # self.logger.log_image(key=f'samples_test', images=[grid_imgs])\n",
    "        #     metric_dict = {f'val/{k}':v for k, v in zip(metric_list, metric)}\n",
    "        #     # self.logger.log_metrics(metric_dict)\n",
    "        #     if metric[-1] > self.run_full_validation_threshold:\n",
    "        #         self.full_validation(batch, state=state)\n",
    "        self.validation_count += 1\n",
    "\n",
    "\n",
    "    def full_validation(self, batch, state=None):\n",
    "        print('###### run full validation! ######\\n')\n",
    "        c = batch['eeg']\n",
    "\n",
    "        c, re_latent = self.get_learned_conditioning(c)\n",
    "\n",
    "        # loss, loss_dict = self.p_losses(x, c, t, *args, **kwargs)\n",
    "        pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "        # grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=5, limit=None, state=state)\n",
    "        # metric, metric_list = self.get_eval_metric(all_samples)\n",
    "        # self.save_images(all_samples, suffix='%.4f'%metric[-1])\n",
    "        # metric_dict = {f'val/{k}_full':v for k, v in zip(metric_list, metric)}\n",
    "        # self.logger.log_metrics(metric_dict)\n",
    "        acc1, acc5 = self.accuracy(pre_cls, batch['label'], topk=(1, 5))\n",
    "        print(acc1, acc5)\n",
    "        # acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
    "        # grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "\n",
    "        # self.logger.log_image(key=f'samples_test_full', images=[grid_imgs])\n",
    "        if acc1[0] > self.best_val:\n",
    "            self.best_val = acc1[0]\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict': self.state_dict(),\n",
    "                    'config': self.main_config,\n",
    "                    'state': state\n",
    "\n",
    "                },\n",
    "                os.path.join(self.output_path, 'checkpoint_best.pth')\n",
    "            )\n",
    "    def get_learned_conditioning(self, c):\n",
    "        # self.cond_stage_model.eval()\n",
    "        if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n",
    "            c, re_latent = self.cond_stage_model.encode(c)\n",
    "            # c = self.cond_stage_model.encode(c)\n",
    "        else:\n",
    "            c, re_latent = self.cond_stage_model(c)\n",
    "            # c = self.cond_stage_model(c)\n",
    "        # return c\n",
    "        return c, re_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "id": "_gJpyhyiJr8b"
   },
   "outputs": [],
   "source": [
    "#@title Diffusion util\n",
    "# adopted from\n",
    "# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n",
    "# and\n",
    "# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n",
    "# and\n",
    "# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n",
    "#\n",
    "# thanks!\n",
    "\n",
    "\n",
    "def make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "    if schedule == \"linear\":\n",
    "        betas = (\n",
    "                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "        )\n",
    "\n",
    "    elif schedule == \"cosine\":\n",
    "        timesteps = (\n",
    "                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n",
    "        )\n",
    "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
    "        alphas = torch.cos(alphas).pow(2)\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - alphas[1:] / alphas[:-1]\n",
    "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
    "\n",
    "    elif schedule == \"sqrt_linear\":\n",
    "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n",
    "    elif schedule == \"sqrt\":\n",
    "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n",
    "    else:\n",
    "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
    "    return betas.numpy()\n",
    "\n",
    "\n",
    "def make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n",
    "    if ddim_discr_method == 'uniform':\n",
    "        c = num_ddpm_timesteps // num_ddim_timesteps\n",
    "        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
    "    elif ddim_discr_method == 'quad':\n",
    "        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n",
    "    else:\n",
    "        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
    "\n",
    "    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n",
    "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "    steps_out = ddim_timesteps + 1\n",
    "    if verbose:\n",
    "        print(f'Selected timesteps for ddim sampler: {steps_out}')\n",
    "    return steps_out\n",
    "\n",
    "\n",
    "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n",
    "    # select alphas for computing the variance schedule\n",
    "    alphas = alphacums[ddim_timesteps]\n",
    "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
    "\n",
    "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
    "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
    "    if verbose:\n",
    "        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n",
    "        print(f'For the chosen value of eta, which is {eta}, '\n",
    "              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n",
    "    return sigmas, alphas, alphas_prev\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def checkpoint(func, inputs, params, flag):\n",
    "    \"\"\"\n",
    "    Evaluate a function without caching intermediate activations, allowing for\n",
    "    reduced memory at the expense of extra compute in the backward pass.\n",
    "    :param func: the function to evaluate.\n",
    "    :param inputs: the argument sequence to pass to `func`.\n",
    "    :param params: a sequence of parameters `func` depends on but does not\n",
    "                   explicitly take as arguments.\n",
    "    :param flag: if False, disable gradient checkpointing.\n",
    "    \"\"\"\n",
    "    if flag:\n",
    "        args = tuple(inputs) + tuple(params)\n",
    "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
    "    else:\n",
    "        return func(*inputs)\n",
    "\n",
    "\n",
    "class CheckpointFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, run_function, length, *args):\n",
    "        ctx.run_function = run_function\n",
    "        ctx.input_tensors = list(args[:length])\n",
    "        ctx.input_params = list(args[length:])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
    "        return output_tensors\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *output_grads):\n",
    "        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n",
    "        with torch.enable_grad():\n",
    "            # Fixes a bug where the first op in run_function modifies the\n",
    "            # Tensor storage in place, which is not allowed for detach()'d\n",
    "            # Tensors.\n",
    "            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n",
    "            output_tensors = ctx.run_function(*shallow_copies)\n",
    "        input_grads = torch.autograd.grad(\n",
    "            output_tensors,\n",
    "            ctx.input_tensors + ctx.input_params,\n",
    "            output_grads,\n",
    "            allow_unused=True,\n",
    "        )\n",
    "        del ctx.input_tensors\n",
    "        del ctx.input_params\n",
    "        del output_tensors\n",
    "        return (None, None) + input_grads\n",
    "\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    if not repeat_only:\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    else:\n",
    "        embedding = repeat(timesteps, 'b -> b d', d=dim)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "def scale_module(module, scale):\n",
    "    \"\"\"\n",
    "    Scale the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().mul_(scale)\n",
    "    return module\n",
    "\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "\n",
    "def normalization(channels):\n",
    "    \"\"\"\n",
    "    Make a standard normalization layer.\n",
    "    :param channels: number of input channels.\n",
    "    :return: an nn.Module for normalization.\n",
    "    \"\"\"\n",
    "    return GroupNorm32(32, channels)\n",
    "\n",
    "\n",
    "# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\n",
    "class SiLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "def conv_nd(dims, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a 1D, 2D, or 3D convolution module.\n",
    "    \"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.Conv1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.Conv2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.Conv3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def linear(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a linear module.\n",
    "    \"\"\"\n",
    "    return nn.Linear(*args, **kwargs)\n",
    "\n",
    "\n",
    "def avg_pool_nd(dims, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a 1D, 2D, or 3D average pooling module.\n",
    "    \"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.AvgPool1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.AvgPool2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.AvgPool3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "class HybridConditioner(nn.Module):\n",
    "\n",
    "    def __init__(self, c_concat_config, c_crossattn_config):\n",
    "        super().__init__()\n",
    "        self.concat_conditioner = instantiate_from_config(c_concat_config)\n",
    "        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n",
    "\n",
    "    def forward(self, c_concat, c_crossattn):\n",
    "        c_concat = self.concat_conditioner(c_concat)\n",
    "        c_crossattn = self.crossattn_conditioner(c_crossattn)\n",
    "        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}\n",
    "\n",
    "\n",
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "form",
    "id": "17xf-X3ZJgvf"
   },
   "outputs": [],
   "source": [
    "#@title Linear attention\n",
    "from inspect import isfunction\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def uniq(arr):\n",
    "    return{el: True for el in arr}.keys()\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def max_neg_value(t):\n",
    "    return -torch.finfo(t.dtype).max\n",
    "\n",
    "\n",
    "def init_(tensor):\n",
    "    dim = tensor.shape[-1]\n",
    "    std = 1 / math.sqrt(dim)\n",
    "    tensor.uniform_(-std, std)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# feedforward\n",
    "class GEGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        dim_out = default(dim_out, dim)\n",
    "        project_in = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            nn.GELU()\n",
    "        ) if not glu else GEGLU(dim, inner_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            project_in,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n",
    "        k = k.softmax(dim=-1)\n",
    "        context = torch.einsum('bhdn,bhen->bhde', k, v)\n",
    "        out = torch.einsum('bhde,bhdn->bhen', context, q)\n",
    "        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
    "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
    "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
    "\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
    "        w_ = rearrange(w_, 'b i j -> b j i')\n",
    "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
    "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module): # Optimize this module as well\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., cond_scale=1.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.cond_scale = cond_scale\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class BasicTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True, cond_scale=1.):\n",
    "        super().__init__()\n",
    "        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout,cond_scale=cond_scale)  # is a self-attention\n",
    "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
    "        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,\n",
    "                                    heads=n_heads, dim_head=d_head, dropout=dropout,cond_scale=cond_scale)  # is self-attn if context is none\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
    "\n",
    "    def _forward(self, x, context=None):\n",
    "        x = self.attn1(self.norm1(x)) + x\n",
    "        x = self.attn2(self.norm2(x), context=context) + x\n",
    "        x = self.ff(self.norm3(x)) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block for image-like data.\n",
    "    First, project the input (aka embedding)\n",
    "    and reshape to b, t, d.\n",
    "    Then apply standard transformer action.\n",
    "    Finally, reshape to image\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, n_heads, d_head,\n",
    "                 depth=1, dropout=0., context_dim=None, cond_scale=1.):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        inner_dim = n_heads * d_head\n",
    "        self.norm = Normalize(in_channels)\n",
    "\n",
    "        self.proj_in = nn.Conv2d(in_channels,\n",
    "                                 inner_dim,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim,cond_scale=cond_scale)\n",
    "                for d in range(depth)]\n",
    "        )\n",
    "\n",
    "        self.proj_out = zero_module(nn.Conv2d(inner_dim,\n",
    "                                              in_channels,\n",
    "                                              kernel_size=1,\n",
    "                                              stride=1,\n",
    "                                              padding=0))\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        # note: if no context is given, cross-attention defaults to self-attention\n",
    "        b, c, h, w = x.shape\n",
    "        x_in = x\n",
    "        x = self.norm(x)\n",
    "        x = self.proj_in(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, context=context)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        x = self.proj_out(x)\n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "ibc51caBJSqG"
   },
   "outputs": [],
   "source": [
    "#@title Diffusion models - did this ppl copy the internet?\n",
    "# pytorch_diffusion + derived encoder decoder\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
    "    return emb\n",
    "\n",
    "\n",
    "def nonlinearity(x):\n",
    "    # swish\n",
    "    return x*torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def Normalize(in_channels, num_groups=32):\n",
    "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            self.conv = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            # no asymmetric padding in torch conv, must do it ourselves\n",
    "            self.conv = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=2,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.with_conv:\n",
    "            pad = (0,1,0,1)\n",
    "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "            x = self.conv(x)\n",
    "        else:\n",
    "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
    "                 dropout, temb_channels=512):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        if temb_channels > 0:\n",
    "            self.temb_proj = torch.nn.Linear(temb_channels,\n",
    "                                             out_channels)\n",
    "        self.norm2 = Normalize(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n",
    "                                                     out_channels,\n",
    "                                                     kernel_size=3,\n",
    "                                                     stride=1,\n",
    "                                                     padding=1)\n",
    "            else:\n",
    "                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n",
    "                                                    out_channels,\n",
    "                                                    kernel_size=1,\n",
    "                                                    stride=1,\n",
    "                                                    padding=0)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        if temb is not None:\n",
    "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                x = self.conv_shortcut(x)\n",
    "            else:\n",
    "                x = self.nin_shortcut(x)\n",
    "\n",
    "        return x+h\n",
    "\n",
    "\n",
    "class LinAttnBlock(LinearAttention):\n",
    "    \"\"\"to match AttnBlock usage\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = q.reshape(b,c,h*w)\n",
    "        q = q.permute(0,2,1).contiguous()   # b,hw,c\n",
    "        k = k.reshape(b,c,h*w) # b,c,hw\n",
    "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b,c,h*w)\n",
    "        w_ = w_.permute(0,2,1).contiguous()   # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b,c,h,w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "\n",
    "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
    "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
    "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
    "    if attn_type == \"vanilla\":\n",
    "        return AttnBlock(in_channels)\n",
    "    elif attn_type == \"none\":\n",
    "        return nn.Identity(in_channels)\n",
    "    else:\n",
    "        return LinAttnBlock(in_channels)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
    "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
    "        super().__init__()\n",
    "        if use_linear_attn: attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = self.ch*4\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.use_timestep = use_timestep\n",
    "        if self.use_timestep:\n",
    "            # timestep embedding\n",
    "            self.temb = nn.Module()\n",
    "            self.temb.dense = nn.ModuleList([\n",
    "                torch.nn.Linear(self.ch,\n",
    "                                self.temb_ch),\n",
    "                torch.nn.Linear(self.temb_ch,\n",
    "                                self.temb_ch),\n",
    "            ])\n",
    "\n",
    "        # downsampling\n",
    "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
    "                                       self.ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1,)+tuple(ch_mult)\n",
    "        self.down = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch*in_ch_mult[i_level]\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "            self.down.append(down)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            skip_in = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                if i_block == self.num_res_blocks:\n",
    "                    skip_in = ch*in_ch_mult[i_level]\n",
    "                block.append(ResnetBlock(in_channels=block_in+skip_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "            self.up.insert(0, up) # prepend to get consistent order\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        out_ch,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x, t=None, context=None):\n",
    "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
    "        if context is not None:\n",
    "            # assume aligned context, cat along channel axis\n",
    "            x = torch.cat((x, context), dim=1)\n",
    "        if self.use_timestep:\n",
    "            # timestep embedding\n",
    "            assert t is not None\n",
    "            temb = get_timestep_embedding(t, self.ch)\n",
    "            temb = self.temb.dense[0](temb)\n",
    "            temb = nonlinearity(temb)\n",
    "            temb = self.temb.dense[1](temb)\n",
    "        else:\n",
    "            temb = None\n",
    "\n",
    "        # downsampling\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        # middle\n",
    "        h = hs[-1]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # upsampling\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                h = self.up[i_level].block[i_block](\n",
    "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # end\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.conv_out.weight\n",
    "\n",
    "\n",
    "class DiffusionEncoder(nn.Module):\n",
    "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
    "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
    "                 **ignore_kwargs):\n",
    "        super().__init__()\n",
    "        if use_linear_attn: attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # downsampling\n",
    "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
    "                                       self.ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1,)+tuple(ch_mult)\n",
    "        self.in_ch_mult = in_ch_mult\n",
    "        self.down = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch*in_ch_mult[i_level]\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "            self.down.append(down)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        2*z_channels if double_z else z_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # timestep embedding\n",
    "        temb = None\n",
    "\n",
    "        # downsampling\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        # middle\n",
    "        h = hs[-1]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # end\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
    "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
    "                 attn_type=\"vanilla\", **ignorekwargs):\n",
    "        super().__init__()\n",
    "        if use_linear_attn: attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.give_pre_end = give_pre_end\n",
    "        self.tanh_out = tanh_out\n",
    "\n",
    "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
    "        in_ch_mult = (1,)+tuple(ch_mult)\n",
    "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
    "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
    "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
    "        print(\"Working with z of shape {} = {} dimensions.\".format(\n",
    "            self.z_shape, np.prod(self.z_shape)))\n",
    "\n",
    "        # z to block_in\n",
    "        self.conv_in = torch.nn.Conv2d(z_channels,\n",
    "                                       block_in,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "            self.up.insert(0, up) # prepend to get consistent order\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        out_ch,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        #assert z.shape[1:] == self.z_shape[1:]\n",
    "        self.last_z_shape = z.shape\n",
    "\n",
    "        # timestep embedding\n",
    "        temb = None\n",
    "\n",
    "        # z to block_in\n",
    "        h = self.conv_in(z)\n",
    "\n",
    "        # middle\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # upsampling\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                h = self.up[i_level].block[i_block](h, temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # end\n",
    "        if self.give_pre_end:\n",
    "            return h\n",
    "\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        if self.tanh_out:\n",
    "            h = torch.tanh(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
    "                                     ResnetBlock(in_channels=in_channels,\n",
    "                                                 out_channels=2 * in_channels,\n",
    "                                                 temb_channels=0, dropout=0.0),\n",
    "                                     ResnetBlock(in_channels=2 * in_channels,\n",
    "                                                out_channels=4 * in_channels,\n",
    "                                                temb_channels=0, dropout=0.0),\n",
    "                                     ResnetBlock(in_channels=4 * in_channels,\n",
    "                                                out_channels=2 * in_channels,\n",
    "                                                temb_channels=0, dropout=0.0),\n",
    "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
    "                                     Upsample(in_channels, with_conv=True)])\n",
    "        # end\n",
    "        self.norm_out = Normalize(in_channels)\n",
    "        self.conv_out = torch.nn.Conv2d(in_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.model):\n",
    "            if i in [1,2,3]:\n",
    "                x = layer(x, None)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        h = self.norm_out(x)\n",
    "        h = nonlinearity(h)\n",
    "        x = self.conv_out(h)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpsampleDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
    "                 ch_mult=(2,2), dropout=0.0):\n",
    "        super().__init__()\n",
    "        # upsampling\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        block_in = in_channels\n",
    "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        self.upsample_blocks = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            res_block = []\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                res_block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "            self.res_blocks.append(nn.ModuleList(res_block))\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                self.upsample_blocks.append(Upsample(block_in, True))\n",
    "                curr_res = curr_res * 2\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # upsampling\n",
    "        h = x\n",
    "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                h = self.res_blocks[i_level][i_block](h, None)\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                h = self.upsample_blocks[k](h)\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class LatentRescaler(nn.Module):\n",
    "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
    "        super().__init__()\n",
    "        # residual block, interpolate, residual block\n",
    "        self.factor = factor\n",
    "        self.conv_in = nn.Conv2d(in_channels,\n",
    "                                 mid_channels,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1)\n",
    "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n",
    "                                                     out_channels=mid_channels,\n",
    "                                                     temb_channels=0,\n",
    "                                                     dropout=0.0) for _ in range(depth)])\n",
    "        self.attn = AttnBlock(mid_channels)\n",
    "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n",
    "                                                     out_channels=mid_channels,\n",
    "                                                     temb_channels=0,\n",
    "                                                     dropout=0.0) for _ in range(depth)])\n",
    "\n",
    "        self.conv_out = nn.Conv2d(mid_channels,\n",
    "                                  out_channels,\n",
    "                                  kernel_size=1,\n",
    "                                  )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        for block in self.res_block1:\n",
    "            x = block(x, None)\n",
    "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
    "        x = self.attn(x)\n",
    "        for block in self.res_block2:\n",
    "            x = block(x, None)\n",
    "        x = self.conv_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MergedRescaleEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
    "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
    "        super().__init__()\n",
    "        intermediate_chn = ch * ch_mult[-1]\n",
    "        self.encoder = DiffusionEncoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
    "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
    "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
    "                               out_ch=None)\n",
    "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
    "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.rescaler(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MergedRescaleDecoder(nn.Module):\n",
    "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
    "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
    "        super().__init__()\n",
    "        tmp_chn = z_channels*ch_mult[-1]\n",
    "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
    "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
    "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
    "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
    "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rescaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
    "        super().__init__()\n",
    "        assert out_size >= in_size\n",
    "        num_blocks = int(np.log2(out_size//in_size))+1\n",
    "        factor_up = 1.+ (out_size % in_size)\n",
    "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
    "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
    "                                       out_channels=in_channels)\n",
    "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
    "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
    "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rescaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Resize(nn.Module):\n",
    "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
    "        super().__init__()\n",
    "        self.with_conv = learned\n",
    "        self.mode = mode\n",
    "        if self.with_conv:\n",
    "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
    "            raise NotImplementedError()\n",
    "            assert in_channels is not None\n",
    "            # no asymmetric padding in torch conv, must do it ourselves\n",
    "            self.conv = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=2,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x, scale_factor=1.0):\n",
    "        if scale_factor==1.0:\n",
    "            return x\n",
    "        else:\n",
    "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
    "        return x\n",
    "\n",
    "class FirstStagePostProcessor(nn.Module):\n",
    "\n",
    "    def __init__(self, ch_mult:list, in_channels,\n",
    "                 pretrained_model:nn.Module=None,\n",
    "                 reshape=False,\n",
    "                 n_channels=None,\n",
    "                 dropout=0.,\n",
    "                 pretrained_config=None):\n",
    "        super().__init__()\n",
    "        if pretrained_config is None:\n",
    "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
    "            self.pretrained_model = pretrained_model\n",
    "        else:\n",
    "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
    "            self.instantiate_pretrained(pretrained_config)\n",
    "\n",
    "        self.do_reshape = reshape\n",
    "\n",
    "        if n_channels is None:\n",
    "            n_channels = self.pretrained_model.encoder.ch\n",
    "\n",
    "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
    "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3,\n",
    "                            stride=1,padding=1)\n",
    "\n",
    "        blocks = []\n",
    "        downs = []\n",
    "        ch_in = n_channels\n",
    "        for m in ch_mult:\n",
    "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
    "            ch_in = m * n_channels\n",
    "            downs.append(Downsample(ch_in, with_conv=False))\n",
    "\n",
    "        self.model = nn.ModuleList(blocks)\n",
    "        self.downsampler = nn.ModuleList(downs)\n",
    "\n",
    "\n",
    "    def instantiate_pretrained(self, config):\n",
    "        model = instantiate_from_config(config)\n",
    "        self.pretrained_model = model.eval()\n",
    "        # self.pretrained_model.train = False\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_with_pretrained(self,x):\n",
    "        c = self.pretrained_model.encode(x)\n",
    "        if isinstance(c, DiagonalGaussianDistribution):\n",
    "            c = c.mode()\n",
    "        return  c\n",
    "\n",
    "    def forward(self,x):\n",
    "        z_fs = self.encode_with_pretrained(x)\n",
    "        z = self.proj_norm(z_fs)\n",
    "        z = self.proj(z)\n",
    "        z = nonlinearity(z)\n",
    "\n",
    "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
    "            z = submodel(z,temb=None)\n",
    "            z = downmodel(z)\n",
    "\n",
    "        if self.do_reshape:\n",
    "            z = rearrange(z,'b c h w -> b (h w) c')\n",
    "        return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "urhR3N8NKGQW"
   },
   "outputs": [],
   "source": [
    "#@title OpenAI model\n",
    "from abc import abstractmethod\n",
    "from functools import partial\n",
    "import math\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# dummy replace\n",
    "def convert_module_to_f16(x):\n",
    "    pass\n",
    "\n",
    "def convert_module_to_f32(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "## go\n",
    "class AttentionPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spacial_dim: int,\n",
    "        embed_dim: int,\n",
    "        num_heads_channels: int,\n",
    "        output_dim: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(th.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5)\n",
    "        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
    "        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
    "        self.num_heads = embed_dim // num_heads_channels\n",
    "        self.attention = QKVAttention(self.num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, *_spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)  # NC(HW)\n",
    "        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n",
    "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n",
    "        x = self.qkv_proj(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x[:, :, 0].contiguous()\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Any module where forward() takes timestep embeddings as a second argument.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)\n",
    "            elif isinstance(layer, SpatialTransformer):\n",
    "                x = layer(x, context)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpsampleAI(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
    "                 upsampling occurs in the inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        if use_conv:\n",
    "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.dims == 3:\n",
    "            x = F.interpolate(\n",
    "                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n",
    "            )\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class TransposedUpsample(nn.Module):\n",
    "    'Learned 2x upsampling without padding'\n",
    "    def __init__(self, channels, out_channels=None, ks=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(self.channels,self.out_channels,kernel_size=ks,stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.up(x)\n",
    "\n",
    "\n",
    "class DownsampleAI(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
    "                 downsampling occurs in the inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None,padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        stride = 2 if dims != 3 else (1, 2, 2)\n",
    "        if use_conv:\n",
    "            self.op = conv_nd(\n",
    "                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n",
    "            )\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):\n",
    "    \"\"\"\n",
    "    A residual block that can optionally change the number of channels.\n",
    "    :param channels: the number of input channels.\n",
    "    :param emb_channels: the number of timestep embedding channels.\n",
    "    :param dropout: the rate of dropout.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    :param use_conv: if True and out_channels is specified, use a spatial\n",
    "        convolution instead of a smaller 1x1 convolution to change the\n",
    "        channels in the skip connection.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param use_checkpoint: if True, use gradient checkpointing on this module.\n",
    "    :param up: if True, use this block for upsampling.\n",
    "    :param down: if True, use this block for downsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            normalization(channels),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = UpsampleAI(channels, False, dims)\n",
    "            self.x_upd = UpsampleAI(channels, False, dims)\n",
    "        elif down:\n",
    "            self.h_upd = DownsampleAI(channels, False, dims)\n",
    "            self.x_upd = DownsampleAI(channels, False, dims)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            normalization(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(\n",
    "                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = conv_nd(\n",
    "                dims, channels, self.out_channels, 3, padding=1\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the block to a Tensor, conditioned on a timestep embedding.\n",
    "        :param x: an [N x C x ...] Tensor of features.\n",
    "        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        return checkpoint(\n",
    "            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n",
    "        )\n",
    "\n",
    "\n",
    "    def _forward(self, x, emb):\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = th.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert (\n",
    "                channels % num_head_channels == 0\n",
    "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
    "        if use_new_attention_order:\n",
    "            # split qkv before split heads\n",
    "            self.attention = QKVAttention(self.num_heads)\n",
    "        else:\n",
    "            # split heads before split qkv\n",
    "            self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self._forward, (x,), self.parameters(), True)   # TODO: check checkpoint usage, is True # TODO: fix the .half call!!!\n",
    "        #return pt_checkpoint(self._forward, x)  # pytorch\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x)).contiguous()\n",
    "        h = self.attention(qkv).contiguous()\n",
    "        h = self.proj_out(h).contiguous()\n",
    "        return (x + h).reshape(b, c, *spatial).contiguous()\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"\n",
    "    A counter for the `thop` package to count the operations in an\n",
    "    attention operation.\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial ** 2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention and splits in a different order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\",\n",
    "            (q * scale).view(bs * self.n_heads, ch, length).contiguous(),\n",
    "            (k * scale).view(bs * self.n_heads, ch, length).contiguous(),\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The full UNet model with attention and timestep embedding.\n",
    "    :param in_channels: channels in the input Tensor.\n",
    "    :param model_channels: base channel count for the model.\n",
    "    :param out_channels: channels in the output Tensor.\n",
    "    :param num_res_blocks: number of residual blocks per downsample.\n",
    "    :param attention_resolutions: a collection of downsample rates at which\n",
    "        attention will take place. May be a set, list, or tuple.\n",
    "        For example, if this contains 4, then at 4x downsampling, attention\n",
    "        will be used.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param channel_mult: channel multiplier for each level of the UNet.\n",
    "    :param conv_resample: if True, use learned convolutions for upsampling and\n",
    "        downsampling.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param num_classes: if specified (as an int), then this model will be\n",
    "        class-conditional with `num_classes` classes.\n",
    "    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
    "    :param num_heads: the number of attention heads in each attention layer.\n",
    "    :param num_heads_channels: if specified, ignore num_heads and instead use\n",
    "                               a fixed channel width per attention head.\n",
    "    :param num_heads_upsample: works with num_heads to set a different number\n",
    "                               of heads for upsampling. Deprecated.\n",
    "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
    "    :param resblock_updown: use residual blocks for up/downsampling.\n",
    "    :param use_new_attention_order: use a different attention pattern for potentially\n",
    "                                    increased efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=-1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        use_spatial_transformer=False,    # custom transformer support\n",
    "        transformer_depth=1,              # custom transformer support\n",
    "        context_dim=None,                 # custom transformer support\n",
    "        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n",
    "        legacy=True,\n",
    "        cond_scale=1.0,\n",
    "        global_pool=False,\n",
    "        use_time_cond=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if use_spatial_transformer:\n",
    "            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n",
    "\n",
    "        if context_dim is not None:\n",
    "            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n",
    "            from omegaconf.listconfig import ListConfig\n",
    "            if type(context_dim) == ListConfig:\n",
    "                context_dim = list(context_dim)\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        if num_heads == -1:\n",
    "            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n",
    "\n",
    "        if num_head_channels == -1:\n",
    "            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_classes = num_classes\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "        self.predict_codebook_ids = n_embed is not None\n",
    "        self.cond_scale = cond_scale\n",
    "        self.use_time_cond = use_time_cond\n",
    "        self.global_pool = global_pool\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
    "\n",
    "        # self.time_embed_condtion = nn.Linear(context_dim, time_embed_dim, bias=False)\n",
    "        if use_time_cond:\n",
    "            self.time_embed_condtion = nn.Sequential(\n",
    "                nn.Conv1d(77, 77//2, 1, bias=True),\n",
    "                nn.Conv1d(77//2, 1, 1, bias=True),\n",
    "                nn.Linear(context_dim, time_embed_dim, bias=True)\n",
    "            ) if global_pool == False else nn.Linear(context_dim, time_embed_dim, bias=True)\n",
    "\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TimestepEmbedSequential(\n",
    "                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self._feature_size = model_channels\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=mult * model_channels,\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    if num_head_channels == -1:\n",
    "                        dim_head = ch // num_heads\n",
    "                    else:\n",
    "                        num_heads = ch // num_head_channels\n",
    "                        dim_head = num_head_channels\n",
    "                    if legacy:\n",
    "                        #num_heads = 1\n",
    "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=dim_head,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        ) if not use_spatial_transformer else SpatialTransformer(\n",
    "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,cond_scale=cond_scale\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else DownsampleAI(\n",
    "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        if num_head_channels == -1:\n",
    "            dim_head = ch // num_heads\n",
    "        else:\n",
    "            num_heads = ch // num_head_channels\n",
    "            dim_head = num_head_channels\n",
    "        if legacy:\n",
    "            #num_heads = 1\n",
    "            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=dim_head,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ) if not use_spatial_transformer else SpatialTransformer(\n",
    "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, cond_scale=cond_scale\n",
    "                        ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch + ich,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=model_channels * mult,\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    if num_head_channels == -1:\n",
    "                        dim_head = ch // num_heads\n",
    "                    else:\n",
    "                        num_heads = ch // num_head_channels\n",
    "                        dim_head = num_head_channels\n",
    "                    if legacy:\n",
    "                        #num_heads = 1\n",
    "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads_upsample,\n",
    "                            num_head_channels=dim_head,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        ) if not use_spatial_transformer else SpatialTransformer(\n",
    "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,cond_scale=cond_scale\n",
    "                        )\n",
    "                    )\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            up=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else UpsampleAI(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n",
    "        )\n",
    "        if self.predict_codebook_ids:\n",
    "            self.id_predictor = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            conv_nd(dims, model_channels, n_embed, 1),\n",
    "            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n",
    "        )\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float16.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "        self.output_blocks.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float32.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "        self.output_blocks.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, x, timesteps=None, context=None, y=None,**kwargs):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param context: conditioning plugged in via crossattn\n",
    "        :param y: an [N] Tensor of labels, if class-conditional.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        assert (y is not None) == (\n",
    "            self.num_classes is not None\n",
    "        ), \"must specify y if and only if the model is class-conditional\"\n",
    "        hs = []\n",
    "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
    "        emb = self.time_embed(t_emb)\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            assert y.shape == (x.shape[0],)\n",
    "            emb = emb + self.label_emb(y)\n",
    "        if self.use_time_cond: # add time conditioning\n",
    "            c = self.time_embed_condtion(context)\n",
    "            assert c.shape[1] == 1, f'found {c.shape}'\n",
    "            emb = emb + torch.squeeze(c, dim=1)\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb, context)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb, context)\n",
    "        for module in self.output_blocks:\n",
    "            h = th.cat([h, hs.pop()], dim=1)\n",
    "            h = module(h, emb, context)\n",
    "        h = h.type(x.dtype)\n",
    "        if self.predict_codebook_ids:\n",
    "            return self.id_predictor(h)\n",
    "        else:\n",
    "            return self.out(h).contiguous()\n",
    "\n",
    "\n",
    "class EncoderUNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The half UNet model with attention and timestep embedding.\n",
    "    For usage, see UNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        pool=\"adaptive\",\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TimestepEmbedSequential(\n",
    "                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self._feature_size = model_channels\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=mult * model_channels,\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else DownsampleAI(\n",
    "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=num_head_channels,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "        self.pool = pool\n",
    "        if pool == \"adaptive\":\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
    "                nn.Flatten(),\n",
    "            )\n",
    "        elif pool == \"attention\":\n",
    "            assert num_head_channels != -1\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                AttentionPool2d(\n",
    "                    (image_size // ds), ch, num_head_channels, out_channels\n",
    "                ),\n",
    "            )\n",
    "        elif pool == \"spatial\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        elif pool == \"spatial_v2\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                normalization(2048),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float16.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float32.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x K] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        results = []\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "            if self.pool.startswith(\"spatial\"):\n",
    "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "        h = self.middle_block(h, emb)\n",
    "        if self.pool.startswith(\"spatial\"):\n",
    "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "            h = th.cat(results, axis=-1)\n",
    "            return self.out(h)\n",
    "        else:\n",
    "            h = h.type(x.dtype)\n",
    "            return self.out(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "form",
    "id": "ue0D_XT_KmR2"
   },
   "outputs": [],
   "source": [
    "#@title X-transformer\n",
    "\"\"\"shout-out to https://github.com/lucidrains/x-transformers/tree/main/x_transformers\"\"\"\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "from collections import namedtuple\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "# constants\n",
    "\n",
    "DEFAULT_DIM_HEAD = 64\n",
    "\n",
    "Intermediates = namedtuple('Intermediates', [\n",
    "    'pre_softmax_attn',\n",
    "    'post_softmax_attn'\n",
    "])\n",
    "\n",
    "LayerIntermediates = namedtuple('Intermediates', [\n",
    "    'hiddens',\n",
    "    'attn_intermediates'\n",
    "])\n",
    "\n",
    "\n",
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "        self.init_()\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.normal_(self.emb.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = torch.arange(x.shape[-2], device=x.device)\n",
    "        return self.emb(n)[None, ...]\n",
    "\n",
    "\n",
    "class FixedPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_dim=1, offset=0):\n",
    "        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq) + offset\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def always(val):\n",
    "    def inner(*args, **kwargs):\n",
    "        return val\n",
    "    return inner\n",
    "\n",
    "\n",
    "def not_equals(val):\n",
    "    def inner(x):\n",
    "        return x != val\n",
    "    return inner\n",
    "\n",
    "\n",
    "def equals(val):\n",
    "    def inner(x):\n",
    "        return x == val\n",
    "    return inner\n",
    "\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max\n",
    "\n",
    "\n",
    "# keyword argument helpers\n",
    "\n",
    "def pick_and_pop(keys, d):\n",
    "    values = list(map(lambda key: d.pop(key), keys))\n",
    "    return dict(zip(keys, values))\n",
    "\n",
    "\n",
    "def group_dict_by_key(cond, d):\n",
    "    return_val = [dict(), dict()]\n",
    "    for key in d.keys():\n",
    "        match = bool(cond(key))\n",
    "        ind = int(not match)\n",
    "        return_val[ind][key] = d[key]\n",
    "    return (*return_val,)\n",
    "\n",
    "\n",
    "def string_begins_with(prefix, str):\n",
    "    return str.startswith(prefix)\n",
    "\n",
    "\n",
    "def group_by_key_prefix(prefix, d):\n",
    "    return group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "\n",
    "\n",
    "def groupby_prefix_and_trim(prefix, d):\n",
    "    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
    "    return kwargs_without_prefix, kwargs\n",
    "\n",
    "\n",
    "# classes\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, value, fn):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x, *rest = self.fn(x, **kwargs)\n",
    "        return (x * self.value, *rest)\n",
    "\n",
    "\n",
    "class Rezero(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.g = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x, *rest = self.fn(x, **kwargs)\n",
    "        return (x * self.g, *rest)\n",
    "\n",
    "\n",
    "class ScaleNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n",
    "        return x / norm.clamp(min=self.eps) * self.g\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n",
    "        return x / norm.clamp(min=self.eps) * self.g\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def forward(self, x, residual):\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class GRUGating(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(dim, dim)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        gated_output = self.gru(\n",
    "            rearrange(x, 'b n d -> (b n) d'),\n",
    "            rearrange(residual, 'b n d -> (b n) d')\n",
    "        )\n",
    "\n",
    "        return gated_output.reshape_as(x)\n",
    "\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        dim_out = default(dim_out, dim)\n",
    "        project_in = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            nn.GELU()\n",
    "        ) if not glu else GEGLU(dim, inner_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            project_in,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# attention.\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            dim_head=DEFAULT_DIM_HEAD,\n",
    "            heads=8,\n",
    "            causal=False,\n",
    "            mask=None,\n",
    "            talking_heads=False,\n",
    "            sparse_topk=None,\n",
    "            use_entmax15=False,\n",
    "            num_mem_kv=0,\n",
    "            dropout=0.,\n",
    "            on_attn=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if use_entmax15:\n",
    "            raise NotImplementedError(\"Check out entmax activation instead of softmax activation!\")\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.mask = mask\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # talking heads\n",
    "        self.talking_heads = talking_heads\n",
    "        if talking_heads:\n",
    "            self.pre_softmax_proj = nn.Parameter(torch.randn(heads, heads))\n",
    "            self.post_softmax_proj = nn.Parameter(torch.randn(heads, heads))\n",
    "\n",
    "        # explicit topk sparse attention\n",
    "        self.sparse_topk = sparse_topk\n",
    "\n",
    "        # entmax\n",
    "        #self.attn_fn = entmax15 if use_entmax15 else F.softmax\n",
    "        self.attn_fn = F.softmax\n",
    "\n",
    "        # add memory key / values\n",
    "        self.num_mem_kv = num_mem_kv\n",
    "        if num_mem_kv > 0:\n",
    "            self.mem_k = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n",
    "            self.mem_v = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n",
    "\n",
    "        # attention on attention\n",
    "        self.attn_on_attn = on_attn\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim * 2), nn.GLU()) if on_attn else nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            context=None,\n",
    "            mask=None,\n",
    "            context_mask=None,\n",
    "            rel_pos=None,\n",
    "            sinusoidal_emb=None,\n",
    "            prev_attn=None,\n",
    "            mem=None\n",
    "    ):\n",
    "        b, n, _, h, talking_heads, device = *x.shape, self.heads, self.talking_heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q_input = x\n",
    "        k_input = kv_input\n",
    "        v_input = kv_input\n",
    "\n",
    "        if exists(mem):\n",
    "            k_input = torch.cat((mem, k_input), dim=-2)\n",
    "            v_input = torch.cat((mem, v_input), dim=-2)\n",
    "\n",
    "        if exists(sinusoidal_emb):\n",
    "            # in shortformer, the query would start at a position offset depending on the past cached memory\n",
    "            offset = k_input.shape[-2] - q_input.shape[-2]\n",
    "            q_input = q_input + sinusoidal_emb(q_input, offset=offset)\n",
    "            k_input = k_input + sinusoidal_emb(k_input)\n",
    "\n",
    "        q = self.to_q(q_input)\n",
    "        k = self.to_k(k_input)\n",
    "        v = self.to_v(v_input)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device=device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device=device).bool())\n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            input_mask = q_mask * k_mask\n",
    "\n",
    "        if self.num_mem_kv > 0:\n",
    "            mem_k, mem_v = map(lambda t: repeat(t, 'h n d -> b h n d', b=b), (self.mem_k, self.mem_v))\n",
    "            k = torch.cat((mem_k, k), dim=-2)\n",
    "            v = torch.cat((mem_v, v), dim=-2)\n",
    "            if exists(input_mask):\n",
    "                input_mask = F.pad(input_mask, (self.num_mem_kv, 0), value=True)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        mask_value = max_neg_value(dots)\n",
    "\n",
    "        if exists(prev_attn):\n",
    "            dots = dots + prev_attn\n",
    "\n",
    "        pre_softmax_attn = dots\n",
    "\n",
    "        if talking_heads:\n",
    "            dots = einsum('b h i j, h k -> b k i j', dots, self.pre_softmax_proj).contiguous()\n",
    "\n",
    "        if exists(rel_pos):\n",
    "            dots = rel_pos(dots)\n",
    "\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, mask_value)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = dots.shape[-2:]\n",
    "            r = torch.arange(i, device=device)\n",
    "            mask = rearrange(r, 'i -> () () i ()') < rearrange(r, 'j -> () () () j')\n",
    "            mask = F.pad(mask, (j - i, 0), value=False)\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        if exists(self.sparse_topk) and self.sparse_topk < dots.shape[-1]:\n",
    "            top, _ = dots.topk(self.sparse_topk, dim=-1)\n",
    "            vk = top[..., -1].unsqueeze(-1).expand_as(dots)\n",
    "            mask = dots < vk\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        attn = self.attn_fn(dots, dim=-1)\n",
    "        post_softmax_attn = attn\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        if talking_heads:\n",
    "            attn = einsum('b h i j, h k -> b k i j', attn, self.post_softmax_proj).contiguous()\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        intermediates = Intermediates(\n",
    "            pre_softmax_attn=pre_softmax_attn,\n",
    "            post_softmax_attn=post_softmax_attn\n",
    "        )\n",
    "\n",
    "        return self.to_out(out), intermediates\n",
    "\n",
    "\n",
    "class AttentionLayers(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            depth,\n",
    "            heads=8,\n",
    "            causal=False,\n",
    "            cross_attend=False,\n",
    "            only_cross=False,\n",
    "            use_scalenorm=False,\n",
    "            use_rmsnorm=False,\n",
    "            use_rezero=False,\n",
    "            rel_pos_num_buckets=32,\n",
    "            rel_pos_max_distance=128,\n",
    "            position_infused_attn=False,\n",
    "            custom_layers=None,\n",
    "            sandwich_coef=None,\n",
    "            par_ratio=None,\n",
    "            residual_attn=False,\n",
    "            cross_residual_attn=False,\n",
    "            macaron=False,\n",
    "            pre_norm=True,\n",
    "            gate_residual=False,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ff_kwargs, kwargs = groupby_prefix_and_trim('ff_', kwargs)\n",
    "        attn_kwargs, _ = groupby_prefix_and_trim('attn_', kwargs)\n",
    "\n",
    "        dim_head = attn_kwargs.get('dim_head', DEFAULT_DIM_HEAD)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        self.has_pos_emb = position_infused_attn\n",
    "        self.pia_pos_emb = FixedPositionalEmbedding(dim) if position_infused_attn else None\n",
    "        self.rotary_pos_emb = always(None)\n",
    "\n",
    "        assert rel_pos_num_buckets <= rel_pos_max_distance, 'number of relative position buckets must be less than the relative position max distance'\n",
    "        self.rel_pos = None\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "        self.residual_attn = residual_attn\n",
    "        self.cross_residual_attn = cross_residual_attn\n",
    "\n",
    "        norm_class = ScaleNorm if use_scalenorm else nn.LayerNorm\n",
    "        norm_class = RMSNorm if use_rmsnorm else norm_class\n",
    "        norm_fn = partial(norm_class, dim)\n",
    "\n",
    "        norm_fn = nn.Identity if use_rezero else norm_fn\n",
    "        branch_fn = Rezero if use_rezero else None\n",
    "\n",
    "        if cross_attend and not only_cross:\n",
    "            default_block = ('a', 'c', 'f')\n",
    "        elif cross_attend and only_cross:\n",
    "            default_block = ('c', 'f')\n",
    "        else:\n",
    "            default_block = ('a', 'f')\n",
    "\n",
    "        if macaron:\n",
    "            default_block = ('f',) + default_block\n",
    "\n",
    "        if exists(custom_layers):\n",
    "            layer_types = custom_layers\n",
    "        elif exists(par_ratio):\n",
    "            par_depth = depth * len(default_block)\n",
    "            assert 1 < par_ratio <= par_depth, 'par ratio out of range'\n",
    "            default_block = tuple(filter(not_equals('f'), default_block))\n",
    "            par_attn = par_depth // par_ratio\n",
    "            depth_cut = par_depth * 2 // 3  # 2 / 3 attention layer cutoff suggested by PAR paper\n",
    "            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n",
    "            assert len(default_block) <= par_width, 'default block is too large for par_ratio'\n",
    "            par_block = default_block + ('f',) * (par_width - len(default_block))\n",
    "            par_head = par_block * par_attn\n",
    "            layer_types = par_head + ('f',) * (par_depth - len(par_head))\n",
    "        elif exists(sandwich_coef):\n",
    "            assert sandwich_coef > 0 and sandwich_coef <= depth, 'sandwich coefficient should be less than the depth'\n",
    "            layer_types = ('a',) * sandwich_coef + default_block * (depth - sandwich_coef) + ('f',) * sandwich_coef\n",
    "        else:\n",
    "            layer_types = default_block * depth\n",
    "\n",
    "        self.layer_types = layer_types\n",
    "        self.num_attn_layers = len(list(filter(equals('a'), layer_types)))\n",
    "\n",
    "        for layer_type in self.layer_types:\n",
    "            if layer_type == 'a':\n",
    "                layer = Attention(dim, heads=heads, causal=causal, **attn_kwargs)\n",
    "            elif layer_type == 'c':\n",
    "                layer = Attention(dim, heads=heads, **attn_kwargs)\n",
    "            elif layer_type == 'f':\n",
    "                layer = FeedForward(dim, **ff_kwargs)\n",
    "                layer = layer if not macaron else Scale(0.5, layer)\n",
    "            else:\n",
    "                raise Exception(f'invalid layer type {layer_type}')\n",
    "\n",
    "            if isinstance(layer, Attention) and exists(branch_fn):\n",
    "                layer = branch_fn(layer)\n",
    "\n",
    "            if gate_residual:\n",
    "                residual_fn = GRUGating(dim)\n",
    "            else:\n",
    "                residual_fn = Residual()\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                norm_fn(),\n",
    "                layer,\n",
    "                residual_fn\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            context=None,\n",
    "            mask=None,\n",
    "            context_mask=None,\n",
    "            mems=None,\n",
    "            return_hiddens=False\n",
    "    ):\n",
    "        hiddens = []\n",
    "        intermediates = []\n",
    "        prev_attn = None\n",
    "        prev_cross_attn = None\n",
    "\n",
    "        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n",
    "\n",
    "        for ind, (layer_type, (norm, block, residual_fn)) in enumerate(zip(self.layer_types, self.layers)):\n",
    "            is_last = ind == (len(self.layers) - 1)\n",
    "\n",
    "            if layer_type == 'a':\n",
    "                hiddens.append(x)\n",
    "                layer_mem = mems.pop(0)\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            if self.pre_norm:\n",
    "                x = norm(x)\n",
    "\n",
    "            if layer_type == 'a':\n",
    "                out, inter = block(x, mask=mask, sinusoidal_emb=self.pia_pos_emb, rel_pos=self.rel_pos,\n",
    "                                   prev_attn=prev_attn, mem=layer_mem)\n",
    "            elif layer_type == 'c':\n",
    "                out, inter = block(x, context=context, mask=mask, context_mask=context_mask, prev_attn=prev_cross_attn)\n",
    "            elif layer_type == 'f':\n",
    "                out = block(x)\n",
    "\n",
    "            x = residual_fn(out, residual)\n",
    "\n",
    "            if layer_type in ('a', 'c'):\n",
    "                intermediates.append(inter)\n",
    "\n",
    "            if layer_type == 'a' and self.residual_attn:\n",
    "                prev_attn = inter.pre_softmax_attn\n",
    "            elif layer_type == 'c' and self.cross_residual_attn:\n",
    "                prev_cross_attn = inter.pre_softmax_attn\n",
    "\n",
    "            if not self.pre_norm and not is_last:\n",
    "                x = norm(x)\n",
    "\n",
    "        if return_hiddens:\n",
    "            intermediates = LayerIntermediates(\n",
    "                hiddens=hiddens,\n",
    "                attn_intermediates=intermediates\n",
    "            )\n",
    "\n",
    "            return x, intermediates\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(AttentionLayers):\n",
    "    def __init__(self, **kwargs):\n",
    "        assert 'causal' not in kwargs, 'cannot set causality on encoder'\n",
    "        super().__init__(causal=False, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            num_tokens,\n",
    "            max_seq_len,\n",
    "            attn_layers,\n",
    "            emb_dim=None,\n",
    "            max_mem_len=0.,\n",
    "            emb_dropout=0.,\n",
    "            num_memory_tokens=None,\n",
    "            tie_embedding=False,\n",
    "            use_pos_emb=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, AttentionLayers), 'attention layers must be one of Encoder or Decoder'\n",
    "\n",
    "        dim = attn_layers.dim\n",
    "        emb_dim = default(emb_dim, dim)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_mem_len = max_mem_len\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n",
    "        self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len) if (\n",
    "                    use_pos_emb and not attn_layers.has_pos_emb) else always(0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.init_()\n",
    "\n",
    "        self.to_logits = nn.Linear(dim, num_tokens) if not tie_embedding else lambda t: t @ self.token_emb.weight.t()\n",
    "\n",
    "        # memory tokens (like [cls]) from Memory Transformers paper\n",
    "        num_memory_tokens = default(num_memory_tokens, 0)\n",
    "        self.num_memory_tokens = num_memory_tokens\n",
    "        if num_memory_tokens > 0:\n",
    "            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n",
    "\n",
    "            # let funnel encoder know number of memory tokens, if specified\n",
    "            if hasattr(attn_layers, 'num_memory_tokens'):\n",
    "                attn_layers.num_memory_tokens = num_memory_tokens\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.normal_(self.token_emb.weight, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            return_embeddings=False,\n",
    "            mask=None,\n",
    "            return_mems=False,\n",
    "            return_attn=False,\n",
    "            mems=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        # b, n, device, num_mem = *x.shape, x.device, self.num_memory_tokens\n",
    "        b = x.shape[0]\n",
    "        device = x.device\n",
    "        num_mem = self.num_memory_tokens\n",
    "\n",
    "        x = self.token_emb(x)\n",
    "        x += self.pos_emb(x)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        x = self.project_emb(x)\n",
    "\n",
    "        if num_mem > 0:\n",
    "            mem = repeat(self.memory_tokens, 'n d -> b n d', b=b)\n",
    "            x = torch.cat((mem, x), dim=1)\n",
    "\n",
    "            # auto-handle masking after appending memory tokens\n",
    "            if exists(mask):\n",
    "                mask = F.pad(mask, (num_mem, 0), value=True)\n",
    "\n",
    "        x, intermediates = self.attn_layers(x, mask=mask, mems=mems, return_hiddens=True, **kwargs)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        mem, x = x[:, :num_mem], x[:, num_mem:]\n",
    "\n",
    "        out = self.to_logits(x) if not return_embeddings else x\n",
    "\n",
    "        if return_mems:\n",
    "            hiddens = intermediates.hiddens\n",
    "            new_mems = list(map(lambda pair: torch.cat(pair, dim=-2), zip(mems, hiddens))) if exists(mems) else hiddens\n",
    "            new_mems = list(map(lambda t: t[..., -self.max_mem_len:, :].detach(), new_mems))\n",
    "            return out, new_mems\n",
    "\n",
    "        if return_attn:\n",
    "            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))\n",
    "            return out, attn_maps\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rMLcm0JdKah",
    "outputId": "7854dd26-f7a1-4882-f883-db99214c7b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kornia in /home/guisi/.local/lib/python3.12/site-packages (0.7.2)\n",
      "Requirement already satisfied: kornia-rs>=0.1.0 in /home/guisi/.local/lib/python3.12/site-packages (from kornia) (0.1.3)\n",
      "Requirement already satisfied: packaging in /opt/miniconda/lib/python3.12/site-packages (from kornia) (23.1)\n",
      "Requirement already satisfied: torch>=1.9.1 in /home/guisi/.local/lib/python3.12/site-packages (from kornia) (2.3.0+cu118)\n",
      "Requirement already satisfied: filelock in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (1.12)\n",
      "Requirement already satisfied: networkx in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/guisi/.local/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.8.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda/lib/python3.12/site-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/guisi/.local/lib/python3.12/site-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ov8f0OKhKVKN",
    "outputId": "390a17a2-c04e-430a-b6a8-fe34f32bb8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.0174, -0.0711, -0.0082],\n",
      "        [-0.0174,  1.0000, -0.0658, -0.0140],\n",
      "        [-0.0711, -0.0658,  1.0000,  0.0800],\n",
      "        [-0.0082, -0.0140,  0.0800,  1.0000]])\n",
      "tensor([[-0.7271, -1.7443, -1.8062, -1.7625],\n",
      "        [-1.7445, -0.7270, -1.8009, -1.7683],\n",
      "        [-1.7982, -1.7928, -0.7351, -1.6743],\n",
      "        [-1.7353, -1.7409, -1.6550, -0.7543]])\n",
      "tensor([-0.7271, -0.7270, -0.7351, -0.7543])\n",
      "tensor(0.7359)\n",
      "tensor([[-0.7271, -1.7445, -1.7982, -1.7353],\n",
      "        [-1.7443, -0.7270, -1.7928, -1.7409],\n",
      "        [-1.8062, -1.8009, -0.7351, -1.6550],\n",
      "        [-1.7625, -1.7683, -1.6743, -0.7543]])\n",
      "tensor([-0.7271, -0.7270, -0.7351, -0.7543])\n",
      "tensor(0.7359)\n",
      "tensor(0.7359)\n"
     ]
    }
   ],
   "source": [
    "#@title Encoders modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "# import clip\n",
    "import sys\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, AutoProcessor, CLIPVisionModel, CLIPVisionModelWithProjection\n",
    "import kornia\n",
    "\n",
    "class AbstractEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "class ClassEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim, n_classes=1000, key='class'):\n",
    "        super().__init__()\n",
    "        self.key = key\n",
    "        self.embedding = nn.Embedding(n_classes, embed_dim)\n",
    "\n",
    "    def forward(self, batch, key=None):\n",
    "        if key is None:\n",
    "            key = self.key\n",
    "        # this is for use in crossattn\n",
    "        c = batch[key][:, None]\n",
    "        c = self.embedding(c)\n",
    "        return c\n",
    "\n",
    "\n",
    "class TransformerEmbedder(AbstractEncoder):\n",
    "    \"\"\"Some transformer encoder layers\"\"\"\n",
    "    def __init__(self, n_embed, n_layer, vocab_size, max_seq_len=77, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,\n",
    "                                              attn_layers=Encoder(dim=n_embed, depth=n_layer))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        tokens = tokens.to(self.device)  # meh\n",
    "        z = self.transformer(tokens, return_embeddings=True)\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self(x)\n",
    "\n",
    "\n",
    "class BERTTokenizer(AbstractEncoder):\n",
    "    \"\"\" Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)\"\"\"\n",
    "    def __init__(self, device=\"cuda\", vq_interface=True, max_length=77):\n",
    "        super().__init__()\n",
    "        from transformers import BertTokenizerFast  # TODO: add to reuquirements\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "        self.device = device\n",
    "        self.vq_interface = vq_interface\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        return tokens\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, text):\n",
    "        tokens = self(text)\n",
    "        if not self.vq_interface:\n",
    "            return tokens\n",
    "        return None, None, [None, None, tokens]\n",
    "\n",
    "    def decode(self, text):\n",
    "        return text\n",
    "\n",
    "\n",
    "class BERTEmbedder(AbstractEncoder):\n",
    "    \"\"\"Uses the BERT tokenizr model and add some transformer encoder layers\"\"\"\n",
    "    def __init__(self, n_embed, n_layer, vocab_size=30522, max_seq_len=77,\n",
    "                 device=\"cuda\",use_tokenizer=True, embedding_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.use_tknz_fn = use_tokenizer\n",
    "        if self.use_tknz_fn:\n",
    "            self.tknz_fn = BERTTokenizer(vq_interface=False, max_length=max_seq_len)\n",
    "        self.device = device\n",
    "        self.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,\n",
    "                                              attn_layers=Encoder(dim=n_embed, depth=n_layer),\n",
    "                                              emb_dropout=embedding_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        if self.use_tknz_fn:\n",
    "            tokens = self.tknz_fn(text)#.to(self.device)\n",
    "        else:\n",
    "            tokens = text\n",
    "        z = self.transformer(tokens, return_embeddings=True)\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        # output of length 77\n",
    "        return self(text)\n",
    "\n",
    "\n",
    "class SpatialRescaler(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_stages=1,\n",
    "                 method='bilinear',\n",
    "                 multiplier=0.5,\n",
    "                 in_channels=3,\n",
    "                 out_channels=None,\n",
    "                 bias=False):\n",
    "        super().__init__()\n",
    "        self.n_stages = n_stages\n",
    "        assert self.n_stages >= 0\n",
    "        assert method in ['nearest','linear','bilinear','trilinear','bicubic','area']\n",
    "        self.multiplier = multiplier\n",
    "        self.interpolator = partial(torch.nn.functional.interpolate, mode=method)\n",
    "        self.remap_output = out_channels is not None\n",
    "        if self.remap_output:\n",
    "            print(f'Spatial Rescaler mapping from {in_channels} to {out_channels} channels after resizing.')\n",
    "            self.channel_mapper = nn.Conv2d(in_channels,out_channels,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        for stage in range(self.n_stages):\n",
    "            x = self.interpolator(x, scale_factor=self.multiplier)\n",
    "\n",
    "\n",
    "        if self.remap_output:\n",
    "            x = self.channel_mapper(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self(x)\n",
    "\n",
    "class FrozenCLIPEmbedder(AbstractEncoder):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n",
    "        super().__init__()\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokens = batch_encoding[\"input_ids\"]#.to(self.device)\n",
    "        outputs = self.transformer(input_ids=tokens)\n",
    "\n",
    "        z = outputs.last_hidden_state\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self(text)\n",
    "\n",
    "class FrozenImageEmbedder(AbstractEncoder):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n",
    "        super().__init__()\n",
    "        # self.processor = AutoProcessor.from_pretrained(version)\n",
    "        self.transformer = CLIPVisionModelWithProjection.from_pretrained(version)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # image = Image.open(requests.get(url, stream=True).raw)\n",
    "        # inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = self.transformer(**inputs)\n",
    "        image_embeds = outputs.image_embeds\n",
    "        return image_embeds\n",
    "        # z = outputs.last_hidden_state\n",
    "\n",
    "        # return z\n",
    "\n",
    "    def encode(self, inputs):\n",
    "        return self(inputs)\n",
    "\n",
    "\n",
    "class FrozenCLIPTextEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses the CLIP transformer encoder for text.\n",
    "    \"\"\"\n",
    "    def __init__(self, version='ViT-L/14', device=\"cuda\", max_length=77, n_repeat=1, normalize=True):\n",
    "        super().__init__()\n",
    "        self.model, _ = clip.load(version, jit=False, device=\"cpu\")\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.n_repeat = n_repeat\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def freeze(self):\n",
    "        self.model = self.model.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        tokens = clip.tokenize(text).to(self.device)\n",
    "        z = self.model.encode_text(tokens)\n",
    "        if self.normalize:\n",
    "            z = z / torch.linalg.norm(z, dim=1, keepdim=True)\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        z = self(text)\n",
    "        if z.ndim==2:\n",
    "            z = z[:, None, :]\n",
    "        z = repeat(z, 'b 1 d -> b k d', k=self.n_repeat)\n",
    "        return z\n",
    "\n",
    "\n",
    "class FrozenClipImageEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "        Uses the CLIP image encoder.\n",
    "        \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model = 'ViT-L/14',\n",
    "            jit=False,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            antialias=False,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.model, _ = clip.load(name=model, device=device, jit=jit)\n",
    "\n",
    "        self.antialias = antialias\n",
    "\n",
    "        self.register_buffer('mean', torch.Tensor([0.48145466, 0.4578275, 0.40821073]), persistent=False)\n",
    "        self.register_buffer('std', torch.Tensor([0.26862954, 0.26130258, 0.27577711]), persistent=False)\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        # normalize to [0,1]\n",
    "        x = kornia.geometry.resize(x, (224, 224),\n",
    "                                   interpolation='bicubic',align_corners=True,\n",
    "                                   antialias=self.antialias)\n",
    "        x = (x + 1.) / 2.\n",
    "        # renormalize according to clip\n",
    "        x = kornia.enhance.normalize(x, self.mean, self.std)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is assumed to be in range [-1,1]\n",
    "        return self.model.encode_image(self.preprocess(x))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from dc_ldm.util import count_params\n",
    "    # text_model = FrozenCLIPEmbedder()\n",
    "    # text = ['a dog']\n",
    "    # text_out = text_model(text)\n",
    "    # print(text_out.shape)\n",
    "    # FrozenCLIPEmbedder\n",
    "\n",
    "#     def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "#         return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n",
    "\n",
    "\n",
    "#     def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "#         caption_loss = contrastive_loss(similarity)\n",
    "#         image_loss = contrastive_loss(similarity.t())\n",
    "#         return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "#     input = Image.open('../dreamdiffusion/datasets/imageNet_images/n02106662/n02106662_1451.JPEG')\n",
    "\n",
    "#     from transformers import AutoProcessor, CLIPModel\n",
    "\n",
    "#     model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "#     processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# # url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# # image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "#     inputs = processor(\n",
    "#         text=[\"a photo of a cat\", \"a photo of a dog\"], images=input, return_tensors=\"pt\", padding=True\n",
    "#     )\n",
    "    def contrastive_loss(logits, dim):\n",
    "        m = nn.functional.log_softmax(logits, dim=dim)\n",
    "        print(m)\n",
    "        neg_ce = torch.diag(m)\n",
    "        print(neg_ce)\n",
    "        print(-neg_ce.mean())\n",
    "        return -neg_ce.mean()\n",
    "\n",
    "    def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "        caption_loss = contrastive_loss(similarity, dim=0)\n",
    "        image_loss = contrastive_loss(similarity, dim=1)\n",
    "        return (caption_loss + image_loss) / 2.0\n",
    "#     outputs = model(**inputs)\n",
    "#     logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "#     probs = logits_per_image.softmax(dim=1)\n",
    "#     print(probs)\n",
    "#     print(outputs.text_embeds.shape)\n",
    "#     print(outputs.image_embeds.shape)\n",
    "#     f = torch.cosine_similarity(outputs.text_embeds, outputs.image_embeds, dim=-1)\n",
    "#     print(f)\n",
    "#     print(model.logit_scale.exp())\n",
    "# # logits_per_text\n",
    "#     logits_per_text = torch.matmul(outputs.text_embeds, outputs.image_embeds.t()) * model.logit_scale.exp()\n",
    "#     logits_per_image = logits_per_text.t()\n",
    "#     print(logits_per_text)\n",
    "#     print(logits_per_image)\n",
    "#     print(clip_loss(logits_per_text))\n",
    "    z_i = torch.randn(4, 768)\n",
    "    z_j = z_i\n",
    "    # representations = torch.cat([z_i, z_j], dim=0)          # repre: (2*bs, dim)\n",
    "    # print(representations.shape)\n",
    "    # print(representations.unsqueeze(1).shape)\n",
    "    # print(representations.unsqueeze(0).shape)\n",
    "    similarity_matrix = nn.functional.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2)\n",
    "    print(similarity_matrix)\n",
    "    print(clip_loss(similarity_matrix))\n",
    "\n",
    "    # model = FrozenImageEmbedder()\n",
    "    # # out = model(input)\n",
    "    # # print(out.shape)\n",
    "\n",
    "    # # model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    # processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "\n",
    "\n",
    "    # # input = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    # inputs = processor(images=input, return_tensors=\"pt\")\n",
    "    # # for k, v in inputs.items():\n",
    "    # #     print(k)\n",
    "    # #     print(v.shape)\n",
    "    # # print()\n",
    "    # # print(inputs)\n",
    "\n",
    "    # outputs = model(inputs)\n",
    "    # # image_embeds = outputs.image_embeds\n",
    "    # print(outputs.shape)\n",
    "\n",
    "\n",
    "    # from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "    # model_text = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # inputs_text = tokenizer([\"a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # outputs_text = model_text(**inputs_text)\n",
    "    # text_embeds = outputs_text.text_embeds\n",
    "    # f = torch.cosine_similarity(outputs, text_embeds, dim=-1)\n",
    "    # print(f)\n",
    "\n",
    "    # image_embeds = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "    # text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    #     # cosine similarity as logits\n",
    "    # logit_scale = torch.tensor([2.6592]).exp()\n",
    "    # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "    # print(logits_per_text)\n",
    "    # logits_per_image = logits_per_text.t()\n",
    "    # print(logits_per_image)\n",
    "\n",
    "\n",
    "\n",
    "    # print(outputs)\n",
    "    # count_params(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLmX43DaTP9Q",
    "outputId": "505ae968-14f7-43ff-974c-a775f2791bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping keras as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-datasets as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-estimator as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-gcs-config as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-hub as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-io-gcs-filesystem as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-metadata as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y keras tensorflow tensorflow-datasets tensorflow-estimator tensorflow-gcs-config tensorflow-hub tensorflow-io-gcs-filesystem tensorflow-metadata tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "oSPMkYaGVMej"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep -E \"tensorflow|keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib-inline in /opt/miniconda/lib/python3.12/site-packages (0.1.6)\n",
      "Requirement already satisfied: traitlets in /opt/miniconda/lib/python3.12/site-packages (from matplotlib-inline) (5.14.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib-inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib-inline in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (0.1.7)\n",
      "Requirement already satisfied: traitlets in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from matplotlib-inline) (5.14.3)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib-inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipython in /opt/miniconda/lib/python3.12/site-packages (8.22.2)\n",
      "Requirement already satisfied: decorator in /opt/miniconda/lib/python3.12/site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/miniconda/lib/python3.12/site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/miniconda/lib/python3.12/site-packages (from ipython) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/miniconda/lib/python3.12/site-packages (from ipython) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/miniconda/lib/python3.12/site-packages (from ipython) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/miniconda/lib/python3.12/site-packages (from ipython) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/miniconda/lib/python3.12/site-packages (from ipython) (5.14.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/miniconda/lib/python3.12/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/miniconda/lib/python3.12/site-packages (from jedi>=0.16->ipython) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/miniconda/lib/python3.12/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/miniconda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/miniconda/lib/python3.12/site-packages (from stack-data->ipython) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/miniconda/lib/python3.12/site-packages (from stack-data->ipython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/miniconda/lib/python3.12/site-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/miniconda/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (8.12.3)\n",
      "Requirement already satisfied: backcall in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: pickleshare in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (3.0.45)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (4.12.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from stack-data->ipython) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from stack-data->ipython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"main.py\", line 339\n",
      "    self.data = torch.from_numpy(ndarray)[loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==subject]\n",
      "                                                               ^\n",
      "SyntaxError: invalid syntax\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 9390) of binary: /home/guisi/.conda/envs/dreamdiffusion/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/guisi/.conda/envs/dreamdiffusion/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torch/distributed/run.py\", line 761, in main\n",
      "    run(args)\n",
      "  File \"/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torch/distributed/run.py\", line 752, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 245, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "main.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-06-04_09:57:29\n",
      "  host      : nvidia-gpu-optimized-vmi-1-vm.c.neuro-cife.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 9390)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset', 'image', 'label'])\n",
      "[{'eeg': array([[-6.80129412e-03, -8.13837010e-03, -7.24686161e-03, ...,\n",
      "         1.38477710e-05,  1.24644385e-05,  1.26223081e-05],\n",
      "       [-3.86516619e-03, -4.62602843e-03, -4.12041591e-03, ...,\n",
      "         7.35388677e-06,  6.62768373e-06,  7.17609039e-06],\n",
      "       [-2.82500738e-03, -3.38188469e-03, -3.01239802e-03, ...,\n",
      "         5.32329222e-06,  4.18285358e-06,  4.79385925e-06],\n",
      "       ...,\n",
      "       [-7.14428731e-03, -8.54811083e-03, -7.61207333e-03, ...,\n",
      "         1.20062692e-05,  9.07801884e-06,  8.92867290e-06],\n",
      "       [-5.59089883e-03, -6.69045521e-03, -5.95824649e-03, ...,\n",
      "         1.27233213e-05,  9.12206139e-06,  8.60137537e-06],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), 'image': 0, 'label': 0, 'subject': 1}, {'eeg': array([[ 1.48554921e-05,  1.76714405e-05,  1.84898875e-05, ...,\n",
      "         7.72299451e-06,  6.32875855e-06,  5.53869265e-06],\n",
      "       [ 9.14268908e-06,  1.12128490e-05,  1.17397055e-05, ...,\n",
      "         2.68476280e-06,  2.22038851e-06,  1.72381014e-06],\n",
      "       [ 7.16032478e-06,  9.61598830e-06,  1.02727957e-05, ...,\n",
      "        -1.68751564e-07, -5.00811681e-07, -3.78137523e-07],\n",
      "       ...,\n",
      "       [ 1.25768446e-05,  1.68197430e-05,  1.71353733e-05, ...,\n",
      "         2.01430724e-05,  1.78473398e-05,  1.53718078e-05],\n",
      "       [ 1.23003479e-05,  1.67425134e-05,  1.68673839e-05, ...,\n",
      "         1.55748510e-05,  1.59815400e-05,  1.65169499e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), 'image': 1, 'label': 0, 'subject': 1}, {'eeg': array([[ 5.46787843e-06,  6.30932125e-06,  7.63585833e-06, ...,\n",
      "         1.53836829e-05,  1.47124864e-05,  1.35900713e-05],\n",
      "       [ 1.36415514e-06,  1.52572115e-06,  2.18212528e-06, ...,\n",
      "         8.62947691e-06,  8.00443110e-06,  8.37126703e-06],\n",
      "       [-5.84021017e-08,  4.70350767e-07,  1.09233576e-06, ...,\n",
      "         7.77922884e-06,  7.24054055e-06,  7.10461921e-06],\n",
      "       ...,\n",
      "       [ 1.33697984e-05,  1.25026965e-05,  1.22846506e-05, ...,\n",
      "         5.86078202e-06,  4.24590489e-06,  2.38572976e-06],\n",
      "       [ 1.54992600e-05,  1.31340043e-05,  1.10681438e-05, ...,\n",
      "         2.42496502e-06,  2.17874873e-06,  3.61948463e-07],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), 'image': 2, 'label': 0, 'subject': 1}, {'eeg': array([[ 1.27923176e-05,  1.35376500e-05,  1.60580067e-05, ...,\n",
      "         3.59857907e-06,  9.55364657e-06,  1.63211172e-05],\n",
      "       [ 9.34321193e-06,  1.03874620e-05,  1.11416156e-05, ...,\n",
      "        -4.69074923e-06, -2.69901315e-06,  1.04922191e-06],\n",
      "       [ 7.19987672e-06,  7.82945820e-06,  9.27721404e-06, ...,\n",
      "        -7.70587807e-06, -6.09871408e-06, -2.89538870e-06],\n",
      "       ...,\n",
      "       [ 1.81485394e-06,  3.27949361e-06,  6.06313765e-06, ...,\n",
      "         1.09796148e-05,  1.94476012e-05,  2.48479297e-05],\n",
      "       [-1.03534607e-06,  2.19547310e-07,  4.08766564e-06, ...,\n",
      "         7.90772377e-06,  1.40182896e-05,  1.68534484e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), 'image': 3, 'label': 0, 'subject': 1}, {'eeg': array([[2.12296399e-05, 2.26830233e-05, 2.10074022e-05, ...,\n",
      "        1.35161778e-05, 1.05636408e-05, 6.04643659e-06],\n",
      "       [5.64184485e-06, 8.70503149e-06, 8.26514009e-06, ...,\n",
      "        9.63094526e-06, 7.95942938e-06, 5.58017949e-06],\n",
      "       [8.79734209e-07, 3.35739910e-06, 3.25658687e-06, ...,\n",
      "        8.47181338e-06, 6.64347758e-06, 4.63420607e-06],\n",
      "       ...,\n",
      "       [2.60217381e-05, 2.39543817e-05, 2.05298591e-05, ...,\n",
      "        3.73924275e-05, 3.44034965e-05, 3.16355166e-05],\n",
      "       [1.56706769e-05, 1.22719670e-05, 9.31928776e-06, ...,\n",
      "        3.80860422e-05, 3.76466959e-05, 3.38937670e-05],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]), 'image': 4, 'label': 0, 'subject': 1}, {'eeg': array([[ 2.44642016e-06,  8.10098480e-07,  4.85105830e-08, ...,\n",
      "        -2.83398297e-05, -3.36435565e-05, -3.02705378e-05],\n",
      "       [ 3.41667777e-06,  1.50630805e-06, -6.17145250e-07, ...,\n",
      "        -1.18103570e-05, -1.50896546e-05, -1.42003479e-05],\n",
      "       [ 3.07774174e-06,  1.50049018e-06, -8.14001249e-07, ...,\n",
      "        -7.76808896e-06, -1.07083690e-05, -1.11301229e-05],\n",
      "       ...,\n",
      "       [ 3.18358890e-05,  3.29654551e-05,  3.12419401e-05, ...,\n",
      "         1.78398349e-06, -3.20006419e-06, -6.14112946e-06],\n",
      "       [ 3.04474113e-05,  2.90263837e-05,  2.84299897e-05, ...,\n",
      "         1.21838212e-05,  8.63723662e-06,  4.63096582e-06],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), 'image': 5, 'label': 0, 'subject': 1}, {'eeg': array([[-2.04704754e-05, -9.69065264e-06, -2.53383763e-06, ...,\n",
      "         9.65091823e-06,  9.54348684e-06,  9.07809097e-06],\n",
      "       [-9.67868146e-06, -3.96408854e-06,  4.10105660e-07, ...,\n",
      "         7.64418483e-06,  8.10942069e-06,  7.88527461e-06],\n",
      "       [-8.47505248e-06, -3.68620318e-06,  1.36970962e-06, ...,\n",
      "         6.73159080e-06,  7.31277181e-06,  6.68382883e-06],\n",
      "       ...,\n",
      "       [-5.78059255e-06, -2.65607597e-06,  1.18701495e-06, ...,\n",
      "         1.56788510e-05,  1.58479360e-05,  1.59754242e-05],\n",
      "       [ 2.12380714e-06,  2.60028403e-06,  5.55207670e-06, ...,\n",
      "         1.17120067e-05,  1.14418510e-05,  1.21442435e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), 'image': 6, 'label': 0, 'subject': 1}, {'eeg': array([[8.56126015e-06, 8.41952327e-06, 8.44439759e-06, ...,\n",
      "        1.51406645e-05, 1.62707313e-05, 1.61389973e-05],\n",
      "       [6.57256144e-06, 4.69030181e-06, 3.08582158e-06, ...,\n",
      "        8.44380517e-06, 9.80619640e-06, 9.80194657e-06],\n",
      "       [4.96788427e-06, 2.92206227e-06, 1.22509604e-06, ...,\n",
      "        9.01370413e-06, 9.27986603e-06, 8.08870989e-06],\n",
      "       ...,\n",
      "       [1.54638234e-05, 1.42410864e-05, 1.30451558e-05, ...,\n",
      "        1.54934724e-05, 1.62734028e-05, 1.57329576e-05],\n",
      "       [1.31399013e-05, 1.36638130e-05, 1.35504654e-05, ...,\n",
      "        1.34070214e-05, 1.28167497e-05, 1.15894565e-05],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]), 'image': 7, 'label': 0, 'subject': 1}, {'eeg': array([[1.44026280e-05, 1.16932587e-05, 9.36628031e-06, ...,\n",
      "        1.00435648e-05, 1.04029151e-05, 1.16479214e-05],\n",
      "       [8.21865916e-06, 6.01078953e-06, 4.73293184e-06, ...,\n",
      "        7.71782608e-06, 7.62128053e-06, 8.54888130e-06],\n",
      "       [6.06124985e-06, 4.23014406e-06, 3.55500703e-06, ...,\n",
      "        8.56863621e-06, 7.37191964e-06, 6.67837117e-06],\n",
      "       ...,\n",
      "       [1.40469751e-05, 1.23817386e-05, 1.22240213e-05, ...,\n",
      "        5.90723391e-06, 5.36697844e-06, 6.18224790e-06],\n",
      "       [1.01199073e-05, 9.25365437e-06, 9.95384059e-06, ...,\n",
      "        4.87861380e-06, 2.95444208e-06, 2.98468829e-06],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]), 'image': 8, 'label': 0, 'subject': 1}, {'eeg': array([[1.36667999e-05, 1.54283669e-05, 1.55551096e-05, ...,\n",
      "        7.85388496e-06, 5.60676341e-06, 2.59279155e-06],\n",
      "       [1.01301546e-05, 1.11376102e-05, 1.03502272e-05, ...,\n",
      "        5.88573758e-06, 3.72127468e-06, 2.04294637e-06],\n",
      "       [6.81283759e-06, 7.27950158e-06, 7.04273422e-06, ...,\n",
      "        2.68979364e-06, 1.65138487e-06, 7.82351394e-07],\n",
      "       ...,\n",
      "       [8.17026355e-06, 9.98178676e-06, 1.02361441e-05, ...,\n",
      "        1.35362026e-05, 1.47168360e-05, 1.55431466e-05],\n",
      "       [5.46270841e-06, 8.48670594e-06, 9.50992673e-06, ...,\n",
      "        1.38247825e-05, 1.30938085e-05, 1.23236714e-05],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]), 'image': 9, 'label': 0, 'subject': 1}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the .pth file\n",
    "checkpoint = torch.load('dataset/eegdataset/eeg_data.pth')\n",
    "print(checkpoint.keys())\n",
    "\n",
    "model_state_dict = checkpoint['dataset'][0:10]\n",
    "print(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'config'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(checkpoint\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 11\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_state_dict)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Load the .pth file\n",
    "checkpoint = torch.load('/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth')\n",
    "print(checkpoint.keys())\n",
    "\n",
    "model_state_dict = checkpoint['model'][0]\n",
    "print(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-iqg1q0UWEZu",
    "outputId": "31967e4d-3996-4a74-c7b7-5d168edc2fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root_path': 'dataset/eegdataset2/', 'output_path': 'dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03', 'eeg_signals_path': 'dataset/eegdataset/eeg_data.pth', 'splits_path': 'dataset/eegdataset/empty_data.pth', 'dataset': 'EEG', 'pretrain_mbm_path': '/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth', 'include_nonavg_test': True, 'lr': 5.3e-05, 'min_lr': 0.0, 'weight_decay': 0.05, 'num_epoch': 15, 'warmup_epochs': 2, 'batch_size': 4, 'clip_grad': 0.8, 'mask_ratio': 0.5, 'patch_size': 4, 'embed_dim': 1024, 'decoder_embed_dim': 512, 'depth': 24, 'num_heads': 16, 'decoder_num_heads': 16, 'mlp_ratio': 1.0, 'seed': 2022, 'roi': 'VC', 'aug_times': 1, 'num_sub_limit': None, 'include_hcp': True, 'include_kam': True, 'accum_iter': 1, 'use_nature_img_loss': False, 'img_recon_weight': 0.5, 'focus_range': None, 'focus_rate': 0.6, 'local_rank': 0, 'time_len': 512, 'pretrain_gm_path': '/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Splitter object at 0x7384366d9280>\n",
      "OLA\n",
      "164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ivh0jsai) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 17.7%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-yogurt-74</strong> at: <a href='https://wandb.ai/neuro-cife/dreamdiffusion/runs/ivh0jsai' target=\"_blank\">https://wandb.ai/neuro-cife/dreamdiffusion/runs/ivh0jsai</a><br/> View project at: <a href='https://wandb.ai/neuro-cife/dreamdiffusion' target=\"_blank\">https://wandb.ai/neuro-cife/dreamdiffusion</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240618_182954-ivh0jsai/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ivh0jsai). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240618_183114-e9zwkbq2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neuro-cife/dreamdiffusion/runs/e9zwkbq2' target=\"_blank\">stellar-terrain-75</a></strong> to <a href='https://wandb.ai/neuro-cife/dreamdiffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neuro-cife/dreamdiffusion' target=\"_blank\">https://wandb.ai/neuro-cife/dreamdiffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neuro-cife/dreamdiffusion/runs/e9zwkbq2' target=\"_blank\">https://wandb.ai/neuro-cife/dreamdiffusion/runs/e9zwkbq2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "missing keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "unexpected keys: ['mask_token']\n",
      "load ldm successfully\n"
     ]
    }
   ],
   "source": [
    "#@title Use the pretrained EEG representation with existing generative checkpoints to generate images\n",
    "import sys\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import datetime\n",
    "import argparse\n",
    "import torch\n",
    "from torch.nn import Identity\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "sys.path.append(\"/home/guisi/DreamDiffusion/code\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"EINOPS_BACKEND\"] = \"pytorch\"\n",
    "\n",
    "def to_image(img):\n",
    "    if img.shape[-1] != 3:\n",
    "        img = torch.tensor(img)\n",
    "        img = rearrange(img, 'c h w -> h w c')\n",
    "    img = 255. * img\n",
    "    return Image.fromarray(img.astype(np.uint8))\n",
    "\n",
    "def channel_last(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        return img\n",
    "    return rearrange(img, 'c h w -> h w c')\n",
    "\n",
    "def normalize(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "\n",
    "def wandb_init(config):\n",
    "    wandb.init( project=\"dreamdiffusion\",\n",
    "                group='eval',\n",
    "                anonymous=\"allow\",\n",
    "                config=config,\n",
    "                reinit=True)\n",
    "\n",
    "class random_crop:\n",
    "    def __init__(self, size, p):\n",
    "        self.size = size\n",
    "        self.p = p\n",
    "    def __call__(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.RandomCrop(size=(self.size, self.size))(img)\n",
    "        return img\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Double Conditioning LDM Finetuning', add_help=False)\n",
    "    # project parameters\n",
    "    parser.add_argument('--root', type=str, default='../dreamdiffusion/')\n",
    "    parser.add_argument('--dataset', type=str, default='GOD')\n",
    "    parser.add_argument('--model_path', type=str)\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "#args = get_args_parser()\n",
    "#root = args.root\n",
    "target = 'EEG'\n",
    "\n",
    "sd = torch.load('/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth', map_location='cpu')\n",
    "config = sd['config']\n",
    "# update paths\n",
    "config.root_path = 'dataset/eegdataset2/'\n",
    "config.pretrain_mbm_path = '/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth'\n",
    "config.pretrain_gm_path = '/home/guisi/DreamDiffusion/dataset/eegdataset2/results/eeg_pretrain/04-06-2024-09-49-03/checkpoints/checkpoint.pth'\n",
    "print(config.__dict__)\n",
    "\n",
    "output_path = os.path.join(config.root_path, 'results', 'eval',\n",
    "                '%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "crop_pix = int(0.5*256)\n",
    "img_transform_train = transforms.Compose([\n",
    "    normalize,\n",
    "    transforms.Resize((512, 512)),\n",
    "    # random_crop(config.img_size-crop_pix, p=0.5),\n",
    "    # transforms.Resize((256, 256)),\n",
    "    channel_last\n",
    "])\n",
    "img_transform_test = transforms.Compose([\n",
    "    normalize, transforms.Resize((512, 512)),\n",
    "    channel_last\n",
    "])\n",
    "\n",
    "\n",
    "splits_path = 'dataset/eeg_dataset/block_splits_by_image_single.pth'\n",
    "dataset_train, dataset_test = create_EEG_dataset(eeg_signals_path = 'dataset/eegdataset/eeg_data.pth', splits_path = splits_path,\n",
    "            image_transform=[img_transform_train, img_transform_test], subject = 1)\n",
    "\n",
    "print(dataset_train)\n",
    "print(\"OLA\")\n",
    "\n",
    "num_voxels = dataset_test.dataset.data_len\n",
    "\n",
    "# num_voxels = dataset_test.num_voxels\n",
    "print(len(dataset_test))\n",
    "# prepare pretrained mae\n",
    "pretrain_mbm_metafile = torch.load(config.pretrain_mbm_path, map_location='cpu')\n",
    "config.ddim_steps = 15\n",
    "config.global_pool = False\n",
    "config.use_time_cond = False\n",
    "# create generateive model\n",
    "\n",
    "\n",
    "logger = wandb_logger(config) if config.local_rank == 0 else None\n",
    "\n",
    "generative_model = eLDM(pretrain_mbm_metafile, num_voxels,\n",
    "            device=device, pretrain_root=config.pretrain_gm_path, logger=logger,\n",
    "            ddim_steps=config.ddim_steps, global_pool=config.global_pool, use_time_cond=config.use_time_cond)\n",
    "# m, u = model.load_state_dict(pl_sd, strict=False)\n",
    "generative_model.model.load_state_dict(sd['model'], strict=False)\n",
    "print('load ldm successfully')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "config.num_samples = 3\n",
    "config.HW = None \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import datetime\n",
    "import argparse\n",
    "import torch\n",
    "from torch.nn import Identity\n",
    "\n",
    "def create_trainer(num_epoch, precision=32, accumulate_grad_batches=2,logger=None,check_val_every_n_epoch=0):\n",
    "    acc = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    return pl.Trainer(accelerator=acc, max_epochs=num_epoch, logger=logger, \n",
    "            precision=precision, accumulate_grad_batches=accumulate_grad_batches,\n",
    "            enable_checkpointing=False, enable_model_summary=False, gradient_clip_val=0.5,\n",
    "            check_val_every_n_epoch=check_val_every_n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(num_epoch=6, precision=32, logger=logger, check_val_every_n_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "splits_path = 'dataset/eeg_dataset/block_splits_by_image_single.pth'\n",
    "dataset_train, dataset_test = create_EEG_dataset(eeg_signals_path = 'dataset/eegdataset/eeg_data.pth', splits_path = splits_path,\n",
    "            image_transform=[img_transform_train, img_transform_test], subject = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_path = 'dataset/eeg_dataset/block_splits_by_image_single.pth'\n",
    "dataset_train, dataset_test = create_EEG_dataset(eeg_signals_path = 'dataset/eegdataset/eeg_data.pth', splits_path = splits_path,\n",
    "            image_transform=[img_transform_train, img_transform_test], subject = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: wandb\n",
      "Version: 0.17.0\n",
      "Summary: A CLI and library for interacting with the Weights & Biases API.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Weights & Biases <support@wandb.com>\n",
      "License: MIT License\n",
      "        \n",
      "        Copyright (c) 2021 Weights and Biases, Inc.\n",
      "        \n",
      "        Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "        of this software and associated documentation files (the \"Software\"), to deal\n",
      "        in the Software without restriction, including without limitation the rights\n",
      "        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "        copies of the Software, and to permit persons to whom the Software is\n",
      "        furnished to do so, subject to the following conditions:\n",
      "        \n",
      "        The above copyright notice and this permission notice shall be included in all\n",
      "        copies or substantial portions of the Software.\n",
      "        \n",
      "        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "        SOFTWARE.\n",
      "Location: /home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages\n",
      "Requires: click, docker-pycreds, gitpython, platformdirs, protobuf, psutil, pyyaml, requests, sentry-sdk, setproctitle, setuptools, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Stage One: only optimize conditional encoders #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Only optimizing conditioner params!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torch/optim/adamw.py:91: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(AdamW, self).__init__(params, defaults)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007138967514038086,
       "initial": 0,
       "n": 0,
       "ncols": 99,
       "nrows": 12,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "firmitas/firmitas\n",
      "2\n",
      "firmitas/firmitas\n",
      "7\n",
      "firmitas/firmitas\n",
      "26\n",
      "firmitas/firmitas\n",
      "rendering 3 examples in 15 steps.\n",
      "torch.Size([3, 129, 512])\n",
      "torch.Size([3, 128, 512])\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:12,  1.25it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:05,  2.38it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:01<00:03,  3.35it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:01<00:02,  4.14it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:01<00:02,  4.76it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:01,  5.23it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:01,  5.59it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:01<00:01,  5.85it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:02<00:01,  6.03it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:02<00:00,  6.17it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:02<00:00,  6.26it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:02<00:00,  6.33it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:02<00:00,  6.38it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:02<00:00,  6.42it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:02<00:00,  6.45it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:03<00:00,  5.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 3 examples in 15 steps.\n",
      "torch.Size([3, 129, 512])\n",
      "torch.Size([3, 128, 512])\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:04,  3.33it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:02,  4.67it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:00<00:02,  5.37it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:00<00:02,  5.77it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:00<00:01,  6.02it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:01,  6.18it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:01,  6.28it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:01<00:01,  6.36it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:01<00:01,  6.38it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:01<00:00,  6.43it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:01<00:00,  6.45it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:01<00:00,  6.47it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:02<00:00,  6.49it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:02<00:00,  6.50it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:02<00:00,  6.50it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:02<00:00,  6.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 3 examples in 15 steps.\n",
      "torch.Size([3, 129, 512])\n",
      "torch.Size([3, 128, 512])\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:04,  3.31it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:03,  4.66it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:00<00:02,  5.36it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:00<00:02,  5.76it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:00<00:01,  6.01it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:01,  6.17it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:01,  6.29it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:01<00:01,  6.37it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:01<00:01,  6.41it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:01<00:00,  6.44it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:01<00:00,  6.47it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:01<00:00,  6.48it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:02<00:00,  6.49it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:02<00:00,  6.50it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:02<00:00,  6.50it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:02<00:00,  6.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 3 examples in 15 steps.\n",
      "torch.Size([3, 129, 512])\n",
      "torch.Size([3, 128, 512])\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:04,  3.32it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:03,  4.66it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:00<00:02,  5.36it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:00<00:02,  5.76it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:00<00:01,  6.01it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:01,  6.18it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:01,  6.28it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:01<00:01,  6.36it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:01<00:01,  6.41it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:01<00:00,  6.45it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:01<00:00,  6.47it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:01<00:00,  6.48it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:02<00:00,  6.50it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:02<00:00,  6.51it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:02<00:00,  6.51it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:02<00:00,  6.14it/s]\u001b[A\n",
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### run full validation! ######\n",
      "\n",
      "rendering 5 examples in 15 steps.\n",
      "torch.Size([5, 129, 512])\n",
      "torch.Size([5, 128, 512])\n",
      "Data shape for PLMS sampling is (5, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:07,  2.00it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:04,  2.92it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:00<00:03,  3.42it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:01<00:03,  3.72it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:01<00:02,  3.91it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:02,  4.04it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:02,  4.12it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:02<00:01,  4.18it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:02<00:01,  4.22it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:02<00:01,  4.24it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:02<00:01,  4.26it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:03<00:00,  4.28it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:03<00:00,  4.01it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 5 examples in 15 steps.\n",
      "torch.Size([5, 129, 512])\n",
      "torch.Size([5, 128, 512])\n",
      "Data shape for PLMS sampling is (5, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:06,  2.18it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:04,  3.07it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:00<00:03,  3.53it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:01<00:03,  3.80it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:01<00:02,  3.97it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:02,  4.08it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:02,  4.15it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:02<00:01,  4.20it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:02<00:01,  4.23it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:02<00:01,  4.25it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:02<00:01,  4.27it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:03<00:00,  4.28it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:03<00:00,  4.30it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:03<00:00,  4.05it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 5 examples in 15 steps.\n",
      "torch.Size([5, 129, 512])\n",
      "torch.Size([5, 128, 512])\n",
      "Data shape for PLMS sampling is (5, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:06,  2.18it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:04,  3.07it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:00<00:03,  3.53it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:01<00:03,  3.80it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:01<00:02,  3.97it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:02,  4.07it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:02,  4.14it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:02<00:01,  4.19it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:02<00:01,  4.22it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:02<00:01,  4.25it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:02<00:01,  4.26it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:03<00:00,  4.28it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:03<00:00,  4.28it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:03<00:00,  4.30it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:03<00:00,  4.05it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 5 examples in 15 steps.\n",
      "torch.Size([5, 129, 512])\n",
      "torch.Size([5, 128, 512])\n",
      "Data shape for PLMS sampling is (5, 4, 64, 64)\n",
      "Running PLMS Sampling with 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PLMS Sampler:   0%|                                                         | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "PLMS Sampler:   6%|                                              | 1/16 [00:00<00:06,  2.18it/s]\u001b[A\n",
      "PLMS Sampler:  12%|                                          | 2/16 [00:00<00:04,  3.07it/s]\u001b[A\n",
      "PLMS Sampler:  19%|                                       | 3/16 [00:00<00:03,  3.53it/s]\u001b[A\n",
      "PLMS Sampler:  25%|                                    | 4/16 [00:01<00:03,  3.80it/s]\u001b[A\n",
      "PLMS Sampler:  31%|                                 | 5/16 [00:01<00:02,  3.97it/s]\u001b[A\n",
      "PLMS Sampler:  38%|                              | 6/16 [00:01<00:02,  4.07it/s]\u001b[A\n",
      "PLMS Sampler:  44%|                           | 7/16 [00:01<00:02,  4.14it/s]\u001b[A\n",
      "PLMS Sampler:  50%|                        | 8/16 [00:02<00:01,  4.19it/s]\u001b[A\n",
      "PLMS Sampler:  56%|                     | 9/16 [00:02<00:01,  4.23it/s]\u001b[A\n",
      "PLMS Sampler:  62%|                  | 10/16 [00:02<00:01,  4.25it/s]\u001b[A\n",
      "PLMS Sampler:  69%|               | 11/16 [00:02<00:01,  4.27it/s]\u001b[A\n",
      "PLMS Sampler:  75%|            | 12/16 [00:03<00:00,  4.28it/s]\u001b[A\n",
      "PLMS Sampler:  81%|         | 13/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler:  88%|      | 14/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler:  94%|   | 15/16 [00:03<00:00,  4.29it/s]\u001b[A\n",
      "PLMS Sampler: 100%|| 16/16 [00:03<00:00,  4.05it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "firmitas/firmitas\n",
      "32\n",
      "firmitas/firmitas\n",
      "40\n",
      "firmitas/firmitas\n",
      "49\n",
      "firmitas/firmitas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0061931610107421875,
       "initial": 0,
       "n": 0,
       "ncols": 99,
       "nrows": 12,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781ef43119304237acc3afa7be3b382d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n",
      "utilitas/utilitas\n",
      "496\n",
      "venustas/venustas\n",
      "690\n",
      "venustas/venustas\n",
      "280\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:98: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "firmitas/firmitas\n",
      "392\n",
      "venustas/venustas\n",
      "270\n",
      "firmitas/firmitas\n",
      "217\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "906\n",
      "venustas/venustas\n",
      "302\n",
      "firmitas/firmitas\n",
      "329\n",
      "firmitas/firmitas\n",
      "112\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "626\n",
      "venustas/venustas\n",
      "939\n",
      "venustas/venustas\n",
      "116\n",
      "firmitas/firmitas\n",
      "402\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "39\n",
      "firmitas/firmitas\n",
      "928\n",
      "venustas/venustas\n",
      "599\n",
      "venustas/venustas\n",
      "259\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "220\n",
      "utilitas/utilitas\n",
      "598\n",
      "venustas/venustas\n",
      "33\n",
      "firmitas/firmitas\n",
      "71\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "409\n",
      "utilitas/utilitas\n",
      "609\n",
      "venustas/venustas\n",
      "645\n",
      "venustas/venustas\n",
      "863\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "417\n",
      "utilitas/utilitas\n",
      "147\n",
      "firmitas/firmitas\n",
      "786\n",
      "venustas/venustas\n",
      "272\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "526\n",
      "utilitas/utilitas\n",
      "113\n",
      "firmitas/firmitas\n",
      "511\n",
      "utilitas/utilitas\n",
      "772\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "900\n",
      "venustas/venustas\n",
      "516\n",
      "utilitas/utilitas\n",
      "634\n",
      "venustas/venustas\n",
      "468\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "754\n",
      "venustas/venustas\n",
      "603\n",
      "venustas/venustas\n",
      "15\n",
      "firmitas/firmitas\n",
      "169\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "761\n",
      "venustas/venustas\n",
      "374\n",
      "venustas/venustas\n",
      "397\n",
      "venustas/venustas\n",
      "5\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "557\n",
      "venustas/venustas\n",
      "389\n",
      "venustas/venustas\n",
      "348\n",
      "firmitas/firmitas\n",
      "628\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "630\n",
      "venustas/venustas\n",
      "295\n",
      "firmitas/firmitas\n",
      "679\n",
      "venustas/venustas\n",
      "588\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "105\n",
      "firmitas/firmitas\n",
      "176\n",
      "utilitas/utilitas\n",
      "343\n",
      "firmitas/firmitas\n",
      "268\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "739\n",
      "firmitas/firmitas\n",
      "680\n",
      "venustas/venustas\n",
      "29\n",
      "firmitas/firmitas\n",
      "266\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "104\n",
      "firmitas/firmitas\n",
      "544\n",
      "utilitas/utilitas\n",
      "25\n",
      "firmitas/firmitas\n",
      "706\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "608\n",
      "venustas/venustas\n",
      "455\n",
      "venustas/venustas\n",
      "382\n",
      "venustas/venustas\n",
      "832\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "746\n",
      "firmitas/firmitas\n",
      "660\n",
      "venustas/venustas\n",
      "489\n",
      "venustas/venustas\n",
      "79\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "146\n",
      "firmitas/firmitas\n",
      "933\n",
      "venustas/venustas\n",
      "390\n",
      "venustas/venustas\n",
      "83\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "48\n",
      "firmitas/firmitas\n",
      "340\n",
      "firmitas/firmitas\n",
      "247\n",
      "utilitas/utilitas\n",
      "253\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "381\n",
      "venustas/venustas\n",
      "308\n",
      "firmitas/firmitas\n",
      "54\n",
      "firmitas/firmitas\n",
      "73\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "522\n",
      "utilitas/utilitas\n",
      "430\n",
      "utilitas/utilitas\n",
      "711\n",
      "firmitas/firmitas\n",
      "509\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "842\n",
      "utilitas/utilitas\n",
      "42\n",
      "firmitas/firmitas\n",
      "784\n",
      "venustas/venustas\n",
      "110\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "35\n",
      "firmitas/firmitas\n",
      "383\n",
      "venustas/venustas\n",
      "366\n",
      "venustas/venustas\n",
      "242\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "908\n",
      "venustas/venustas\n",
      "831\n",
      "utilitas/utilitas\n",
      "540\n",
      "utilitas/utilitas\n",
      "331\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "393\n",
      "venustas/venustas\n",
      "655\n",
      "venustas/venustas\n",
      "743\n",
      "firmitas/firmitas\n",
      "156\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "90\n",
      "firmitas/firmitas\n",
      "87\n",
      "firmitas/firmitas\n",
      "200\n",
      "utilitas/utilitas\n",
      "883\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "621\n",
      "venustas/venustas\n",
      "971\n",
      "firmitas/firmitas\n",
      "429\n",
      "utilitas/utilitas\n",
      "13\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "108\n",
      "firmitas/firmitas\n",
      "321\n",
      "firmitas/firmitas\n",
      "778\n",
      "venustas/venustas\n",
      "768\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "3\n",
      "firmitas/firmitas\n",
      "0\n",
      "firmitas/firmitas\n",
      "231\n",
      "utilitas/utilitas\n",
      "359\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "293\n",
      "firmitas/firmitas\n",
      "993\n",
      "firmitas/firmitas\n",
      "256\n",
      "firmitas/firmitas\n",
      "843\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "60\n",
      "firmitas/firmitas\n",
      "826\n",
      "utilitas/utilitas\n",
      "796\n",
      "venustas/venustas\n",
      "241\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "309\n",
      "firmitas/firmitas\n",
      "937\n",
      "venustas/venustas\n",
      "150\n",
      "utilitas/utilitas\n",
      "978\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "131\n",
      "firmitas/firmitas\n",
      "799\n",
      "venustas/venustas\n",
      "671\n",
      "venustas/venustas\n",
      "909\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "870\n",
      "venustas/venustas\n",
      "675\n",
      "venustas/venustas\n",
      "846\n",
      "utilitas/utilitas\n",
      "98\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "521\n",
      "utilitas/utilitas\n",
      "775\n",
      "venustas/venustas\n",
      "841\n",
      "utilitas/utilitas\n",
      "610\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "760\n",
      "venustas/venustas\n",
      "481\n",
      "venustas/venustas\n",
      "130\n",
      "firmitas/firmitas\n",
      "45\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "505\n",
      "utilitas/utilitas\n",
      "899\n",
      "venustas/venustas\n",
      "492\n",
      "venustas/venustas\n",
      "288\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "968\n",
      "firmitas/firmitas\n",
      "644\n",
      "venustas/venustas\n",
      "694\n",
      "venustas/venustas\n",
      "356\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "546\n",
      "utilitas/utilitas\n",
      "469\n",
      "venustas/venustas\n",
      "101\n",
      "firmitas/firmitas\n",
      "771\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "161\n",
      "utilitas/utilitas\n",
      "175\n",
      "utilitas/utilitas\n",
      "498\n",
      "venustas/venustas\n",
      "168\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "845\n",
      "utilitas/utilitas\n",
      "244\n",
      "utilitas/utilitas\n",
      "125\n",
      "firmitas/firmitas\n",
      "44\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "145\n",
      "firmitas/firmitas\n",
      "191\n",
      "utilitas/utilitas\n",
      "179\n",
      "utilitas/utilitas\n",
      "483\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "8\n",
      "firmitas/firmitas\n",
      "440\n",
      "utilitas/utilitas\n",
      "355\n",
      "venustas/venustas\n",
      "854\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "765\n",
      "venustas/venustas\n",
      "640\n",
      "venustas/venustas\n",
      "976\n",
      "firmitas/firmitas\n",
      "926\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "140\n",
      "firmitas/firmitas\n",
      "987\n",
      "firmitas/firmitas\n",
      "794\n",
      "venustas/venustas\n",
      "949\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "718\n",
      "firmitas/firmitas\n",
      "255\n",
      "firmitas/firmitas\n",
      "358\n",
      "venustas/venustas\n",
      "856\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "135\n",
      "firmitas/firmitas\n",
      "352\n",
      "venustas/venustas\n",
      "411\n",
      "utilitas/utilitas\n",
      "475\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "400\n",
      "utilitas/utilitas\n",
      "905\n",
      "venustas/venustas\n",
      "970\n",
      "firmitas/firmitas\n",
      "472\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "465\n",
      "venustas/venustas\n",
      "448\n",
      "utilitas/utilitas\n",
      "595\n",
      "venustas/venustas\n",
      "336\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "316\n",
      "firmitas/firmitas\n",
      "18\n",
      "firmitas/firmitas\n",
      "226\n",
      "utilitas/utilitas\n",
      "534\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "569\n",
      "venustas/venustas\n",
      "558\n",
      "venustas/venustas\n",
      "159\n",
      "utilitas/utilitas\n",
      "376\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "833\n",
      "utilitas/utilitas\n",
      "977\n",
      "firmitas/firmitas\n",
      "790\n",
      "venustas/venustas\n",
      "494\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "620\n",
      "venustas/venustas\n",
      "708\n",
      "firmitas/firmitas\n",
      "733\n",
      "firmitas/firmitas\n",
      "174\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "642\n",
      "venustas/venustas\n",
      "897\n",
      "venustas/venustas\n",
      "812\n",
      "utilitas/utilitas\n",
      "236\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "89\n",
      "firmitas/firmitas\n",
      "528\n",
      "utilitas/utilitas\n",
      "502\n",
      "utilitas/utilitas\n",
      "967\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "828\n",
      "utilitas/utilitas\n",
      "954\n",
      "firmitas/firmitas\n",
      "470\n",
      "venustas/venustas\n",
      "495\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "674\n",
      "venustas/venustas\n",
      "173\n",
      "utilitas/utilitas\n",
      "779\n",
      "venustas/venustas\n",
      "785\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "491\n",
      "venustas/venustas\n",
      "695\n",
      "venustas/venustas\n",
      "94\n",
      "firmitas/firmitas\n",
      "667\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "803\n",
      "utilitas/utilitas\n",
      "574\n",
      "venustas/venustas\n",
      "120\n",
      "firmitas/firmitas\n",
      "479\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "917\n",
      "venustas/venustas\n",
      "423\n",
      "utilitas/utilitas\n",
      "224\n",
      "utilitas/utilitas\n",
      "488\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "180\n",
      "utilitas/utilitas\n",
      "16\n",
      "firmitas/firmitas\n",
      "744\n",
      "firmitas/firmitas\n",
      "137\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "433\n",
      "utilitas/utilitas\n",
      "248\n",
      "utilitas/utilitas\n",
      "305\n",
      "firmitas/firmitas\n",
      "773\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "208\n",
      "utilitas/utilitas\n",
      "252\n",
      "firmitas/firmitas\n",
      "395\n",
      "venustas/venustas\n",
      "545\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "122\n",
      "firmitas/firmitas\n",
      "211\n",
      "utilitas/utilitas\n",
      "426\n",
      "utilitas/utilitas\n",
      "697\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "72\n",
      "firmitas/firmitas\n",
      "593\n",
      "venustas/venustas\n",
      "96\n",
      "firmitas/firmitas\n",
      "791\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "199\n",
      "utilitas/utilitas\n",
      "398\n",
      "venustas/venustas\n",
      "666\n",
      "venustas/venustas\n",
      "855\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "267\n",
      "firmitas/firmitas\n",
      "55\n",
      "firmitas/firmitas\n",
      "474\n",
      "venustas/venustas\n",
      "704\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "478\n",
      "venustas/venustas\n",
      "918\n",
      "venustas/venustas\n",
      "615\n",
      "venustas/venustas\n",
      "958\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "731\n",
      "firmitas/firmitas\n",
      "506\n",
      "utilitas/utilitas\n",
      "867\n",
      "venustas/venustas\n",
      "868\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "601\n",
      "venustas/venustas\n",
      "431\n",
      "utilitas/utilitas\n",
      "360\n",
      "venustas/venustas\n",
      "21\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "466\n",
      "venustas/venustas\n",
      "142\n",
      "firmitas/firmitas\n",
      "345\n",
      "firmitas/firmitas\n",
      "36\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "164\n",
      "utilitas/utilitas\n",
      "121\n",
      "firmitas/firmitas\n",
      "292\n",
      "firmitas/firmitas\n",
      "100\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "683\n",
      "venustas/venustas\n",
      "685\n",
      "venustas/venustas\n",
      "99\n",
      "firmitas/firmitas\n",
      "93\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "575\n",
      "venustas/venustas\n",
      "239\n",
      "utilitas/utilitas\n",
      "576\n",
      "venustas/venustas\n",
      "656\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "27\n",
      "firmitas/firmitas\n",
      "804\n",
      "utilitas/utilitas\n",
      "152\n",
      "utilitas/utilitas\n",
      "756\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "476\n",
      "venustas/venustas\n",
      "651\n",
      "venustas/venustas\n",
      "942\n",
      "venustas/venustas\n",
      "973\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "549\n",
      "utilitas/utilitas\n",
      "869\n",
      "venustas/venustas\n",
      "188\n",
      "utilitas/utilitas\n",
      "346\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "234\n",
      "utilitas/utilitas\n",
      "350\n",
      "venustas/venustas\n",
      "950\n",
      "firmitas/firmitas\n",
      "880\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "69\n",
      "firmitas/firmitas\n",
      "623\n",
      "venustas/venustas\n",
      "500\n",
      "utilitas/utilitas\n",
      "487\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "66\n",
      "firmitas/firmitas\n",
      "965\n",
      "firmitas/firmitas\n",
      "327\n",
      "firmitas/firmitas\n",
      "559\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "74\n",
      "firmitas/firmitas\n",
      "339\n",
      "firmitas/firmitas\n",
      "922\n",
      "venustas/venustas\n",
      "979\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "960\n",
      "firmitas/firmitas\n",
      "788\n",
      "venustas/venustas\n",
      "338\n",
      "firmitas/firmitas\n",
      "924\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "287\n",
      "firmitas/firmitas\n",
      "915\n",
      "venustas/venustas\n",
      "596\n",
      "venustas/venustas\n",
      "902\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "187\n",
      "utilitas/utilitas\n",
      "166\n",
      "utilitas/utilitas\n",
      "808\n",
      "utilitas/utilitas\n",
      "149\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "567\n",
      "venustas/venustas\n",
      "696\n",
      "venustas/venustas\n",
      "524\n",
      "utilitas/utilitas\n",
      "702\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "264\n",
      "firmitas/firmitas\n",
      "215\n",
      "utilitas/utilitas\n",
      "539\n",
      "utilitas/utilitas\n",
      "237\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "622\n",
      "venustas/venustas\n",
      "263\n",
      "firmitas/firmitas\n",
      "836\n",
      "utilitas/utilitas\n",
      "827\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "394\n",
      "venustas/venustas\n",
      "182\n",
      "utilitas/utilitas\n",
      "320\n",
      "firmitas/firmitas\n",
      "19\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "97\n",
      "firmitas/firmitas\n",
      "460\n",
      "venustas/venustas\n",
      "209\n",
      "utilitas/utilitas\n",
      "606\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "190\n",
      "utilitas/utilitas\n",
      "10\n",
      "firmitas/firmitas\n",
      "127\n",
      "firmitas/firmitas\n",
      "550\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "940\n",
      "venustas/venustas\n",
      "194\n",
      "utilitas/utilitas\n",
      "62\n",
      "firmitas/firmitas\n",
      "379\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "344\n",
      "firmitas/firmitas\n",
      "407\n",
      "utilitas/utilitas\n",
      "930\n",
      "venustas/venustas\n",
      "462\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "477\n",
      "venustas/venustas\n",
      "4\n",
      "firmitas/firmitas\n",
      "988\n",
      "firmitas/firmitas\n",
      "945\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "493\n",
      "venustas/venustas\n",
      "625\n",
      "venustas/venustas\n",
      "401\n",
      "utilitas/utilitas\n",
      "391\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "792\n",
      "venustas/venustas\n",
      "717\n",
      "firmitas/firmitas\n",
      "63\n",
      "firmitas/firmitas\n",
      "432\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "669\n",
      "venustas/venustas\n",
      "672\n",
      "venustas/venustas\n",
      "797\n",
      "venustas/venustas\n",
      "517\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "580\n",
      "venustas/venustas\n",
      "795\n",
      "venustas/venustas\n",
      "723\n",
      "firmitas/firmitas\n",
      "9\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "370\n",
      "venustas/venustas\n",
      "703\n",
      "firmitas/firmitas\n",
      "464\n",
      "venustas/venustas\n",
      "418\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "955\n",
      "firmitas/firmitas\n",
      "257\n",
      "firmitas/firmitas\n",
      "563\n",
      "venustas/venustas\n",
      "22\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "279\n",
      "firmitas/firmitas\n",
      "106\n",
      "firmitas/firmitas\n",
      "230\n",
      "utilitas/utilitas\n",
      "77\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "233\n",
      "utilitas/utilitas\n",
      "691\n",
      "venustas/venustas\n",
      "722\n",
      "firmitas/firmitas\n",
      "659\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "951\n",
      "firmitas/firmitas\n",
      "824\n",
      "utilitas/utilitas\n",
      "898\n",
      "venustas/venustas\n",
      "740\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "396\n",
      "venustas/venustas\n",
      "547\n",
      "utilitas/utilitas\n",
      "893\n",
      "venustas/venustas\n",
      "12\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "387\n",
      "venustas/venustas\n",
      "871\n",
      "venustas/venustas\n",
      "58\n",
      "firmitas/firmitas\n",
      "78\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "767\n",
      "venustas/venustas\n",
      "874\n",
      "venustas/venustas\n",
      "61\n",
      "firmitas/firmitas\n",
      "776\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "6\n",
      "firmitas/firmitas\n",
      "536\n",
      "utilitas/utilitas\n",
      "303\n",
      "firmitas/firmitas\n",
      "70\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "265\n",
      "firmitas/firmitas\n",
      "88\n",
      "firmitas/firmitas\n",
      "538\n",
      "utilitas/utilitas\n",
      "119\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "585\n",
      "venustas/venustas\n",
      "815\n",
      "utilitas/utilitas\n",
      "195\n",
      "utilitas/utilitas\n",
      "693\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "780\n",
      "venustas/venustas\n",
      "839\n",
      "utilitas/utilitas\n",
      "185\n",
      "utilitas/utilitas\n",
      "770\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "291\n",
      "firmitas/firmitas\n",
      "520\n",
      "utilitas/utilitas\n",
      "235\n",
      "utilitas/utilitas\n",
      "657\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "931\n",
      "venustas/venustas\n",
      "932\n",
      "venustas/venustas\n",
      "963\n",
      "firmitas/firmitas\n",
      "361\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "269\n",
      "firmitas/firmitas\n",
      "118\n",
      "firmitas/firmitas\n",
      "319\n",
      "firmitas/firmitas\n",
      "910\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "529\n",
      "utilitas/utilitas\n",
      "11\n",
      "firmitas/firmitas\n",
      "301\n",
      "firmitas/firmitas\n",
      "758\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "818\n",
      "utilitas/utilitas\n",
      "555\n",
      "venustas/venustas\n",
      "600\n",
      "venustas/venustas\n",
      "250\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "805\n",
      "utilitas/utilitas\n",
      "342\n",
      "firmitas/firmitas\n",
      "454\n",
      "venustas/venustas\n",
      "46\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "202\n",
      "utilitas/utilitas\n",
      "646\n",
      "venustas/venustas\n",
      "504\n",
      "utilitas/utilitas\n",
      "512\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "802\n",
      "utilitas/utilitas\n",
      "887\n",
      "venustas/venustas\n",
      "736\n",
      "firmitas/firmitas\n",
      "186\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "157\n",
      "utilitas/utilitas\n",
      "943\n",
      "venustas/venustas\n",
      "457\n",
      "venustas/venustas\n",
      "886\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "103\n",
      "firmitas/firmitas\n",
      "830\n",
      "utilitas/utilitas\n",
      "227\n",
      "utilitas/utilitas\n",
      "997\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "326\n",
      "firmitas/firmitas\n",
      "956\n",
      "firmitas/firmitas\n",
      "568\n",
      "venustas/venustas\n",
      "759\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "91\n",
      "firmitas/firmitas\n",
      "419\n",
      "utilitas/utilitas\n",
      "789\n",
      "venustas/venustas\n",
      "533\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "314\n",
      "firmitas/firmitas\n",
      "777\n",
      "venustas/venustas\n",
      "51\n",
      "firmitas/firmitas\n",
      "271\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "482\n",
      "venustas/venustas\n",
      "258\n",
      "firmitas/firmitas\n",
      "543\n",
      "utilitas/utilitas\n",
      "914\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "285\n",
      "firmitas/firmitas\n",
      "923\n",
      "venustas/venustas\n",
      "413\n",
      "utilitas/utilitas\n",
      "866\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "825\n",
      "utilitas/utilitas\n",
      "821\n",
      "utilitas/utilitas\n",
      "530\n",
      "utilitas/utilitas\n",
      "68\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "888\n",
      "venustas/venustas\n",
      "929\n",
      "venustas/venustas\n",
      "228\n",
      "utilitas/utilitas\n",
      "751\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "617\n",
      "venustas/venustas\n",
      "647\n",
      "venustas/venustas\n",
      "627\n",
      "venustas/venustas\n",
      "123\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "124\n",
      "firmitas/firmitas\n",
      "624\n",
      "venustas/venustas\n",
      "727\n",
      "firmitas/firmitas\n",
      "317\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "43\n",
      "firmitas/firmitas\n",
      "633\n",
      "venustas/venustas\n",
      "556\n",
      "venustas/venustas\n",
      "445\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "921\n",
      "venustas/venustas\n",
      "425\n",
      "utilitas/utilitas\n",
      "730\n",
      "firmitas/firmitas\n",
      "399\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "904\n",
      "venustas/venustas\n",
      "232\n",
      "utilitas/utilitas\n",
      "652\n",
      "venustas/venustas\n",
      "631\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "281\n",
      "firmitas/firmitas\n",
      "962\n",
      "firmitas/firmitas\n",
      "734\n",
      "firmitas/firmitas\n",
      "716\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "999\n",
      "firmitas/firmitas\n",
      "37\n",
      "firmitas/firmitas\n",
      "357\n",
      "venustas/venustas\n",
      "959\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "686\n",
      "venustas/venustas\n",
      "284\n",
      "firmitas/firmitas\n",
      "151\n",
      "utilitas/utilitas\n",
      "307\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "753\n",
      "venustas/venustas\n",
      "497\n",
      "venustas/venustas\n",
      "763\n",
      "venustas/venustas\n",
      "641\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "735\n",
      "firmitas/firmitas\n",
      "128\n",
      "firmitas/firmitas\n",
      "766\n",
      "venustas/venustas\n",
      "212\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "17\n",
      "firmitas/firmitas\n",
      "614\n",
      "venustas/venustas\n",
      "809\n",
      "utilitas/utilitas\n",
      "436\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "948\n",
      "venustas/venustas\n",
      "878\n",
      "venustas/venustas\n",
      "213\n",
      "utilitas/utilitas\n",
      "643\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "443\n",
      "utilitas/utilitas\n",
      "564\n",
      "venustas/venustas\n",
      "367\n",
      "venustas/venustas\n",
      "23\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "714\n",
      "firmitas/firmitas\n",
      "141\n",
      "firmitas/firmitas\n",
      "243\n",
      "utilitas/utilitas\n",
      "330\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "852\n",
      "venustas/venustas\n",
      "637\n",
      "venustas/venustas\n",
      "223\n",
      "utilitas/utilitas\n",
      "189\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "664\n",
      "venustas/venustas\n",
      "177\n",
      "utilitas/utilitas\n",
      "282\n",
      "firmitas/firmitas\n",
      "318\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "275\n",
      "firmitas/firmitas\n",
      "384\n",
      "venustas/venustas\n",
      "513\n",
      "utilitas/utilitas\n",
      "862\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "310\n",
      "firmitas/firmitas\n",
      "990\n",
      "firmitas/firmitas\n",
      "895\n",
      "venustas/venustas\n",
      "403\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "748\n",
      "firmitas/firmitas\n",
      "882\n",
      "venustas/venustas\n",
      "819\n",
      "utilitas/utilitas\n",
      "365\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "764\n",
      "venustas/venustas\n",
      "238\n",
      "utilitas/utilitas\n",
      "953\n",
      "firmitas/firmitas\n",
      "438\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "412\n",
      "utilitas/utilitas\n",
      "531\n",
      "utilitas/utilitas\n",
      "447\n",
      "utilitas/utilitas\n",
      "537\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "632\n",
      "venustas/venustas\n",
      "709\n",
      "firmitas/firmitas\n",
      "974\n",
      "firmitas/firmitas\n",
      "439\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "273\n",
      "firmitas/firmitas\n",
      "590\n",
      "venustas/venustas\n",
      "222\n",
      "utilitas/utilitas\n",
      "594\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "947\n",
      "venustas/venustas\n",
      "984\n",
      "firmitas/firmitas\n",
      "982\n",
      "firmitas/firmitas\n",
      "24\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "115\n",
      "firmitas/firmitas\n",
      "514\n",
      "utilitas/utilitas\n",
      "738\n",
      "firmitas/firmitas\n",
      "405\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "453\n",
      "venustas/venustas\n",
      "515\n",
      "utilitas/utilitas\n",
      "527\n",
      "utilitas/utilitas\n",
      "129\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "324\n",
      "firmitas/firmitas\n",
      "84\n",
      "firmitas/firmitas\n",
      "143\n",
      "firmitas/firmitas\n",
      "872\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "663\n",
      "venustas/venustas\n",
      "377\n",
      "venustas/venustas\n",
      "52\n",
      "firmitas/firmitas\n",
      "490\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "817\n",
      "utilitas/utilitas\n",
      "944\n",
      "venustas/venustas\n",
      "552\n",
      "venustas/venustas\n",
      "163\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "138\n",
      "firmitas/firmitas\n",
      "607\n",
      "venustas/venustas\n",
      "687\n",
      "venustas/venustas\n",
      "480\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "611\n",
      "venustas/venustas\n",
      "312\n",
      "firmitas/firmitas\n",
      "153\n",
      "utilitas/utilitas\n",
      "654\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "653\n",
      "venustas/venustas\n",
      "881\n",
      "venustas/venustas\n",
      "525\n",
      "utilitas/utilitas\n",
      "332\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "486\n",
      "venustas/venustas\n",
      "995\n",
      "firmitas/firmitas\n",
      "848\n",
      "utilitas/utilitas\n",
      "782\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "369\n",
      "venustas/venustas\n",
      "38\n",
      "firmitas/firmitas\n",
      "278\n",
      "firmitas/firmitas\n",
      "677\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "95\n",
      "firmitas/firmitas\n",
      "584\n",
      "venustas/venustas\n",
      "422\n",
      "utilitas/utilitas\n",
      "885\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "903\n",
      "venustas/venustas\n",
      "57\n",
      "firmitas/firmitas\n",
      "441\n",
      "utilitas/utilitas\n",
      "583\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "107\n",
      "firmitas/firmitas\n",
      "591\n",
      "venustas/venustas\n",
      "420\n",
      "utilitas/utilitas\n",
      "424\n",
      "utilitas/utilitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "47\n",
      "firmitas/firmitas\n",
      "985\n",
      "firmitas/firmitas\n",
      "781\n",
      "venustas/venustas\n",
      "737\n",
      "firmitas/firmitas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "602\n",
      "venustas/venustas\n",
      "658\n",
      "venustas/venustas\n",
      "353\n",
      "venustas/venustas\n",
      "891\n",
      "venustas/venustas\n",
      "torch.Size([4, 129, 512])\n",
      "torch.Size([4, 128, 512])\n",
      "286\n",
      "firmitas/firmitas\n",
      "torch.Size([1, 129, 512])\n",
      "torch.Size([1, 128, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:98: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'wandb_logger' object has no attribute 'log_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m config\u001b[38;5;241m.\u001b[39meval_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m      4\u001b[0m logger\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mgenerative_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m##AQUI\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 164\u001b[0m, in \u001b[0;36meLDM.finetune\u001b[0;34m(self, trainers, dataset, test_dataset, bs1, lr1, output_path, config, logger)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval_avg \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39meval_avg\n\u001b[1;32m    163\u001b[0m logger\u001b[38;5;241m.\u001b[39mwatch_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)        \n\u001b[0;32m--> 164\u001b[0m \u001b[43mtrainers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39munfreeze_whole_model()\n\u001b[1;32m    168\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    169\u001b[0m     {\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    731\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    733\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    734\u001b[0m )\n\u001b[0;32m--> 735\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1166\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1283\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:201\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:314\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_batches_that_stepped \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# log epoch metrics\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logger_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_train_epoch_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_batches_that_stepped \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_progress\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:163\u001b[0m, in \u001b[0;36mLoggerConnector.update_train_epoch_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_train_epoch_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# add the metrics to the loggers\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch_end_reached\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# reset result collection for next epoch\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_results()\n",
      "File \u001b[0;32m~/.conda/envs/dreamdiffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:109\u001b[0m, in \u001b[0;36mLoggerConnector.log_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    107\u001b[0m     logger\u001b[38;5;241m.\u001b[39magg_and_log_metrics(metrics\u001b[38;5;241m=\u001b[39mscalar_metrics, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m(metrics\u001b[38;5;241m=\u001b[39mscalar_metrics, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[1;32m    110\u001b[0m logger\u001b[38;5;241m.\u001b[39msave()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'wandb_logger' object has no attribute 'log_metrics'"
     ]
    }
   ],
   "source": [
    "config.num_samples = 3\n",
    "dataset_train.num_samples = 3\n",
    "config.eval_avg = 12\n",
    "logger.version = \"1\"\n",
    "generative_model.finetune(trainer, dataset_train, dataset_test,\n",
    "                4, 1e-3, output_path, config=config, logger = logger)\n",
    "\n",
    "##AQUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Splitter object at 0x74cd5c375a60>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firmitas/firmitas\n",
      "COUNT FMRI\n",
      "{'eeg': tensor([[-2.5920e-03, -2.6755e-03, -2.6248e-03,  ...,  2.9129e-06,\n",
      "          3.9656e-06,  5.2704e-06],\n",
      "        [-1.4760e-03, -1.5230e-03, -1.4935e-03,  ...,  1.7551e-06,\n",
      "          3.5761e-06,  5.9581e-06],\n",
      "        [-1.0803e-03, -1.1147e-03, -1.0936e-03,  ...,  1.8780e-06,\n",
      "          3.0231e-06,  4.3320e-06],\n",
      "        ...,\n",
      "        [-2.7246e-03, -2.8144e-03, -2.7611e-03,  ...,  3.0964e-06,\n",
      "          6.2904e-06,  1.0543e-05],\n",
      "        [-2.1341e-03, -2.2035e-03, -2.1603e-03,  ...,  1.5066e-06,\n",
      "          5.5208e-06,  1.0565e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 'label': 'firmitas/firmitas', 'image': tensor([[[0.9799, 0.9901, 0.9887],\n",
      "         [0.9963, 0.9894, 0.9808],\n",
      "         [0.9555, 0.9568, 0.9592],\n",
      "         ...,\n",
      "         [0.9965, 0.9792, 0.9687],\n",
      "         [0.9974, 0.9903, 0.9841],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.9865, 0.9897, 0.9493],\n",
      "         [0.9985, 0.9897, 0.9867],\n",
      "         [0.6946, 0.6918, 0.8464],\n",
      "         ...,\n",
      "         [0.9977, 0.9947, 0.9930],\n",
      "         [0.9897, 0.9956, 0.9813],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.9962, 0.9915, 0.9793],\n",
      "         [0.9919, 0.9838, 0.9839],\n",
      "         [0.5561, 0.5303, 0.9258],\n",
      "         ...,\n",
      "         [0.9882, 0.9921, 0.9829],\n",
      "         [0.9799, 0.9929, 0.9983],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[1.9303, 1.9303, 1.9303,  ..., 1.9011, 1.8865, 1.9011],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9157, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.8865, 1.2588, 1.2442],\n",
      "         ...,\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303]],\n",
      "\n",
      "        [[2.0749, 2.0749, 2.0749,  ..., 2.0449, 2.0599, 2.0599],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 1.9848, 1.3845, 1.5646],\n",
      "         ...,\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749]],\n",
      "\n",
      "        [[2.1459, 2.1459, 2.1459,  ..., 2.1175, 2.1175, 2.1032],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1175, 2.1175, 2.1175],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1032, 2.0748, 2.0464],\n",
      "         ...,\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459]]])}}\n",
      "{'eeg': tensor([[-2.5920e-03, -2.6755e-03, -2.6248e-03,  ...,  2.9129e-06,\n",
      "          3.9656e-06,  5.2704e-06],\n",
      "        [-1.4760e-03, -1.5230e-03, -1.4935e-03,  ...,  1.7551e-06,\n",
      "          3.5761e-06,  5.9581e-06],\n",
      "        [-1.0803e-03, -1.1147e-03, -1.0936e-03,  ...,  1.8780e-06,\n",
      "          3.0231e-06,  4.3320e-06],\n",
      "        ...,\n",
      "        [-2.7246e-03, -2.8144e-03, -2.7611e-03,  ...,  3.0964e-06,\n",
      "          6.2904e-06,  1.0543e-05],\n",
      "        [-2.1341e-03, -2.2035e-03, -2.1603e-03,  ...,  1.5066e-06,\n",
      "          5.5208e-06,  1.0565e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 'label': 'firmitas/firmitas', 'image': tensor([[[0.9799, 0.9901, 0.9887],\n",
      "         [0.9963, 0.9894, 0.9808],\n",
      "         [0.9555, 0.9568, 0.9592],\n",
      "         ...,\n",
      "         [0.9965, 0.9792, 0.9687],\n",
      "         [0.9974, 0.9903, 0.9841],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.9865, 0.9897, 0.9493],\n",
      "         [0.9985, 0.9897, 0.9867],\n",
      "         [0.6946, 0.6918, 0.8464],\n",
      "         ...,\n",
      "         [0.9977, 0.9947, 0.9930],\n",
      "         [0.9897, 0.9956, 0.9813],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.9962, 0.9915, 0.9793],\n",
      "         [0.9919, 0.9838, 0.9839],\n",
      "         [0.5561, 0.5303, 0.9258],\n",
      "         ...,\n",
      "         [0.9882, 0.9921, 0.9829],\n",
      "         [0.9799, 0.9929, 0.9983],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[1.9303, 1.9303, 1.9303,  ..., 1.9011, 1.8865, 1.9011],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9157, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.8865, 1.2588, 1.2442],\n",
      "         ...,\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303]],\n",
      "\n",
      "        [[2.0749, 2.0749, 2.0749,  ..., 2.0449, 2.0599, 2.0599],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 1.9848, 1.3845, 1.5646],\n",
      "         ...,\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749]],\n",
      "\n",
      "        [[2.1459, 2.1459, 2.1459,  ..., 2.1175, 2.1175, 2.1032],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1175, 2.1175, 2.1175],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1032, 2.0748, 2.0464],\n",
      "         ...,\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459]]])}}\n",
      "rendering 3 examples in 250 steps.\n",
      "torch.Size([3, 129, 512])\n",
      "torch.Size([3, 128, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guisi/.local/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m splits_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/eeg_dataset/block_splits_by_image_single.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m dataset_train, dataset_test \u001b[38;5;241m=\u001b[39m create_EEG_dataset(eeg_signals_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/eegdataset/eeg_data.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, splits_path \u001b[38;5;241m=\u001b[39m splits_path,\n\u001b[1;32m     13\u001b[0m             image_transform\u001b[38;5;241m=\u001b[39m[img_transform_train, img_transform_test], subject \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m grid, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgenerative_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# generate 2 instances\u001b[39;00m\n\u001b[1;32m     17\u001b[0m grid_imgs \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(grid\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n\u001b[1;32m     19\u001b[0m grid_imgs\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamples_train.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 211\u001b[0m, in \u001b[0;36meLDM.generate\u001b[0;34m(self, fmri_embedding, num_samples, ddim_steps, HW, limit, state, output_path)\u001b[0m\n\u001b[1;32m    209\u001b[0m c, re_latent \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_learned_conditioning(repeat(latent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh w -> c h w\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39mnum_samples)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# c = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m samples_ddim, _ \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m x_samples_ddim \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_first_stage(samples_ddim)\n\u001b[1;32m    218\u001b[0m x_samples_ddim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp((x_samples_ddim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 90\u001b[0m, in \u001b[0;36mPLMSSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m conditioning\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m batch_size:\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconditioning\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m conditionings but batch-size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddim_num_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddim_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# sampling\u001b[39;00m\n\u001b[1;32m     92\u001b[0m C, H, W \u001b[38;5;241m=\u001b[39m shape\n",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m, in \u001b[0;36mPLMSSampler.make_schedule\u001b[0;34m(self, ddim_num_steps, ddim_discretize, ddim_eta, verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt_recipm1_alphas_cumprod\u001b[39m\u001b[38;5;124m'\u001b[39m, to_torch(np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m alphas_cumprod\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# ddim sampling parameters\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m ddim_sigmas, ddim_alphas, ddim_alphas_prev \u001b[38;5;241m=\u001b[39m \u001b[43mmake_ddim_sampling_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphacums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malphas_cumprod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43mddim_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_eta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddim_sigmas\u001b[39m\u001b[38;5;124m'\u001b[39m, ddim_sigmas)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddim_alphas\u001b[39m\u001b[38;5;124m'\u001b[39m, ddim_alphas)\n",
      "Cell \u001b[0;32mIn[23], line 60\u001b[0m, in \u001b[0;36mmake_ddim_sampling_parameters\u001b[0;34m(alphacums, ddim_timesteps, eta, verbose)\u001b[0m\n\u001b[1;32m     57\u001b[0m alphas_prev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([alphacums[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m alphacums[ddim_timesteps[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# according the the formula provided in https://arxiv.org/abs/2010.02502\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m sigmas \u001b[38;5;241m=\u001b[39m eta \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alphas_prev) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alphas) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43malphas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malphas_prev\u001b[49m))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelected alphas for ddim sampler: a_t: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; a_(t-1): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphas_prev\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m __array_priority__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# prefer Tensor ops over numpy ones\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1085\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import datetime\n",
    "import argparse\n",
    "import torch\n",
    "from torch.nn import Identity\n",
    "\n",
    "splits_path = 'dataset/eeg_dataset/block_splits_by_image_single.pth'\n",
    "dataset_train, dataset_test = create_EEG_dataset(eeg_signals_path = 'dataset/eegdataset/eeg_data.pth', splits_path = splits_path,\n",
    "            image_transform=[img_transform_train, img_transform_test], subject = 1)\n",
    "\n",
    "grid, _ = generative_model.generate(dataset_train, config.num_samples,\n",
    "            config.ddim_steps, config.HW, 2) # generate 2 instances\n",
    "grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "\n",
    "grid_imgs.save(os.path.join(output_path, f'samples_train.png'))\n",
    "\n",
    "\n",
    "\n",
    "grid, samples = generative_model.generate(dataset_test, config.num_samples,\n",
    "            config.ddim_steps, config.HW, limit=None, output_path = output_path) # generate 10 instances\n",
    "grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "\n",
    "\n",
    "grid_imgs.save(os.path.join(output_path, f'samples_test.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "id": "3D5E6Oa0870-",
    "outputId": "0e689c38-5a31-4a32-a872-5ac7a96a9e59"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/eegdataset/results/eeg_pretrain/26-10-2023-16-30-54/reconst-26-10-2023-17-46-40.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/eegdataset/results/eeg_pretrain/26-10-2023-16-30-54/reconst-26-10-2023-17-46-40.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m display(\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.12/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.12/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.12/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.12/site-packages/IPython/core/display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/eegdataset/results/eeg_pretrain/26-10-2023-16-30-54/reconst-26-10-2023-17-46-40.png'"
     ]
    }
   ],
   "source": [
    "#@title Show some examples from the eeg pretrain reconstruction\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Path to the image\n",
    "image_path = \"dataset/eegdataset/results/eeg_pretrain/26-10-2023-16-30-54/reconst-26-10-2023-17-46-40.png\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(filename=image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "Yn7HHsTPRpzm",
    "outputId": "ce9b34b6-0d0f-4f33-efcd-f321c37eda12"
   },
   "outputs": [],
   "source": [
    "#@title Show some examples from the model generation using EEG pretrain from above and author trained generation - training sample\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Path to the image\n",
    "image_path = \"dataset/eegdataset/gen_results/samples_train.png\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(filename=image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i2g2J4_HR9dC",
    "outputId": "85371462-d6b2-4ec7-c8eb-6b72730b7343"
   },
   "outputs": [],
   "source": [
    "#@title Show some examples from the model generation using EEG pretrain from above and author trained generation - eval examples\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Path to the image\n",
    "image_path = \"dataset/eegdataset/results/eval/28-10-2023-10-48-59/test0-2.png\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(filename=image_path))\n",
    "\n",
    "# Path to the image\n",
    "image_path2 = \"datasete/eegdataset/results/eval/28-10-2023-10-48-59/test3-5.png\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(filename=image_path2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqIYhA3eThgH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
